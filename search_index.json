[
["index.html", "The drake R Package User Manual Chapter 1 Introduction 1.1 Short version 1.2 Long version 1.3 The drake R package 1.4 Installation 1.5 Why drake? 1.6 With Docker 1.7 Documentation 1.8 Help and troubleshooting", " The drake R Package User Manual Will Landau, Kirill Müller, Alex Axthelm, Jasper Clarkberg, Lorenz Walthert, Ellis Hughes, Matthew Mark Strasiotto Copyright Eli Lilly and Company Chapter 1 Introduction 1.1 Short version 1.2 Long version The video above is the recording from the rOpenSci Community Call from 2019-09-24. Visit the call’s page for links to additional resources, and chime in here to propose and vote for ideas for new Community Call topics and speakers. 1.3 The drake R package Data analysis can be slow. A round of scientific computation can take several minutes, hours, or even days to complete. After it finishes, if you update your code or data, your hard-earned results may no longer be valid. How much of that valuable output can you keep, and how much do you need to update? How much runtime must you endure all over again? For projects in R, the drake package can help. It analyzes your workflow, skips steps with up-to-date results, and orchestrates the rest with optional distributed computing. At the end, drake provides evidence that your results match the underlying code and data, which increases your ability to trust your research. 1.4 Installation You can choose among different versions of drake. The latest CRAN release may be more convenient to install, but this manual is kept up to date with the GitHub version, so some features described here may not yet be available on CRAN. # Install the latest stable release from CRAN. install.packages(&quot;drake&quot;) # Alternatively, install the development version from GitHub. install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;ropensci/drake&quot;) 1.5 Why drake? 1.5.1 What gets done stays done. Too many data science projects follow a Sisyphean loop: Launch the code. Wait while it runs. Discover an issue. Restart from scratch. For projects with long runtimes, people tend to get stuck. But with drake, you can automatically Launch the parts that changed since last time. Skip the rest. 1.5.2 Reproducibility with confidence The R community emphasizes reproducibility. Traditional themes include scientific replicability, literate programming with knitr, and version control with git. But internal consistency is important too. Reproducibility carries the promise that your output matches the code and data you say you used. With the exception of non-default triggers and hasty mode, drake strives to keep this promise. 1.5.2.1 Evidence Suppose you are reviewing someone else’s data analysis project for reproducibility. You scrutinize it carefully, checking that the datasets are available and the documentation is thorough. But could you re-create the results without the help of the original author? With drake, it is quick and easy to find out. make(plan) outdated(plan) With everything already up to date, you have tangible evidence of reproducibility. Even though you did not re-create the results, you know the results are re-creatable. They faithfully show what the code is producing. Given the right package environment and system configuration, you have everything you need to reproduce all the output by yourself. 1.5.2.2 Ease When it comes time to actually rerun the entire project, you have much more confidence. Starting over from scratch is trivially easy. clean() # Remove the original author&#39;s results. make(plan) # Independently re-create the results from the code and input data. 1.5.2.3 Independent replication With even more evidence and confidence, you can invest the time to independently replicate the original code base if necessary. Up until this point, you relied on basic drake functions such as make(), so you may not have needed to peek at any substantive author-defined code in advance. In that case, you can stay usefully ignorant as you reimplement the original author’s methodology. In other words, drake could potentially improve the integrity of independent replication. 1.5.2.4 Big data efficiency Select a specialized data format to increase speed and reduce memory consumption. In version 7.5.2.9000 and above, the available formats are “fst” for data frames (example below) and “keras” for Keras models (example here). library(drake) n &lt;- 1e8 # Each target is 1.6 GB in memory. plan &lt;- drake_plan( data_fst = target( data.frame(x = runif(n), y = runif(n)), format = &quot;fst&quot; ), data_old = data.frame(x = runif(n), y = runif(n)) ) make(plan) #&gt; target data_fst #&gt; target data_old build_times(type = &quot;build&quot;) #&gt; # A tibble: 2 x 4 #&gt; target elapsed user system #&gt; &lt;chr&gt; &lt;Duration&gt; &lt;Duration&gt; &lt;Duration&gt; #&gt; 1 data_fst 13.93s 37.562s 7.954s #&gt; 2 data_old 184s (~3.07 minutes) 177s (~2.95 minutes) 4.157s 1.5.2.5 History As of version 7.5.0, drake tracks the history of your analysis: what you built, when you built it, how you built it, the arguments you used in your function calls, and how to get the data back. (Disable with make(history = FALSE)) drake_history(analyze = TRUE) #&gt; # A tibble: 7 x 8 #&gt; target time hash exists command runtime latest quiet #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; #&gt; 1 data 2019-06-23… e580e… TRUE raw_data %&gt;% muta… 0.001 TRUE NA #&gt; 2 fit 2019-06-23… 62a16… TRUE lm(Sepal.Width ~ … 0.00300 TRUE NA #&gt; 3 hist 2019-06-23… 10bcd… TRUE create_plot(data) 0.00500 FALSE NA #&gt; 4 hist 2019-06-23… 00fad… TRUE create_plot(data) 0.00300 TRUE NA #&gt; 5 raw_da… 2019-06-23… 63172… TRUE &quot;readxl::read_exc… 0.00900 TRUE NA #&gt; 6 report 2019-06-23… dd965… TRUE &quot;rmarkdown::rende… 0.476 FALSE TRUE #&gt; 7 report 2019-06-23… dd965… TRUE &quot;rmarkdown::rende… 0.369 TRUE TRUE The history has arguments like quiet (because of the call to knit(quiet = TRUE)) and hashes to help you recover old data. To learn more, see the end of the walkthrough chapter and the drake_history() help file. 1.5.2.6 Reproducible recovery drake’s data recovery feature is another way to avoid rerunning commands. It is useful if: You want to revert to your old code, maybe with git reset. You accidentally clean()ed a target and to get it back. You want to rename an expensive target. See the walkthrough chapter for details. 1.5.2.7 Readability and transparency Ideally, independent observers should be able to read your code and understand it. drake helps in several ways. The drake plan explicitly outlines the steps of the analysis, and vis_drake_graph() visualizes how those steps depend on each other. drake takes care of the parallel scheduling and high-performance computing (HPC) for you. That means the HPC code is no longer tangled up with the code that actually expresses your ideas. You can generate large collections of targets without necessarily changing your code base of imported functions, another nice separation between the concepts and the execution of your workflow 1.5.3 Scale up and out. Not every project can complete in a single R session on your laptop. Some projects need more speed or computing power. Some require a few local processor cores, and some need large high-performance computing systems. But parallel computing is hard. Your tables and figures depend on your analysis results, and your analyses depend on your datasets, so some tasks must finish before others even begin. drake knows what to do. Parallelism is implicit and automatic. See the high-performance computing guide for all the details. # Use the spare cores on your local machine. options(clustermq.scheduler = &quot;multicore&quot;) make(plan, parallelism = &quot;clustermq&quot;, jobs = 4) # Or scale up to a supercomputer. drake_hpc_tmpl_file(&quot;slurm_clustermq.tmpl&quot;) # https://slurm.schedmd.com/ options( clustermq.scheduler = &quot;slurm&quot;, clustermq.template = &quot;slurm_clustermq.tmpl&quot; ) make(plan, parallelism = &quot;clustermq&quot;, jobs = 100) 1.6 With Docker drake and Docker are compatible and complementary. Here are some examples that run drake inside a Docker image. drake-gitlab-docker-example: A small pedagogical example workflow that leverages drake, Docker, GitLab, and continuous integration in a reproducible analysis pipeline. Created by Noam Ross. pleurosoriopsis: The workflow that supports Ebihara et al. 2019. “Growth Dynamics of the Independent Gametophytes of Pleurorosiopsis makinoi (Polypodiaceae)” Bulletin of the National Science Museum Series B (Botany) 45:77-86.. Created by Joel Nitta. Alternatively, it is possible to run drake outside Docker and use the future package to send targets to a Docker image. drake’s Docker-psock example demonstrates how. Download the code with drake_example(&quot;Docker-psock&quot;). 1.7 Documentation The main resources to learn drake are The user manual, which contains a friendly introduction and several long-form tutorials. The documentation website, which serves as a quicker reference. learndrake, an R package for teaching an extended drake workshop. It contains notebooks, slides, Shiny apps, the latter two of which are publicly deployed. See the README for instructions and links. drakeplanner, an R/Shiny app deployed to wlandau.shinyapps.io/drakeplanner. This app is an interactive tool for creating new drake-powered projects. If you have trouble accessing it, you can install it as a package and run it locally. 1.7.1 Frequently asked questions The FAQ page is an index of links to appropriately-labeled issues on GitHub. To contribute, please submit a new issue and ask that it be labeled as a frequently asked question. 1.7.2 Function reference The reference section lists all the available functions. Here are the most important ones. drake_plan(): create a workflow data frame (like my_plan). make(): build your project. drake_history(): show what you built, when you built it, and the function arguments you used. loadd(): load one or more built targets into your R session. readd(): read and return a built target. vis_drake_graph(): show an interactive visual network representation of your workflow. outdated(): see which targets will be built in the next make(). deps(): check the dependencies of a command or function. failed(): list the targets that failed to build in the last make(). diagnose(): return the full context of a build, including errors, warnings, and messages. 1.7.3 Tutorials Thanks to Kirill for constructing two interactive learnr tutorials: one supporting drake itself, and a prerequisite walkthrough of the cooking package. 1.7.4 Examples The official rOpenSci use cases and associated discussion threads describe applications of drake in action. Here are some more real-world sightings of drake in the wild. ecohealthalliance/drake-gitlab-docker-example efcaguab/demografia-del-voto efcaguab/great-white-shark-nsw IndianaCHE/Detailed-SSP-Reports joelnitta/pleurosoriopsis pat-s/pathogen-modeling sol-eng/tensorflow-w-r tiernanmartin/home-and-hope There are also multiple drake-powered example projects available here, ranging from beginner-friendly stubs to demonstrations of high-performance computing. You can generate the files for a project with drake_example() (e.g. drake_example(&quot;gsp&quot;)), and you can list the available projects with drake_examples(). You can contribute your own example project with a fork and pull request. 1.7.5 Presentations Author Venue Date Materials Matt Dray Bioinformatics London Meetup 2020-01-30 slides, source Matt Dray Coffee &amp; Coding, UK Dept for Transport 2019-10-02 slides Patrick Schratz whyR Conference 2019-09-27 workshop, slides, source Will Landau rOpenSci Community Calls 2019-09-24 Video recording and resource links Will Landau R/Pharma 2019 2019-08-21 slides, workspace, source Garrick Aden-Buie Bio-Data Club at Moffitt Cancer Center 2019-07-19 slides, workspace, source Tiernan Martin Cascadia R Conference 2019-06-08 slides Dominik Rafacz satRday Gdansk 2019-05-18 slides, source Amanda Dobbyn R-Ladies NYC 2019-02-12 slides, source Will Landau Harvard DataFest 2019-01-22 slides, source Karthik Ram RStudio Conference 2019-01-18 video, slides, resources Sina Rüeger Geneva R User Group 2018-10-04 slides, example code Will Landau R in Pharma 2018-08-16 video, slides, source Christine Stawitz R-Ladies Seattle 2018-06-25 materials Kirill Müller Swiss Institute of Bioinformatics 2018-03-05 workshop, slides, source, exercises 1.7.6 Context and history For context and history, check out this post on the rOpenSci blog and episode 22 of the R Podcast. 1.8 Help and troubleshooting The GitHub issue tracker is the best place to request help with your use case. Please search both open and closed ones before posting a new issue. Don’t be afraid to open a new issue, just please take 30 seconds to search for existing threads that could solve your problem. "],
["similar-work.html", "Chapter 2 Similar work 2.1 Pipeline tools 2.2 Memoization 2.3 Literate programming 2.4 Acknowledgements", " Chapter 2 Similar work drake enhances reproducibility and high-performance computing, but not in all respects. Literate programming, local library managers, containerization, and strict session managers offer more robust solutions in their respective domains. And for the problems drake does solve, it stands on the shoulders of the giants that came before. 2.1 Pipeline tools 2.1.1 GNU Make The original idea of a time-saving reproducible build system extends back at least as far as GNU Make, which still aids the work of data scientists as well as the original user base of compiled language programmers. In fact, the name “drake” stands for “Data Frames in R for Make”. Make is used widely in reproducible research. Below are some examples from Karl Broman’s website. Bostock, Mike (2013). “A map of flowlines from NHDPlus.” https://github.com/mbostock/us-rivers. Powered by the Makefile at https://github.com/mbostock/us-rivers/blob/master/Makefile. Broman, Karl W (2012). “Halotype Probabilities in Advanced Intercross Populations.” G3 2(2), 199-202.Powered by the Makefile at https://github.com/kbroman/ailProbPaper/blob/master/Makefile. Broman, Karl W (2012). “Genotype Probabilities at Intermediate Generations in the Construction of Recombinant Inbred Lines.” *Genetics 190(2), 403-412. Powered by the Makefile at https://github.com/kbroman/preCCProbPaper/blob/master/Makefile. Broman, Karl W and Kim, Sungjin and Sen, Saunak and Ane, Cecile and Payseur, Bret A (2012). “Mapping Quantitative Trait Loci onto a Phylogenetic Tree.” Genetics 192(2), 267-279. Powered by the Makefile at https://github.com/kbroman/phyloQTLpaper/blob/master/Makefile. Whereas GNU Make is language-agnostic, drake is fundamentally designed for R. Instead of a Makefile, drake supports an R-friendly domain-specific language for declaring targets. Targets in GNU Make are files, whereas targets in drake are arbitrary variables in memory. (drake does have opt-in support for files via file_out(), file_in(), and knitr_in().) drake caches these objects in its own storage system so R users rarely have to think about output files. 2.1.2 Remake remake itself is no longer maintained, but its founding design goals and principles live on through drake. In fact, drake is a direct reimagining of remake with enhanced scalability, reproducibility, high-performance computing, visualization, and documentation. 2.1.3 Factual’s Drake Factual’s Drake is similar in concept, but the development effort is completely unrelated to the drake R package. 2.1.4 Other pipeline tools There are countless other successful pipeline toolkits. The drake package distinguishes itself with its R-focused approach, Tidyverse-friendly interface, and a thorough selection of parallel computing technologies and scheduling algorithms. 2.2 Memoization Memoization is the strategic caching of the return values of functions. It is a lightweight approach to the core problem that drake and other pipeline tools are trying to solve. Every time a memoized function is called with a new set of arguments, the return value is saved for future use. Later, whenever the same function is called with the same arguments, the previous return value is salvaged, and the function call is skipped to save time. The memoise package is the primary implementation of memoization in R. Memoization saves time for small projects, but it arguably does not go far enough for large reproducible pipelines. In reality, the return value of a function depends not only on the function body and the arguments, but also on any nested functions and global variables, the dependencies of those dependencies, and so on upstream. drake tracks this deeper context, while memoise does not. 2.3 Literate programming Literate programming is the practice of narrating code in plain vernacular. The goal is to communicate the research process clearly, transparently, and reproducibly. Whereas commented code is still mostly code, literate knitr / R Markdown reports can become websites, presentation slides, lecture notes, serious scientific manuscripts, and even books. 2.3.1 knitr and R Markdown drake and knitr are symbiotic. drake’s job is to manage large computation and orchestrate the demanding tasks of a complex data analysis pipeline. knitr’s job is to communicate those expensive results after drake computes them. knitr / R Markdown reports are small pieces of an overarching drake pipeline. They should focus on communication, and they should do as little computation as possible. To insert a knitr report in a drake pipeline, use the knitr_in() function inside your drake plan, and use loadd() and readd() to refer to targets in the report itself. See an example here. 2.3.2 Version control drake is not a version control tool. However, it is fully compatible with git, svn, and similar software. In fact, it is good practice to use git alongside drake for reproducible workflows. However, data poses a challenge. The datasets created by make() can get large and numerous, and it is not recommended to put the .drake/ cache or the .drake_history/ logs under version control. Instead, it is recommended to use a data storage solution such as DropBox or OSF. 2.3.3 Containerization and R package environments drake does not track R packages or system dependencies for changes. Instead, it defers to tools like Docker, Singularity, renv, and packrat, which create self-contained portable environments to reproducibly isolate and ship data analysis projects. drake is fully compatible with these tools. 2.3.4 workflowr The workflowr package is a project manager that focuses on literate programming, sharing over the web, file organization, and version control. Its brand of reproducibility is all about transparency, communication, and discoverability. For an example of workflowr and drake working together, see this machine learning project by Patrick Schratz (source). 2.4 Acknowledgements Special thanks to Jarad Niemi, my advisor from graduate school, for first introducing me to the idea of Makefiles for research. He originally set me down the path that led to drake. Many thanks to Julia Lowndes, Ben Marwick, and Peter Slaughter for reviewing drake for rOpenSci, and to Maëlle Salmon for such active involvement as the editor. Thanks also to the following people for contributing early in development. Alex Axthelm Chan-Yub Park Daniel Falster Eric Nantz Henrik Bengtsson Ian Watson Jasper Clarkberg Kendon Bell Kirill Müller Credit for images is attributed here. "],
["walkthrough.html", "Chapter 3 Walkthrough 3.1 Set the stage. 3.2 Make your results. 3.3 Go back and fix things. 3.4 History and provenance 3.5 Reproducible data recovery and renaming 3.6 Try the code yourself! 3.7 Thanks", " Chapter 3 Walkthrough A typical data analysis workflow is a sequence of data transformations. Raw data becomes tidy data, then turns into fitted models, summaries, and reports. Other analyses are usually variations of this pattern, and drake can easily accommodate them. 3.1 Set the stage. To set up a project, load your packages, library(drake) library(dplyr) library(ggplot2) load your custom functions, create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram() } check any supporting files (optional), ## Get the files with drake_example(&quot;main&quot;). file.exists(&quot;raw_data.xlsx&quot;) #&gt; [1] TRUE file.exists(&quot;report.Rmd&quot;) #&gt; [1] TRUE and plan what you are going to do. plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)), hist = create_plot(data), fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) plan #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 raw_data readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) … #&gt; 2 data raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … #&gt; 3 hist create_plot(data) … #&gt; 4 fit lm(Sepal.Width ~ Petal.Width + Species, data) … #&gt; 5 report rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;re… Optionally, visualize your workflow to make sure you set it up correctly. The graph is interactive, so you can click, drag, hover, zoom, and explore. vis_drake_graph(plan) 3.2 Make your results. So far, we have just been setting the stage. Use make() to do the real work. Targets are built in the correct order regardless of the row order of plan. make(plan) #&gt; target raw_data #&gt; target data #&gt; target fit #&gt; target hist #&gt; target report Except for output files like report.html, your output is stored in a hidden .drake/ folder. Reading it back is easy. readd(data) %&gt;% # See also loadd(). head() #&gt; # A tibble: 6 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa The graph shows everything up to date. vis_drake_graph(plan) 3.3 Go back and fix things. You may look back on your work and see room for improvement, but it’s all good! The whole point of drake is to help you go back and change things quickly and painlessly. For example, we forgot to give our histogram a bin width. readd(hist) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. So let’s fix the plotting function. create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram(binwidth = 0.25) + theme_gray(20) } drake knows which results are affected. vis_drake_graph(plan) The next make() just builds hist and report. No point in wasting time on the data or model. make(plan) #&gt; target hist #&gt; target report loadd(hist) hist 3.4 History and provenance As of version 7.5.2, drake tracks the history and provenance of your targets: what you built, when you built it, how you built it, the arguments you used in your function calls, and how to get the data back. history &lt;- drake_history(analyze = TRUE) history #&gt; # A tibble: 7 x 10 #&gt; target current built exists hash command seed runtime quiet output_file #&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; #&gt; 1 data TRUE 2020-… TRUE e580… &quot;raw_dat… 1.29e9 0.002 NA &lt;NA&gt; #&gt; 2 fit TRUE 2020-… TRUE 66b5… &quot;lm(Sepa… 1.11e9 0.00300 NA &lt;NA&gt; #&gt; 3 hist FALSE 2020-… TRUE 607e… &quot;create_… 2.10e8 0.009 NA &lt;NA&gt; #&gt; 4 hist TRUE 2020-… TRUE d447… &quot;create_… 2.10e8 0.00500 NA &lt;NA&gt; #&gt; 5 raw_da… TRUE 2020-… TRUE 6317… &quot;readxl:… 1.20e9 0.00900 NA &lt;NA&gt; #&gt; 6 report TRUE 2020-… TRUE 99c4… &quot;rmarkdo… 1.30e9 0.631 TRUE report.html #&gt; 7 report TRUE 2020-… TRUE 99c4… &quot;rmarkdo… 1.30e9 0.505 TRUE report.html Remarks: The quiet column appears above because one of the drake_plan() commands has knit(quiet = TRUE). The hash column identifies all the previous the versions of your targets. As long as exists is TRUE, you can recover old data. Advanced: if you use make(cache_log_file = TRUE) and put the cache log file under version control, you can match the hashes from drake_history() with the git commit history of your code. Let’s use the history to recover the oldest histogram. hash &lt;- history %&gt;% filter(target == &quot;hist&quot;) %&gt;% pull(hash) %&gt;% head(n = 1) cache &lt;- drake_cache() cache$get_value(hash) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 3.5 Reproducible data recovery and renaming Remember how we made that change to our histogram? What if we want to change it back? If we revert create_plot(), make(plan, recover = TRUE) restores the original plot. create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram() } # The report still needs to run in order to restore report.html. make(plan, recover = TRUE) #&gt; unload targets from environment: #&gt; hist #&gt; recover hist #&gt; target report readd(hist) # old histogram #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. drake’s data recovery feature is another way to avoid rerunning commands. It is useful if: You want to revert to your old code, maybe with git reset. You accidentally clean()ed a target and you want to get it back. You want to rename an expensive target. In version 7.5.2 and above, make(recover = TRUE) can salvage the values of old targets. Before building a target, drake checks if you have ever built something else with the same command, dependencies, seed, etc. that you have right now. If appropriate, drake assigns the old value to the new target instead of rerunning the command. Caveats: This feature is still experimental. Recovery may not be a good idea if your external dependencies have changed a lot over time (R version, package environment, etc.). 3.5.1 Undoing clean() # Is the data really gone? clean() # garbage_collection = FALSE # Nope! make(plan, recover = TRUE) # The report still builds since report.md is gone. #&gt; recover raw_data #&gt; recover data #&gt; recover fit #&gt; recover hist #&gt; recover report # When was the raw data *really* first built? diagnose(raw_data)$date #&gt; [1] &quot;2020-02-06 01:37:26.603567 +0000 GMT&quot; 3.5.2 Renaming You can use recovery to rename a target. The trick is to supply the random number generator seed that drake used with the old target name. Also, renaming a target unavoidably invalidates downstream targets. # Get the old seed. old_seed &lt;- diagnose(data)$seed # Now rename the data and supply the old seed. plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), # Previously just named &quot;data&quot;. iris_data = target( raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)), seed = !!old_seed ), # `iris_data` will be recovered from `data`, # but `hist` and `fit` have changed commands, # so they will build from scratch. hist = create_plot(iris_data), fit = lm(Sepal.Width ~ Petal.Width + Species, iris_data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) make(plan, recover = TRUE) #&gt; recover iris_data #&gt; target fit #&gt; target hist #&gt; target report 3.6 Try the code yourself! Use drake_example(&quot;main&quot;) to download the code files for this example. 3.7 Thanks Thanks to Kirill Müller for originally providing this example. "],
["plans.html", "Chapter 4 drake plans 4.1 Functions 4.2 Intro to plans 4.3 How to choose good targets 4.4 Special data formats for targets 4.5 Special columns 4.6 External files and directories 4.7 Large plans", " Chapter 4 drake plans Most data analysis workflows consist of several steps, such as data cleaning, model fitting, visualization, and reporting. A drake plan is the high-level catalog of all these steps for a single workflow. It is the centerpiece of every drake-powered project, and it is always required. However, the plan is almost never the first thing we write. A typical plan rests on a foundation of carefully-crafted custom functions. 4.1 Functions A function is a reusable instruction that accepts some inputs and returns a single output. After we define a function once, we can easily call it any number of times. root_square_term &lt;- function(l, w, h) { half_w &lt;- w / 2 l * sqrt(half_w ^ 2 + h ^ 2) } root_square_term(1, 2, 3) #&gt; [1] 3.162278 root_square_term(4, 5, 6) #&gt; [1] 26 In practice, functions are vocabulary. They are concise references to complicated ideas, and they help us write instructions of ever increasing complexity. # right rectangular pyramid volume_pyramid &lt;- function(length_base, width_base, height) { area_base &lt;- length_base * width_base term1 &lt;- root_square_term(length_base, width_base, height) term2 &lt;- root_square_term(width_base, length_base, height) area_base + term1 + term2 } volume_pyramid(3, 5, 7) #&gt; [1] 73.09366 The root_square_term() function is custom shorthand that makes volume_pyramid() easier to write and understand. volume_pyramid(), in turn, helps us crudely approximate the total square meters of stone eroded from the Great Pyramid of Giza (dimensions from Wikipedia). volume_original &lt;- volume_pyramid(230.4, 230.4, 146.5) volume_current &lt;- volume_pyramid(230.4, 230.4, 138.8) volume_original - volume_current # volume eroded #&gt; [1] 2760.183 This function-oriented code is concise and clear. Contrast it with the cumbersome mountain of imperative arithmetic that would have otherwise daunted us. # Don&#39;t try this at home! width_original &lt;- 230.4 length_original &lt;- 230.4 height_original &lt;- 146.5 # We supply the same lengths and widths, # but we use different variable names # to illustrate the general case. width_current &lt;- 230.4 length_current &lt;- 230.4 height_current &lt;- 138.8 area_original &lt;- length_original * width_original term1_original &lt;- length_original * sqrt((width_original / 2) ^ 2 + height_original ^ 2) term2_original &lt;- width_original * sqrt((length_original / 2) ^ 2 + height_original ^ 2) volume_original &lt;- area_original + term1_original + term2_original area_current &lt;- length_current * width_current term1_current &lt;- length_current * sqrt((width_current / 2) ^ 2 + height_current ^ 2) term2_current &lt;- width_current * sqrt((length_current / 2) ^ 2 + height_current ^ 2) volume_current &lt;- area_current + term1_current + term2_current volume_original - volume_current # volume eroded #&gt; [1] 2760.183 Unlike imperative scripts, functions break down complex ideas into manageable pieces, and they gradually build up bigger and bigger pieces until an elegant solution materializes. This process of building up functions helps us think clearly, understand what we are doing, and explain our methods to others. 4.2 Intro to plans A drake plan is a data frame with columns named target and command. Each row represents a step in the workflow. Each command is a concise expression that makes use of our functions, and each target is the return value of the command. (The target column has the names of the targets, not the values.) We create plans with the drake_plan() function. plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)), hist = create_plot(data), fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) plan #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 raw_data readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) … #&gt; 2 data raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … #&gt; 3 hist create_plot(data) … #&gt; 4 fit lm(Sepal.Width ~ Petal.Width + Species, data) … #&gt; 5 report rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;re… The plan makes use of a custom create_plot() function to produce target hist. Functions make the plan more concise and easier to read. create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram(binwidth = 0.1) + theme_gray(20) } drake automatically understands the relationships among targets in the plan. It knows data depends on raw_data because the symbol raw_data is mentioned in the command for data. drake represents this dependency relationship with an arrow from raw_data to data in the graph. vis_drake_graph(plan) We can write the targets in any order and drake still understands the dependency relationships. plan &lt;- drake_plan( report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ), fit = lm(Sepal.Width ~ Petal.Width + Species, data), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)), hist = create_plot(data), raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) ) vis_drake_graph(plan) The make() function runs the correct targets in the correct order and stores the results in a hidden cache. library(drake) library(glue) library(purrr) library(rlang) library(tidyverse) make(plan) #&gt; target raw_data #&gt; target data #&gt; target fit #&gt; target hist #&gt; target report readd(hist) The purpose of the plan is to identify steps we can skip in our workflow. If we change some code or data, drake saves time by running some steps and skipping others. create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram(binwidth = 0.25) + # new bin width theme_gray(20) } vis_drake_graph(plan) make(plan) #&gt; target hist #&gt; target report readd(hist) 4.3 How to choose good targets Defining good targets is more of an art than a science, and it requires personal judgement and context specific to your use case. Generally speaking, a good target is Long enough to eat up a decent chunk of runtime, and Small enough that make() frequently skips it, and Meaningful to your project, and A well-behaved R object compatible with saveRDS(). For example, data frames behave better than database connection objects (discussions here and here), R6 classes, and xgboost matrices. Above, “long” and “short” refer to computational runtime, not the size of the target’s value. The more data you return to the targets, the more data drake puts in storage, and the slower your workflow becomes. If you have a large dataset, it may not be wise to copy it over several targets. bad_plan &lt;- drake_plan( raw = get_big_raw_dataset(), # We write this ourselves. selection = select(raw, column1, column2), filtered = filter(selection, column3 == &quot;abc&quot;), analysis = my_analysis_function(filtered) # Same here. ) In the above sketch, the dataset is super large, and selection and filtering are fast by comparison. It is much better to wrap up these steps in a data cleaning function and reduce the number of targets. munged_dataset &lt;- function() { get_big_raw_dataset() %&gt;% select(column1, column2) %&gt;% filter(column3 == &quot;abc&quot;) } good_plan &lt;- drake_plan( dataset = munged_dataset(), analysis = my_analysis_function(dataset) ) 4.4 Special data formats for targets drake supports custom formats for saving and loading large objects and highly specialized objects. For example, the &quot;fst&quot; and &quot;fst_tbl&quot; formats use the fst package to save data.frame and tibble targets faster. Simply enclose the command and the format together with the target() function. library(drake) n &lt;- 1e8 # Each target is 1.6 GB in memory. plan &lt;- drake_plan( data_fst = target( data.frame(x = runif(n), y = runif(n)), format = &quot;fst&quot; ), data_old = data.frame(x = runif(n), y = runif(n)) ) make(plan) #&gt; target data_fst #&gt; target data_old build_times(type = &quot;build&quot;) #&gt; # A tibble: 2 x 4 #&gt; target elapsed user system #&gt; &lt;chr&gt; &lt;Duration&gt; &lt;Duration&gt; &lt;Duration&gt; #&gt; 1 data_fst 13.93s 37.562s 7.954s #&gt; 2 data_old 184s (~3.07 minutes) 177s (~2.95 minutes) 4.157s There are several formats, each with their own system requirements. These system requirements, such as the fst R package for the &quot;fst&quot; format, do not come pre-installed with drake. You will need to install them manually. &quot;fst&quot;: save big data frames fast. Requires the fst package. Note: this format strips non-data-frame attributes such as the &quot;fst_tbl&quot;: Like &quot;fst&quot;, but for tibble objects. Requires the fst and tibble packages. Strips away non-data-frame non-tibble attributes. &quot;fst_dt&quot;: Like &quot;fst&quot; format, but for data.table objects. Requires the fst and data.table packages. Strips away non-data-frame non-data-table attributes. &quot;diskframe&quot;: Stores disk.frame objects, which could potentially be larger than memory. Requires the fst and disk.frame packages. Coerces objects to disk.frames. Note: disk.frame objects get moved to the drake cache (a subfolder of .drake/ for most workflows). To ensure this data transfer is fast, it is best to save your disk.frame objects to the same physical storage drive as the drake cache, as.disk.frame(your_dataset, outdir = drake_tempfile()). &quot;keras&quot;: save Keras models as HDF5 files. Requires the keras package. &quot;qs&quot;: save any R object that can be properly serialized with the qs package. Requires the qs package. Uses qsave() and qread(). Uses the default settings in qs version 0.20.2. &quot;rds&quot;: save any R object that can be properly serialized. Requires R version &gt;= 3.5.0 due to ALTREP. Note: the &quot;rds&quot; format uses gzip compression, which is slow. &quot;qs&quot; is a superior format. 4.5 Special columns With target(), you can define any kind of special column in the plan. drake_plan( x = target((1 + sqrt(5)) / 2, golden = &quot;ratio&quot;), y = target(pi * 3 ^ 2, area = &quot;circle&quot;) ) #&gt; # A tibble: 2 x 4 #&gt; target command golden area #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 x (1 + sqrt(5))/2 ratio &lt;NA&gt; #&gt; 2 y pi * 3^2 &lt;NA&gt; circle The following columns have special meanings, and make() reads and interprets them. format: already described above. dynamic: See the chapter on dynamic branching. transform: Automatically processed by drake_plan() except for drake_plan(transform = FALSE). See the chapter on static branching. trigger: rule to decide whether a target needs to run. See the trigger chapter to learn more. elapsed and cpu: number of seconds to wait for the target to build before timing out (elapsed for elapsed time and cpu for CPU time). hpc: logical values (TRUE/FALSE/NA) whether to send each target to parallel workers. Click here to learn more. resources: target-specific lists of resources for a computing cluster. See the advanced options in the parallel computing chapter for details. caching: overrides the caching argument of make() for each target individually. Only supported in drake version 7.6.1.9000 and above. Possible values: “master”: tell the master process to store the target in the cache. “worker”: tell the HPC worker to store the target in the cache. NA: default to the caching argument of make(). retries: number of times to retry building a target in the event of an error. seed: pseudo-random number generator (RNG) seed for each target. drake usually computes its own unique reproducible target-specific seeds using the target name and the global seed (the seed argument of make()). Any non-missing seeds in the seed column override drake’s default target seeds. 4.6 External files and directories drake has special functions to declare relationships between targets and external storage on disk. file_in() is for input files and directories, file_out() is for output files and directories, and knitr_in() is for R Markdown reports and knitr source files. If you use one of these functions inline in the plan, it tells drake to rerun a target when a file changes (or any of the files in a directory). All three functions appear in this plan. plan #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 report rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;re… #&gt; 2 fit lm(Sepal.Width ~ Petal.Width + Species, data) … #&gt; 3 data raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … #&gt; 4 hist create_plot(data) … #&gt; 5 raw_data readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) … If we break the file_out() file, drake automatically repairs it. unlink(&quot;report.html&quot;) make(plan) #&gt; target report file.exists(&quot;report.html&quot;) #&gt; [1] TRUE As for knitr_in(), recall what happened when we changed the create_plot(). Not only did hist rerun, report ran as well. Why? Because knitr_in() is special. It tells drake to look for mentions of loadd() and readd() in the code chunks. drake finds the targets you mention in those loadd() and readd() calls and treats them as dependencies of the report. This lets you choose to run the report either inside or outside a drake pipeline. cat(readLines(&quot;report.Rmd&quot;), sep = &quot;\\n&quot;) #&gt; --- #&gt; title: &quot;Example R Markdown drake file target&quot; #&gt; author: Will Landau and Kirill Müller #&gt; output: html_document #&gt; --- #&gt; #&gt; Run `make.R` to generate the output `report.pdf` and its dependencies. Because we use `loadd()` and `readd()` below, `drake` knows `report.pdf` depends on targets `fit`, and `hist`. #&gt; #&gt; ```{r content} #&gt; library(drake) #&gt; loadd(fit) #&gt; print(fit) #&gt; readd(hist) #&gt; ``` #&gt; #&gt; More: #&gt; #&gt; - Walkthrough: [this chapter of the user manual](https://books.ropensci.org/drake/walkthrough.html) #&gt; - Code: `drake_example(&quot;main&quot;)` That is why we have an arrow from hist to report in the graph. vis_drake_graph(plan) 4.6.1 URLs file_in() understands URLs. If you supply a string beginning with http://, https://, or ftp://, drake watches the HTTP ETag, file size, and timestamp for changes. drake_plan( external_data = download.file(file_in(&quot;http://example.com/file.zip&quot;)) ) #&gt; # A tibble: 1 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 external_data download.file(file_in(&quot;http://example.com/file.zip&quot;)) 4.6.2 Limitations of file tracking 4.6.2.1 Paths must be literal strings file_in(), file_out(), and knitr_in() require you to mention file and directory names explicitly. You cannot use a variable containing the name of a file. bad_plan &lt;- drake_plan( file = &quot;my_data.csv&quot;, data = read_csv(file_in(file)) ) vis_drake_graph(bad_plan) Instead, write this: good_plan &lt;- drake_plan( file = read_csv(file_in(&quot;my_data.csv&quot;)) ) vis_drake_graph(good_plan) 4.6.2.2 Database connections file_in() and friends do not help us manage database connections. If you work with a database, the most general best practice is to always trigger a snapshot to make sure you have the latest data. plan &lt;- drake_plan( data = target( get_data_from_db(&quot;my_table&quot;), # Define yourself. trigger = trigger(condition = TRUE) # Always runs. ), preprocess = my_preprocessing(data) # Runs when the data change. ) In specific use cases, you may be able to watch database metadata for changes, but this information is situation-specific. library(DBI) # Connection objects are brittle, so they should not be targets. # We define them up front, and we use ignore() to prevent # drake from rerunning targets when the connection object changes. con &lt;- dbConnect(...) plan &lt;- drake_plan( data = target( dbReadTable(ignore(con), &quot;my_table&quot;), # Use ignore() for db connection objects. trigger = trigger(change = somehow_get_db_timestamp()) # Define yourself. ), preprocess = my_preprocessing(data) # runs when the data change ) 4.7 Large plans drake has special interfaces to concisely define large numbers of targets. See the chapters on static branching and dynamic branching for details. "],
["static.html", "Chapter 5 Static branching 5.1 Why static branching? 5.2 Grouping variables 5.3 Tidy evaluation 5.4 Static transformations 5.5 Tags", " Chapter 5 Static branching 5.1 Why static branching? Static branching helps us write large plans compactly. Instead of typing out every single target by hand, we use a special shorthand to declare entire batches of similar targets. To practice static branching in a controlled setting, try the interactive exercises at https://wlandau.shinyapps.io/learndrakeplans (from the workshop at https://github.com/wlandau/learndrake). Without static branching, plans like this one become too cumbersome to type by hand. # Without static branching: drake_plan( data = get_data(), analysis_fast_1_main = main(data, mean = 1, tuning = &quot;fast&quot;), analysis_slow_1_main = main(data, mean = 1, tuning = &quot;slow&quot;), analysis_fast_2_main = main(data, mean = 2, tuning = &quot;fast&quot;), analysis_slow_2_main = main(data, mean = 2, tuning = &quot;slow&quot;), analysis_fast_3_main = main(data, mean = 3, tuning = &quot;fast&quot;), analysis_slow_3_main = main(data, mean = 3, tuning = &quot;slow&quot;), analysis_fast_4_main = main(data, mean = 4, tuning = &quot;fast&quot;), analysis_slow_4_main = main(data, mean = 4, tuning = &quot;slow&quot;), analysis_fast_1_altv = altv(data, mean = 1, tuning = &quot;fast&quot;), analysis_slow_1_altv = altv(data, mean = 1, tuning = &quot;slow&quot;), analysis_fast_2_altv = altv(data, mean = 2, tuning = &quot;fast&quot;), analysis_slow_2_altv = altv(data, mean = 2, tuning = &quot;slow&quot;), analysis_fast_3_altv = altv(data, mean = 3, tuning = &quot;fast&quot;), analysis_slow_3_altv = altv(data, mean = 3, tuning = &quot;slow&quot;), analysis_fast_4_altv = altv(data, mean = 4, tuning = &quot;fast&quot;), analysis_slow_4_altv = altv(data, mean = 4, tuning = &quot;slow&quot;), summary_analysis_fast_1_main = summarize_model(analysis_fast_1_main), summary_analysis_slow_1_main = summarize_model(analysis_slow_1_main), summary_analysis_fast_2_main = summarize_model(analysis_fast_2_main), summary_analysis_slow_2_main = summarize_model(analysis_slow_2_main), summary_analysis_fast_3_main = summarize_model(analysis_fast_3_main), summary_analysis_slow_3_main = summarize_model(analysis_slow_3_main), summary_analysis_fast_4_main = summarize_model(analysis_fast_4_main), summary_analysis_slow_4_main = summarize_model(analysis_slow_4_main), summary_analysis_fast_1_altv = summarize_model(analysis_fast_1_altv), summary_analysis_slow_1_altv = summarize_model(analysis_slow_1_altv), summary_analysis_fast_2_altv = summarize_model(analysis_fast_2_altv), summary_analysis_slow_2_altv = summarize_model(analysis_slow_2_altv), summary_analysis_fast_3_altv = summarize_model(analysis_fast_3_altv), summary_analysis_slow_3_altv = summarize_model(analysis_slow_3_altv), summary_analysis_fast_4_altv = summarize_model(analysis_fast_4_altv), summary_analysis_slow_4_altv = summarize_model(analysis_slow_4_altv), model_summary_altv = dplyr::bind_rows( summary_analysis_fast_1_altv, summary_analysis_slow_1_altv, summary_analysis_fast_2_altv, summary_analysis_slow_2_altv, summary_analysis_fast_3_altv, summary_analysis_slow_3_altv, summary_analysis_fast_4_altv, summary_analysis_slow_4_altv ), model_summary_main = dplyr::bind_rows( summary_analysis_fast_1_main, summary_analysis_slow_1_main, summary_analysis_fast_2_main, summary_analysis_slow_2_main, summary_analysis_fast_3_main, summary_analysis_slow_3_main, summary_analysis_fast_4_main, summary_analysis_slow_4_main ) ) Static branching makes it easier to write and understand plans. To activate static branching, use the transform argument of target(). # With static branching: model_functions &lt;- rlang::syms(c(&quot;main&quot;, &quot;altv&quot;)) # We need symbols. model_functions # List of symbols. #&gt; [[1]] #&gt; main #&gt; #&gt; [[2]] #&gt; altv plan &lt;- drake_plan( data = get_data(), analysis = target( model_function(data, mean = mean_value, tuning = tuning_setting), # Define an analysis target for each combination of # tuning_setting, mean_value, and model_function. transform = cross( tuning_setting = c(&quot;fast&quot;, &quot;slow&quot;), mean_value = !!(1:4), # Why `!!`? See &quot;Tidy Evaluation&quot; below. model_function = !!model_functions # Why `!!`? See &quot;Tidy Evaluation&quot; below. ) ), # Define a new summary target for each analysis target defined previously. summary = target( summarize_model(analysis), transform = map(analysis) ), # Group together the summary targets by the corresponding value # of model_function. model_summary = target( dplyr::bind_rows(summary), transform = combine(summary, .by = model_function) ) ) plan #&gt; # A tibble: 35 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data get_data() #&gt; 2 analysis_fast_1L_main main(data, mean = 1L, tuning = &quot;fast&quot;) #&gt; 3 analysis_slow_1L_main main(data, mean = 1L, tuning = &quot;slow&quot;) #&gt; 4 analysis_fast_2L_main main(data, mean = 2L, tuning = &quot;fast&quot;) #&gt; 5 analysis_slow_2L_main main(data, mean = 2L, tuning = &quot;slow&quot;) #&gt; 6 analysis_fast_3L_main main(data, mean = 3L, tuning = &quot;fast&quot;) #&gt; 7 analysis_slow_3L_main main(data, mean = 3L, tuning = &quot;slow&quot;) #&gt; 8 analysis_fast_4L_main main(data, mean = 4L, tuning = &quot;fast&quot;) #&gt; 9 analysis_slow_4L_main main(data, mean = 4L, tuning = &quot;slow&quot;) #&gt; 10 analysis_fast_1L_altv altv(data, mean = 1L, tuning = &quot;fast&quot;) #&gt; # … with 25 more rows Always check the graph to make sure the plan makes sense. plot(plan) # a quick and dirty alternative to vis_drake_graph() If the graph is too complicated to look at or too slow to load, downsize the plan with max_expand. Then, when you are done debugging and testing, remove max_expand to scale back up to the full plan. model_functions &lt;- rlang::syms(c(&quot;main&quot;, &quot;altv&quot;)) plan &lt;- drake_plan( max_expand = 2, data = get_data(), analysis = target( model_function(data, mean = mean_value, tuning = tuning_setting), transform = cross( tuning_setting = c(&quot;fast&quot;, &quot;slow&quot;), mean_value = !!(1:4), # Why `!!`? See &quot;Tidy Evaluation&quot; below. model_function = !!model_functions # Why `!!`? See &quot;Tidy Evaluation&quot; below. ) ), summary = target( summarize_model(analysis), transform = map(analysis) ), model_summary = target( dplyr::bind_rows(summary), transform = combine(summary, .by = model_function) # defined in &quot;analysis&quot; ) ) # Click and drag the nodes in the graph to improve the view. plot(plan) 5.2 Grouping variables A grouping variable contains iterated values for a single instance of map() or cross(). mean_value and tuning_par are grouping variables below. Notice how they are defined inside cross(). Grouping variables are not targets, and they must be declared inside static transformations. drake_plan( data = get_data(), model = target( fit_model(data, mean_value, tuning_par), transform = cross( mean_value = c(1, 2), tuning_par = c(&quot;fast&quot;, &quot;slow&quot;) ) ) ) #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data get_data() #&gt; 2 model_1_fast fit_model(data, 1, &quot;fast&quot;) #&gt; 3 model_2_fast fit_model(data, 2, &quot;fast&quot;) #&gt; 4 model_1_slow fit_model(data, 1, &quot;slow&quot;) #&gt; 5 model_2_slow fit_model(data, 2, &quot;slow&quot;) Each model has its own mean_value and tuning_par. To see this correspondence, set trace = TRUE. drake_plan( trace = TRUE, data = get_data(), model = target( fit_model(data, mean_value, tuning_par), transform = cross( mean_value = c(1, 2), tuning_par = c(&quot;fast&quot;, &quot;slow&quot;) ) ) ) #&gt; # A tibble: 5 x 5 #&gt; target command mean_value tuning_par model #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 data get_data() &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 model_1_fast fit_model(data, 1, &quot;fast&quot;) 1 &quot;\\&quot;fast\\&quot;&quot; model_1_fast #&gt; 3 model_2_fast fit_model(data, 2, &quot;fast&quot;) 2 &quot;\\&quot;fast\\&quot;&quot; model_2_fast #&gt; 4 model_1_slow fit_model(data, 1, &quot;slow&quot;) 1 &quot;\\&quot;slow\\&quot;&quot; model_1_slow #&gt; 5 model_2_slow fit_model(data, 2, &quot;slow&quot;) 2 &quot;\\&quot;slow\\&quot;&quot; model_2_slow If we summarize those models, each summary has its own mean_value and tuning_par. In other words, grouping variables have a natural nesting, and they propagate forward so we can use them in downstream targets. Notice how mean_value and tuning_par appear in summarize_model() and combine() below. plan &lt;- drake_plan( trace = TRUE, data = get_data(), model = target( fit_model(data, mean_value, tuning_par), transform = cross( mean_value = c(1, 2), tuning_par = c(&quot;fast&quot;, &quot;slow&quot;) ) ), summary = target( # mean_value and tuning_par are old grouping variables from the models summarize_model(model, mean_value, tuning_par), transform = map(model) ), summary_by_tuning = target( dplyr::bind_rows(summary), # tuning_par is an old grouping variable from the models. transform = combine(summary, .by = tuning_par) ) ) plot(plan) 5.2.1 Limitations of grouping variables The forward propagation of grouping variables is powerful, but it can easily go wrong. You should Define each grouping variable only once, and Propagate the grouping variable in a direct linear downstream path. In the following plan, we violate both (1) and (2). We violate (1) because we define a1 twice: once for b1 and again for b2. And we violate (2) because we bypass b1 andb2 when we propagate a1 to c1. a2 and a3 have the same problems, and when we put it all together, drake does not know how to plan c1. drake_plan( b1 = target(1, transform = map(a1 = 1, a2 = 1, .id = FALSE)), b2 = target(1, transform = map(a1 = 1, a3 = 1, .id = FALSE)), b3 = target(1, transform = map(a2 = 1, a3 = 1, .id = FALSE)), c1 = target(1, transform = map(a1, a2, a3, .id = FALSE)), trace = TRUE ) #&gt; Warning in min(vapply(out, length, FUN.VALUE = integer(1))): no non-missing #&gt; arguments to min; returning Inf #&gt; Warning: A grouping variable for target &#39;c1&#39; is either undefined or improperly #&gt; invoked. Transformation skipped and target deleted. To read about grouping #&gt; variables and their limitations, please visit https://books.ropensci.org/drake/ #&gt; static.html#grouping-variables #&gt; # A tibble: 3 x 8 #&gt; target command a1 a2 b1 a3 b2 b3 #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 b1 1 1 1 b1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 b2 1 1 &lt;NA&gt; &lt;NA&gt; 1 b2 &lt;NA&gt; #&gt; 3 b3 1 &lt;NA&gt; 1 &lt;NA&gt; 1 &lt;NA&gt; b3 Workarounds include bind_plans() (on separate sub-plans) and dynamic branching. Regardless, you must still check, inspect, and visualize plans before running them. 5.3 Tidy evaluation In earlier plans, we used “bang-bang” operator !! from tidy evaluation, e.g. model_function = !!model_functions in cross(). But why? Why not just type model_function = model_functions? Consider the following incorrect plan. model_functions &lt;- rlang::syms(c(&quot;main&quot;, &quot;altv&quot;)) plan &lt;- drake_plan( data = get_data(), analysis = target( model_function(data, mean = mean_value, tuning = tuning_setting), transform = cross( tuning_setting = c(&quot;fast&quot;, &quot;slow&quot;), mean_value = 1:4, # without !! model_function = model_functions # without !! ) ) ) drake_plan_source(plan) #&gt; drake_plan( #&gt; data = get_data(), #&gt; analysis_fast_1_model_functions = model_functions(data, mean = 1, tuning = &quot;fast&quot;), #&gt; analysis_slow_1_model_functions = model_functions(data, mean = 1, tuning = &quot;slow&quot;), #&gt; analysis_fast_4_model_functions = model_functions(data, mean = 4, tuning = &quot;fast&quot;), #&gt; analysis_slow_4_model_functions = model_functions(data, mean = 4, tuning = &quot;slow&quot;) #&gt; ) Because we omit !!, we create two problems: The commands use model_functions() instead of the desired main() and altv(). We are missing the targets with mean = 2 and mean = 3. Why? To make static branching work properly, drake does not actually evaluate the arguments to cross(). It just uses the raw symbols and expressions. To force drake to use the values instead, we need !!. model_functions &lt;- rlang::syms(c(&quot;main&quot;, &quot;altv&quot;)) plan &lt;- drake_plan( data = get_data(), analysis = target( model_function(data, mean = mean_value, tuning = tuning_setting), transform = cross( tuning_setting = c(&quot;fast&quot;, &quot;slow&quot;), mean_value = !!(1:4), # with !! model_function = !!model_functions # with !! ) ) ) drake_plan_source(plan) #&gt; drake_plan( #&gt; data = get_data(), #&gt; analysis_fast_1L_main = main(data, mean = 1L, tuning = &quot;fast&quot;), #&gt; analysis_slow_1L_main = main(data, mean = 1L, tuning = &quot;slow&quot;), #&gt; analysis_fast_2L_main = main(data, mean = 2L, tuning = &quot;fast&quot;), #&gt; analysis_slow_2L_main = main(data, mean = 2L, tuning = &quot;slow&quot;), #&gt; analysis_fast_3L_main = main(data, mean = 3L, tuning = &quot;fast&quot;), #&gt; analysis_slow_3L_main = main(data, mean = 3L, tuning = &quot;slow&quot;), #&gt; analysis_fast_4L_main = main(data, mean = 4L, tuning = &quot;fast&quot;), #&gt; analysis_slow_4L_main = main(data, mean = 4L, tuning = &quot;slow&quot;), #&gt; analysis_fast_1L_altv = altv(data, mean = 1L, tuning = &quot;fast&quot;), #&gt; analysis_slow_1L_altv = altv(data, mean = 1L, tuning = &quot;slow&quot;), #&gt; analysis_fast_2L_altv = altv(data, mean = 2L, tuning = &quot;fast&quot;), #&gt; analysis_slow_2L_altv = altv(data, mean = 2L, tuning = &quot;slow&quot;), #&gt; analysis_fast_3L_altv = altv(data, mean = 3L, tuning = &quot;fast&quot;), #&gt; analysis_slow_3L_altv = altv(data, mean = 3L, tuning = &quot;slow&quot;), #&gt; analysis_fast_4L_altv = altv(data, mean = 4L, tuning = &quot;fast&quot;), #&gt; analysis_slow_4L_altv = altv(data, mean = 4L, tuning = &quot;slow&quot;) #&gt; ) 5.4 Static transformations There are four transformations in static branching: map(), cross(), split(), and combine(). They are not actual functions, just special language to supply to the transform argument of target() in drake_plan(). Each transformation is similar to a function from the Tidyverse. drake Tidyverse analogue map() pmap() from purrr cross() crossing() from tidyr split() group_map() from dplyr combine() summarize() from dplyr 5.4.1 map() map() creates a new target for each row in a grid. drake_plan( x = target( simulate_data(center, scale), transform = map(center = c(2, 1, 0), scale = c(3, 2, 1)) ) ) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_2_3 simulate_data(2, 3) #&gt; 2 x_1_2 simulate_data(1, 2) #&gt; 3 x_0_1 simulate_data(0, 1) You can supply the grid directly with the .data argument. Note the use of !! below. (See the tidy evaluation section.) my_grid &lt;- tibble( sim_function = c(&quot;rnorm&quot;, &quot;rt&quot;, &quot;rcauchy&quot;), title = c(&quot;Normal&quot;, &quot;Student t&quot;, &quot;Cauchy&quot;) ) my_grid$sim_function &lt;- rlang::syms(my_grid$sim_function) drake_plan( x = target( simulate_data(sim_function, title, center, scale), transform = map( center = c(2, 1, 0), scale = c(3, 2, 1), .data = !!my_grid, # In `.id`, you can select one or more grouping variables # for pretty target names. # Set to FALSE to use short numeric suffixes. .id = sim_function # Try `.id = c(sim_function, center)` yourself. ) ) ) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_rnorm simulate_data(rnorm, &quot;Normal&quot;, 2, 3) #&gt; 2 x_rt simulate_data(rt, &quot;Student t&quot;, 1, 2) #&gt; 3 x_rcauchy simulate_data(rcauchy, &quot;Cauchy&quot;, 0, 1) 5.4.2 cross() cross() creates a new target for each combination of argument values. drake_plan( x = target( simulate_data(nrow, ncol), transform = cross(nrow = c(1, 2, 3), ncol = c(4, 5)) ) ) #&gt; # A tibble: 6 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_1_4 simulate_data(1, 4) #&gt; 2 x_2_4 simulate_data(2, 4) #&gt; 3 x_3_4 simulate_data(3, 4) #&gt; 4 x_1_5 simulate_data(1, 5) #&gt; 5 x_2_5 simulate_data(2, 5) #&gt; 6 x_3_5 simulate_data(3, 5) 5.4.3 split() The split() transformation distributes a dataset as uniformly as possible across multiple targets. plan &lt;- drake_plan( large_data = get_data(), slice_analysis = target( large_data %&gt;% analyze(), transform = split(large_data, slices = 4) ), results = target( dplyr::bind_rows(slice_analysis), transform = combine(slice_analysis) ) ) plan #&gt; # A tibble: 6 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 large_data get_data() … #&gt; 2 slice_analysi… drake_slice(data = large_data, slices = 4, index = 1) %&gt;% anal… #&gt; 3 slice_analysi… drake_slice(data = large_data, slices = 4, index = 2) %&gt;% anal… #&gt; 4 slice_analysi… drake_slice(data = large_data, slices = 4, index = 3) %&gt;% anal… #&gt; 5 slice_analysi… drake_slice(data = large_data, slices = 4, index = 4) %&gt;% anal… #&gt; 6 results dplyr::bind_rows(slice_analysis_1, slice_analysis_2, slice_ana… plot(plan) At runtime, drake_slice() takes a single subset of the data. It supports data frames, matrices, and arbitrary arrays. drake_slice(iris, slices = 50, index = 1) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa drake_slice(iris, slices = 50, index = 2) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa 5.4.4 combine() combine() aggregates targets. The closest comparison is the unquote-splice operator !!! from tidy evaluation. plan &lt;- drake_plan( data_group1 = target( sim_data(mean = x, sd = y), transform = map(x = c(1, 2), y = c(3, 4)) ), data_group2 = target( pull_data(url), transform = map(url = c(&quot;example1.com&quot;, &quot;example2.com&quot;)) ), larger = target( bind_rows(data_group1, data_group2, .id = &quot;id&quot;) %&gt;% arrange(sd) %&gt;% head(n = 400), transform = combine(data_group1, data_group2) ) ) drake_plan_source(plan) #&gt; drake_plan( #&gt; data_group1_1_3 = sim_data(mean = 1, sd = 3), #&gt; data_group1_2_4 = sim_data(mean = 2, sd = 4), #&gt; data_group2_example1.com = pull_data(&quot;example1.com&quot;), #&gt; data_group2_example2.com = pull_data(&quot;example2.com&quot;), #&gt; larger = bind_rows(data_group1_1_3, data_group1_2_4, data_group2_example1.com, #&gt; data_group2_example2.com, #&gt; .id = &quot;id&quot; #&gt; ) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400) #&gt; ) To create multiple combined groups, use the .by argument. plan &lt;- drake_plan( data = target( sim_data(mean = x, sd = y, skew = z), transform = cross(x = c(1, 2), y = c(3, 4), z = c(5, 6)) ), combined = target( bind_rows(data, .id = &quot;id&quot;) %&gt;% arrange(sd) %&gt;% head(n = 400), transform = combine(data, .by = c(x, y)) ) ) drake_plan_source(plan) #&gt; drake_plan( #&gt; data_1_3_5 = sim_data(mean = 1, sd = 3, skew = 5), #&gt; data_2_3_5 = sim_data(mean = 2, sd = 3, skew = 5), #&gt; data_1_4_5 = sim_data(mean = 1, sd = 4, skew = 5), #&gt; data_2_4_5 = sim_data(mean = 2, sd = 4, skew = 5), #&gt; data_1_3_6 = sim_data(mean = 1, sd = 3, skew = 6), #&gt; data_2_3_6 = sim_data(mean = 2, sd = 3, skew = 6), #&gt; data_1_4_6 = sim_data(mean = 1, sd = 4, skew = 6), #&gt; data_2_4_6 = sim_data(mean = 2, sd = 4, skew = 6), #&gt; combined_1_3 = bind_rows(data_1_3_5, data_1_3_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400), #&gt; combined_2_3 = bind_rows(data_2_3_5, data_2_3_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400), #&gt; combined_1_4 = bind_rows(data_1_4_5, data_1_4_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400), #&gt; combined_2_4 = bind_rows(data_2_4_5, data_2_4_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400) #&gt; ) 5.5 Tags A tag is a custom grouping variable for a transformation. There are two kinds of tags: In-tags, which contain the target name you start with, and Out-tags, which contain the target names generated by the transformations. drake_plan( x = target( command, transform = map(y = c(1, 2), .tag_in = from, .tag_out = c(to, out)) ), trace = TRUE ) #&gt; # A tibble: 2 x 7 #&gt; target command y x from to out #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 x_1 command 1 x_1 x x_1 x_1 #&gt; 2 x_2 command 2 x_2 x x_2 x_2 Subsequent transformations can use tags as grouping variables and add to existing tags. plan &lt;- drake_plan( prep_work = do_prep_work(), local = target( get_local_data(n, prep_work), transform = map(n = c(1, 2), .tag_in = data_source, .tag_out = data) ), online = target( get_online_data(n, prep_work, port = &quot;8080&quot;), transform = map(n = c(1, 2), .tag_in = data_source, .tag_out = data) ), summary = target( summarize(bind_rows(data, .id = &quot;data&quot;)), transform = combine(data, .by = data_source) ), munged = target( munge(bind_rows(data, .id = &quot;data&quot;)), transform = combine(data, .by = n) ) ) drake_plan_source(plan) #&gt; drake_plan( #&gt; prep_work = do_prep_work(), #&gt; local_1 = get_local_data(1, prep_work), #&gt; local_2 = get_local_data(2, prep_work), #&gt; online_1 = get_online_data(1, prep_work, port = &quot;8080&quot;), #&gt; online_2 = get_online_data(2, prep_work, port = &quot;8080&quot;), #&gt; summary_local = summarize(bind_rows(local_1, local_2, .id = &quot;data&quot;)), #&gt; summary_online = summarize(bind_rows(online_1, online_2, .id = &quot;data&quot;)), #&gt; munged_1 = munge(bind_rows(local_1, online_1, .id = &quot;data&quot;)), #&gt; munged_2 = munge(bind_rows(local_2, online_2, .id = &quot;data&quot;)) #&gt; ) plot(plan) 5.5.1 Target names All transformations have an optional .id argument to control the names of targets. Use it to select the grouping variables that go into the names, as well as the order they appear in the suffixes. drake_plan( data = target( get_data(param1, param2), transform = map( param1 = c(123, 456), param2 = c(7, 9), param2 = c(&quot;abc&quot;, &quot;xyz&quot;), .id = param2 ) ) ) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data_7 get_data(123, 7) #&gt; 2 data_9 get_data(456, 9) drake_plan( data = target( get_data(param1, param2), transform = map( param1 = c(123, 456), param2 = c(7, 9), param2 = c(&quot;abc&quot;, &quot;xyz&quot;), .id = c(param2, param1) ) ) ) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data_7_123 get_data(123, 7) #&gt; 2 data_9_456 get_data(456, 9) drake_plan( data = target( get_data(param1, param2), transform = map( param1 = c(123, 456), param2 = c(7, 9), param2 = c(&quot;abc&quot;, &quot;xyz&quot;), .id = c(param1, param2) ) ) ) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data_123_7 get_data(123, 7) #&gt; 2 data_456_9 get_data(456, 9) Set .id to FALSE to ignore the grouping variables altogether. drake_plan( data = target( get_data(param1, param2), transform = map( param1 = c(123, 456), param2 = c(7, 9), param2 = c(&quot;abc&quot;, &quot;xyz&quot;), .id = FALSE ) ) ) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data get_data(123, 7) #&gt; 2 data_2 get_data(456, 9) Finally, drake supports a special .id_chr symbol in commands to let you refer to the name of the current target as a character string. as_chr &lt;- function(x) { deparse(substitute(x)) } plan &lt;- drake_plan( data = target( get_data(param), transform = map(param = c(123, 456)) ), keras_model = target( save_model_hdf5(fit_model(data), file_out(!!sprintf(&quot;%s.h5&quot;, .id_chr))), transform = map(data, .id = param) ), result = target( predict(load_model_hdf5(file_in(!!sprintf(&quot;%s.h5&quot;, as_chr(keras_model))))), transform = map(keras_model, .id = param) ) ) plan #&gt; # A tibble: 6 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data_123 get_data(123) … #&gt; 2 data_456 get_data(456) … #&gt; 3 keras_model_123 save_model_hdf5(fit_model(data_123), file_out(&quot;keras_model_12… #&gt; 4 keras_model_456 save_model_hdf5(fit_model(data_456), file_out(&quot;keras_model_45… #&gt; 5 result_123 predict(load_model_hdf5(file_in(&quot;keras_model_123.h5&quot;))) … #&gt; 6 result_456 predict(load_model_hdf5(file_in(&quot;keras_model_456.h5&quot;))) … drake_plan_source(plan) #&gt; drake_plan( #&gt; data_123 = get_data(123), #&gt; data_456 = get_data(456), #&gt; keras_model_123 = save_model_hdf5(fit_model(data_123), file_out(&quot;keras_model_123.h5&quot;)), #&gt; keras_model_456 = save_model_hdf5(fit_model(data_456), file_out(&quot;keras_model_456.h5&quot;)), #&gt; result_123 = predict(load_model_hdf5(file_in(&quot;keras_model_123.h5&quot;))), #&gt; result_456 = predict(load_model_hdf5(file_in(&quot;keras_model_456.h5&quot;))) #&gt; ) "],
["dynamic.html", "Chapter 6 Dynamic branching 6.1 A note about versions 6.2 Motivation 6.3 Which kind of branching should I use? 6.4 Dynamic targets 6.5 Dynamic transformations 6.6 Trace 6.7 max_expand", " Chapter 6 Dynamic branching 6.1 A note about versions The first release of dynamic branching was in drake version 7.8.0. In subsequent versions, dynamic branching behaves differently. This manual describes how dynamic branching works in development drake (to become version 7.9.0 in early January 2020). If you are using version 7.8.0, please refer to this version of the chapter instead. 6.2 Motivation In large workflows, you may need more targets than you can easily type in a plan, and you may not be able to fully specify all targets in advance. Dynamic branching is an interface to declare new targets while make() is running. It lets you create more compact plans and graphs, it is easier to use than static branching, and it improves the startup speed of make() and friends. 6.3 Which kind of branching should I use? With dynamic branching, make() is faster to initialize, and you have far more flexibility. With static branching, you have meaningful target names, and it is easier to predict what the plan is going to do in advance. There is a ton of room for overlap and personal judgement, and you can even use both kinds of branching together. 6.4 Dynamic targets A dynamic target is a vector of sub-targets. We let make() figure out which sub-targets to create and how to aggregate them. As an example, let’s fit a regression model to each continent in Gapminder data. To activate dynamic branching, use the dynamic argument of target(). library(broom) library(drake) library(gapminder) library(tidyverse) # Split the Gapminder data by continent. gapminder_continents &lt;- function() { gapminder %&gt;% mutate(gdpPercap = scale(gdpPercap)) %&gt;% split(f = .$continent) } # Fit a model to a continent. fit_model &lt;- function(continent_data) { data &lt;- continent_data[[1]] data %&gt;% lm(formula = gdpPercap ~ year) %&gt;% tidy() %&gt;% mutate(continent = data$continent[1]) %&gt;% select(continent, term, statistic, p.value) } plan &lt;- drake_plan( continents = gapminder_continents(), model = target(fit_model(continents), dynamic = map(continents)) ) make(plan) #&gt; target continents #&gt; dynamic model #&gt; subtarget model_23022788 #&gt; subtarget model_e020ff00 #&gt; subtarget model_53c1b086 #&gt; subtarget model_0fc4392a #&gt; subtarget model_8b2d2fd5 #&gt; aggregate model Thanks to vctrs, the data type of every sub-target is the same as the dynamic target it belongs to. In other words, model and model_23022788 are both data frames. To understand how drake splits and concatenates dynamic targets, see functions vec_size(), vec_slice(), and vec_c(). readd(model) #&gt; # A tibble: 10 x 4 #&gt; continent term statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Africa (Intercept) -4.44 1.08e- 5 #&gt; 2 Africa year 4.04 5.90e- 5 #&gt; 3 Americas (Intercept) -5.56 6.10e- 8 #&gt; 4 Americas year 5.55 6.16e- 8 #&gt; 5 Asia (Intercept) -2.74 6.39e- 3 #&gt; 6 Asia year 2.75 6.23e- 3 #&gt; 7 Europe (Intercept) -14.4 3.12e-37 #&gt; 8 Europe year 14.5 7.06e-38 #&gt; 9 Oceania (Intercept) -11.3 1.32e-10 #&gt; 10 Oceania year 11.5 9.48e-11 If automatic concatenation of sub-targets is confusing (e.g. if some sub-targets are NULL, as in https://github.com/ropensci-books/drake/issues/142) you can read the dynamic target as a named list (only in drake version 7.10.0 and above). readd(model, subtarget_list = TRUE) # Requires drake &gt;= 7.10.0. #&gt; $model_23022788 #&gt; # A tibble: 2 x 4 #&gt; continent term statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Africa (Intercept) -4.44 0.0000108 #&gt; 2 Africa year 4.04 0.0000590 #&gt; #&gt; $model_e020ff00 #&gt; # A tibble: 2 x 4 #&gt; continent term statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Americas (Intercept) -5.56 0.0000000610 #&gt; 2 Americas year 5.55 0.0000000616 #&gt; #&gt; $model_53c1b086 #&gt; # A tibble: 2 x 4 #&gt; continent term statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Asia (Intercept) -2.74 0.00639 #&gt; 2 Asia year 2.75 0.00623 #&gt; #&gt; $model_0fc4392a #&gt; # A tibble: 2 x 4 #&gt; continent term statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Europe (Intercept) -14.4 3.12e-37 #&gt; 2 Europe year 14.5 7.06e-38 #&gt; #&gt; $model_8b2d2fd5 #&gt; # A tibble: 2 x 4 #&gt; continent term statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Oceania (Intercept) -11.3 1.32e-10 #&gt; 2 Oceania year 11.5 9.48e-11 Alternatively, you can work with the individual sub-targets. subtargets(model) #&gt; [1] &quot;model_23022788&quot; &quot;model_e020ff00&quot; &quot;model_53c1b086&quot; &quot;model_0fc4392a&quot; #&gt; [5] &quot;model_8b2d2fd5&quot; readd(model_23022788) # equivalent: readd(model, subtargets = 1) #&gt; # A tibble: 2 x 4 #&gt; continent term statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Africa (Intercept) -4.44 0.0000108 #&gt; 2 Africa year 4.04 0.0000590 But automatic concatenation with vctrs is the default behavior, and it is how drake thinks about downstream targets. In our example, when we write dynamic = map(continents) in the plan, drake treats continents as a vector. In other words, target(fit_model(continents), dynamic = map(continents)) is equivalent to commands fit_model(continents[1]) through fit_model(continents[5]). Since continents is really a list of data frames, continents[1] through continents[5] are also lists of data frames, which is why we need the line data &lt;- continent_data[[1]] in fit_model(). This may seem inconvenient at first, but it comes with extra type stability, and it allows us to reference a dynamic target and its sub-targets interchangeably. To post-process our models, we can work with either the individual sub-targets or the whole vector of all the models. Below, year uses the former and intercept uses the latter. plan &lt;- drake_plan( continents = gapminder_continents(), model = target(fit_model(continents), dynamic = map(continents)), # Filter each model individually: year = target(filter(model, term == &quot;year&quot;), dynamic = map(model)), # Aggregate all the models, then filter the whole vector: intercept = filter(model, term != &quot;year&quot;) ) make(plan) #&gt; dynamic year #&gt; subtarget year_8b2ba535 #&gt; subtarget year_9eb9c094 #&gt; subtarget year_063dea11 #&gt; subtarget year_0171ca0a #&gt; subtarget year_7401d5f6 #&gt; aggregate year #&gt; target intercept readd(year) #&gt; # A tibble: 5 x 4 #&gt; continent term statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Africa year 4.04 5.90e- 5 #&gt; 2 Americas year 5.55 6.16e- 8 #&gt; 3 Asia year 2.75 6.23e- 3 #&gt; 4 Europe year 14.5 7.06e-38 #&gt; 5 Oceania year 11.5 9.48e-11 readd(intercept) #&gt; # A tibble: 5 x 4 #&gt; continent term statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Africa (Intercept) -4.44 1.08e- 5 #&gt; 2 Americas (Intercept) -5.56 6.10e- 8 #&gt; 3 Asia (Intercept) -2.74 6.39e- 3 #&gt; 4 Europe (Intercept) -14.4 3.12e-37 #&gt; 5 Oceania (Intercept) -11.3 1.32e-10 6.5 Dynamic transformations Dynamic branching supports transformations map(), cross(), and group(). These transformations tell drake how to create sub-targets. 6.5.1 map() map() iterates over the vector slices of the targets you supply as arguments. We saw above how map() iterates over lists. If you give it a data frame, it will map over the rows. plan &lt;- drake_plan( subset = head(gapminder), row = target(subset, dynamic = map(subset)) ) make(plan) #&gt; target subset #&gt; dynamic row #&gt; subtarget row_9939cae3 #&gt; subtarget row_e8047114 #&gt; subtarget row_2ef3db10 #&gt; subtarget row_f9171bbe #&gt; subtarget row_7d6002e9 #&gt; subtarget row_509468b3 #&gt; aggregate row readd(row_9939cae3) #&gt; # A tibble: 1 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. If you supply multiple targets, map() iterates over the slices of each. plan &lt;- drake_plan( numbers = seq_len(2), letters = c(&quot;a&quot;, &quot;b&quot;), zipped = target(paste0(numbers, letters), dynamic = map(numbers, letters)) ) make(plan) #&gt; target numbers #&gt; target letters #&gt; dynamic zipped #&gt; subtarget zipped_8ac3968c #&gt; subtarget zipped_4a7a9b07 #&gt; aggregate zipped readd(zipped) #&gt; [1] &quot;1a&quot; &quot;2b&quot; 6.5.2 cross() cross() creates a new sub-target for each combination of targets you supply as arguments. plan &lt;- drake_plan( numbers = seq_len(2), letters = c(&quot;a&quot;, &quot;b&quot;), combo = target(paste0(numbers, letters), dynamic = cross(numbers, letters)) ) make(plan) #&gt; dynamic combo #&gt; subtarget combo_8ac3968c #&gt; subtarget combo_ef37ab56 #&gt; subtarget combo_ed1d2e7b #&gt; subtarget combo_4a7a9b07 #&gt; aggregate combo readd(combo) #&gt; [1] &quot;1a&quot; &quot;1b&quot; &quot;2a&quot; &quot;2b&quot; 6.5.3 group() With group(), you can create multiple aggregates of a given target. Use the .by argument to set a grouping variable. plan &lt;- drake_plan( data = gapminder, by = data$continent, gdp = target( tibble(median = median(data$gdpPercap), continent = by[1]), dynamic = group(data, .by = by) ) ) make(plan) #&gt; target data #&gt; target by #&gt; dynamic gdp #&gt; subtarget gdp_9adfc39f #&gt; subtarget gdp_d9f30951 #&gt; subtarget gdp_958a2f81 #&gt; subtarget gdp_962b03c8 #&gt; subtarget gdp_dc1cff81 #&gt; aggregate gdp readd(gdp) #&gt; # A tibble: 5 x 2 #&gt; median continent #&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 2647. Asia #&gt; 2 12082. Europe #&gt; 3 1192. Africa #&gt; 4 5466. Americas #&gt; 5 17983. Oceania 6.6 Trace All dynamic transforms have a .trace argument to record optional metadata for each sub-target. In the example from group(), the trace is another way to keep track of the continent of each median GDP value. plan &lt;- drake_plan( data = gapminder, by = data$continent, gdp = target( median(data$gdpPercap), dynamic = group(data, .by = by, .trace = by) ) ) make(plan) #&gt; dynamic gdp #&gt; subtarget gdp_7e88fb1c #&gt; subtarget gdp_a61b8e1b #&gt; subtarget gdp_278ff532 #&gt; subtarget gdp_6f3facea #&gt; subtarget gdp_73037e69 #&gt; aggregate gdp The gdp target no longer contains any explicit reference to continent. readd(gdp) #&gt; [1] 2646.787 12081.749 1192.138 5465.510 17983.304 However, we can look up the continents in the trace. read_trace(&quot;by&quot;, gdp) #&gt; [1] Asia Europe Africa Americas Oceania #&gt; Levels: Africa Americas Asia Europe Oceania 6.7 max_expand Suppose we want a model for each country. gapminder_countries &lt;- function() { gapminder %&gt;% mutate(gdpPercap = scale(gdpPercap)) %&gt;% split(f = .$country) } plan &lt;- drake_plan( countries = gapminder_countries(), model = target(fit_model(countries), dynamic = map(countries)) ) The Gapminder dataset has 142 countries, which can get overwhelming. In the early stages of the workflow when we are still debugging and testing, we can limit the number of sub-targets using the max_expand argument of make(). make(plan, max_expand = 2) #&gt; target countries #&gt; dynamic model #&gt; subtarget model_c5e7c7a2 #&gt; subtarget model_656f8fe2 #&gt; aggregate model readd(model) #&gt; # A tibble: 4 x 4 #&gt; continent term statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Asia (Intercept) -1.48 0.170 #&gt; 2 Asia year -0.233 0.821 #&gt; 3 Europe (Intercept) -4.76 0.000773 #&gt; 4 Europe year 4.59 0.000998 Then, when we are confident and ready, we can scale up to the full number of models. make(plan) "],
["projects.html", "Chapter 7 drake projects 7.1 Code files 7.2 Safer interactivity 7.3 Script file pitfalls 7.4 Workflows as R packages 7.5 Other tools", " Chapter 7 drake projects drake’s design philosophy is extremely R-focused. It embraces in-memory configuration, in-memory dependencies, interactivity, and flexibility. 7.1 Code files The names and locations of the files are entirely up to you, but this pattern is particularly useful to start with. make.R R/ ├── packages.R ├── functions.R └── plan.R Here, make.R is a master script that Loads your packages, functions, and other in-memory data. Creates the drake plan. Calls make(). Let’s consider the main example, which you can download with drake_example(&quot;main&quot;). Here, our master script is called make.R: source(&quot;R/packages.R&quot;) # loads packages source(&quot;R/functions.R&quot;) # defines the create_plot() function source(&quot;R/plan.R&quot;) # creates the drake plan # options(clustermq.scheduler = &quot;multicore&quot;) # optional parallel computing. Also needs parallelism = &quot;clustermq&quot; make( plan, # defined in R/plan.R verbose = 2 ) We have an R folder containing our supporting files. packages.R typically includes all the packages you will use in the workflow. # packages.R library(drake) library(dplyr) library(ggplot2) drake does not automatically watch package functions for changes, so if you have any specialized packages and you want drake to track those functions, here is a great place to call expose_imports(). # alternative packages.R library(drake) library(dplyr) library(ggplot2) library(PackageContainingYourWorkflow) # Tell drake to track the package&#39;s functions. expose_imports(PackageContainingYourWorkflow) Your functions.R typically has the supporting custom functions you write for the workflow. If there are many functions, you could split them up into multiple files. # functions.R create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram(binwidth = 0.25) + theme_gray(20) } Finally, it is good practice to define a plan.R that defines the plan. # plan.R plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)), hist = create_plot(data), fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) To run the example project above, Start a clean new R session. Run the make.R script. On Mac and Linux, you can do this by opening a terminal and entering R CMD BATCH --no-save make.R. On Windows, restart your R session and call source(&quot;make.R&quot;) in the R console. Note: this part of drake does not inherently focus on your script files. There is nothing magical about the names make.R, packages.R, functions.R, or plan.R. Different projects may require different file structures. drake has other functions to inspect your results and examine your workflow. Before invoking them interactively, it is best to start with a clean new R session. # Restart R. interactive() #&gt; [1] TRUE source(&quot;R/packages.R&quot;) source(&quot;R/functions.R&quot;) source(&quot;R/plan.R&quot;) vis_drake_graph(plan) 7.2 Safer interactivity 7.2.1 Motivation A serious drake workflow should be consistent and reliable, ideally with the help of a master R script. Before it builds your targets, this script should begin in a fresh R session and load your packages and functions in a dependable manner. Batch mode makes sure all this goes according to plan. If you use a single persistent interactive R session to repeatedly invoke make() while you develop the workflow, then over time, your session could grow stale and accidentally invalidate targets. For example, if you interactively tinker with a new version of create_plot(), targets hist and report will fall out of date without warning, and the next make() will build them again. Even worse, the outputs from hist and report will be wrong if they depend on a half-finished create_plot(). The quickest workaround is to restart R and source() your setup scripts all over again. However, a better solution is to use r_make() and friends. r_make() runs make() in a new transient R session so that accidental changes to your interactive environment do not break your workflow. 7.2.2 Usage To use r_make(), you need a configuration R script. Unless you supply a custom file path (e.g. r_make(source = &quot;your_file.R&quot;) or options(drake_source = &quot;your_file.R&quot;)) drake assumes this configuration script is called _drake.R. (So the file name really is magical in this case). The suggested file structure becomes: _drake.R R/ ├── packages.R ├── functions.R └── plan.R Like our previous make.R script, _drake.R runs all our pre-make() setup steps. But this time, rather than calling make(), it ends with a call to drake_config(). drake_config() is the initial preprocessing stage of make(), and it accepts all the same arguments as make(). Example _drake.R: source(&quot;R/packages.R&quot;) source(&quot;R/functions.R&quot;) source(&quot;R/plan.R&quot;) # options(clustermq.scheduler = &quot;multicore&quot;) # optional parallel computing drake_config(plan, verbose = 2) Here is what happens when you call r_make(). drake launches a new transient R session using callr::r(). The remaining steps all happen within this transient session. Run the configuration script (e.g. _drake.R) to Load the packages, functions, global options, drake plan, etc. into the session’s environnment, and Run the call to drake_config()and store the results in a variable called config. Execute make_impl(config = config), an internal drake function. The purpose of drake_config() is to collect and sanitize all the parameters and settings that make() needs to do its job. In fact, if you do not set the config argument explicitly, then make() invokes drake_config() behind the scenes. make(plan, parallelism = &quot;clustermq&quot;, jobs = 2, verbose = 6) is equivalent to config &lt;- drake_config(plan, verbose = 2) make_impl(config = config) There are many more r_*() functions besides r_make(), each of which launches a fresh session and runs an inner drake function on the config object from _drake.R. Outer function call Inner function call r_make() make_impl(config = config) r_drake_build(...) drake_build_impl(config, ...) r_outdated(...) outdated_impl(config, ...) r_missed(...) missed_impl(config, ...) r_vis_drake_graph(...) vis_drake_graph_impl(config, ...) r_sankey_drake_graph(...) sankey_drake_graph_impl(config, ...) r_drake_ggraph(...) drake_ggraph_impl(config, ...) r_drake_graph_info(...) drake_graph_info_impl(config, ...) r_predict_runtime(...) predict_runtime_impl(config, ...) r_predict_workers(...) predict_workers_impl(config, ...) clean() r_outdated(r_args = list(show = FALSE)) #&gt; [1] &quot;data&quot; &quot;fit&quot; &quot;hist&quot; &quot;raw_data&quot; &quot;report&quot; r_make() #&gt; Loading required package: dplyr #&gt; #&gt; Attaching package: ‘dplyr’ #&gt; #&gt; The following objects are masked from ‘package:stats’: #&gt; #&gt; filter, lag #&gt; #&gt; The following objects are masked from ‘package:base’: #&gt; #&gt; intersect, setdiff, setequal, union #&gt; #&gt; Loading required package: ggplot2 #&gt; target raw_data #&gt; target data #&gt; target fit #&gt; target hist #&gt; target report r_outdated(r_args = list(show = FALSE)) #&gt; character(0) r_vis_drake_graph(targets_only = TRUE, r_args = list(show = FALSE)) Remarks: You can run r_make() in an interactive session, but the transient process it launches will not be interactive. Thus, any browser() statements in the commands in your drake plan will be ignored. You can select and configure the underlying callr function using arguments r_fn and r_args, respectively. For example code, you can download the updated main example (drake_example(&quot;main&quot;)) and experiment with files _drake.R and interactive.R. 7.3 Script file pitfalls Despite the above discussion of R scripts, drake plans rely more on in-memory functions. You might be tempted to write a plan like the following, but then drake cannot tell that my_analysis depends on my_data. bad_plan &lt;- drake_plan( my_data = source(file_in(&quot;get_data.R&quot;)), my_analysis = source(file_in(&quot;analyze_data.R&quot;)), my_summaries = source(file_in(&quot;summarize_data.R&quot;)) ) vis_drake_graph(bad_plan, targets_only = TRUE) When it comes to plans, use functions instead. source(&quot;my_functions.R&quot;) # defines get_data(), analyze_data(), etc. good_plan &lt;- drake_plan( my_data = get_data(file_in(&quot;data.csv&quot;)), # External files need to be in commands explicitly. # nolint my_analysis = analyze_data(my_data), my_summaries = summarize_results(my_data, my_analysis) ) vis_drake_graph(good_plan, targets_only = TRUE) In drake &gt;= 7.6.2.9000, code_to_function() leverages existing imperative scripts for use in a drake plan. get_data &lt;- code_to_function(&quot;get_data.R&quot;) do_analysis &lt;- code_to_function(&quot;analyze_data.R&quot;) do_summary &lt;- code_to_function(&quot;summarize_data.R&quot;) good_plan &lt;- drake_plan( my_data = get_data(), my_analysis = do_analysis(my_data), my_summaries = do_summary(my_data, my_analysis) ) vis_drake_graph(good_plan, targets_only = TRUE) 7.4 Workflows as R packages The R package structure is a great way to organize and quality-control a data analysis project. If you write a drake workflow as a package, you will need Use expose_imports() to properly account for all your nested function dependencies, and If you load the package with devtools::load_all(), set the prework argument of make(): e.g. make(prework = &quot;devtools::load_all()&quot;). For a minimal example, see Tiernan Martin’s drakepkg. 7.5 Other tools drake enhances reproducibility, but not in all respects. Local library managers, containerization, and session management tools offer more robust solutions in their respective domains. Reproducibility encompasses a wide variety of tools and techniques all working together. Comprehensive overviews: PLOS article by Wilson et al. RStudio Conference 2019 presentation by Karthik Ram. rrtools by Ben Marwick. "],
["scripts.html", "Chapter 8 Script-based workflows 8.1 Function-oriented workflows 8.2 Traditional and legacy workflows 8.3 Overcoming Technical Debt 8.4 Dependencies 8.5 Building the connections 8.6 Run the workflow 8.7 Keeping the results up to date 8.8 Final thoughts", " Chapter 8 Script-based workflows 8.1 Function-oriented workflows drake works best when you write functions for data analysis. Functions break down complicated ideas into manageable pieces. # R/functions.R get_data &lt;- function(file){ readxl::read_excel(file) } munge_data &lt;- function(raw_data){ raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) } fit_model &lt;- function(munged_data){ lm(Sepal.Width ~ Petal.Width + Species, munged_data) } When we express computational steps as functions like get_data(), munge_data(), and fit_model(), we create special shorthand to make the rest of our code easier to read and understand. # R/plan.R plan &lt;- drake_plan( raw_data = get_data(file_in(&quot;raw_data.xlsx&quot;)), munged_data = munge_data(raw_data), model = fit_model(munged_data) ) This function-oriented approach is elegant, powerful, testable, scalable, and maintainable. However, it can be challenging to convert pre-existing traditional script-based analyses to function-oriented drake-powered workflows. This chapter describes a stopgap to retrofit drake to existing projects. Custom functions are still better in the long run, but the following workaround is quick and painless, and it does not require you to change your original scripts. 8.2 Traditional and legacy workflows It is common to express data analysis tasks as numbered scripts. 01_data.R 02_munge.R 03_histogram.R 04_regression.R 05_report.R The numeric prefixes indicate the order in which these scripts need to run. # run_everything.R source(&quot;01_data.R&quot;) source(&quot;02_munge.R&quot;) source(&quot;03_histogram.R&quot;) source(&quot;04_regression.R&quot;) source(&quot;05_report.R&quot;) # Calls rmarkdown::render() on report.Rmd. 8.3 Overcoming Technical Debt code_to_function() creates drake_plan()-ready functions from scripts like these. # R/functions.R load_data &lt;- code_to_function(&quot;01_data.R&quot;) munge_data &lt;- code_to_function(&quot;02_munge.R&quot;) make_histogram &lt;- code_to_function(&quot;03_histogram.R&quot;) do_regression &lt;- code_to_function(&quot;04_regression.R&quot;) generate_report &lt;- code_to_function(&quot;05_report.R&quot;) Each function contains all the code from its corresponding script, along with a special final line to make sure we never return the same value twice. print(load_data) #&gt; function (...) #&gt; { #&gt; raw_data &lt;- readxl::read_excel(&quot;raw_data.xlsx&quot;) #&gt; saveRDS(raw_data, &quot;data/loaded_data.RDS&quot;) #&gt; list(time = Sys.time(), tempfile = tempfile()) #&gt; } #&gt; &lt;environment: 0x8ed1180&gt; 8.4 Dependencies drake pays close attention to dependencies. In drake, a target’s dependencies are the things it needs in order to build. Dependencies can include functions, files, and other targets upstream. Any time a dependency changes, the target is no longer valid. The make() function automatically detects when dependencies change, and it rebuilds the targets that need to rebuild. To leverage drake’s dependency-watching capabilities, we create a drake plan. This plan should include all the steps of the analysis, from loading the data to generating a report. To write the plan, we plug in the functions we created from code_to_function(). simple_plan &lt;- drake_plan( data = load_data(), munged_data = munge_data(), hist = make_histogram(), fit = do_regression(), report = generate_report() ) It’s a start, but right now, drake has no idea which targets to run first and which need to wait for dependencies! In the following graph, there are no edges (arrows) connecting the targets! vis_drake_graph(simple_plan) 8.5 Building the connections Just as our original scripts had to run in a certain order, so do our targets now. We pass targets as function arguments to express this execution order. For example, when we write munged_data = munge_data(data), we are signaling to drake that the munged_data target depends on the function munge_data() and the target data. script_based_plan &lt;- drake_plan( data = load_data(), munged_data = munge_data(data), hist = make_histogram(munged_data), fit = do_regression(munged_data), report = generate_report(hist, fit) ) vis_drake_graph(script_based_plan) 8.6 Run the workflow We can now run the workflow with the make() function. The first call to make() runs all the data analysis tasks we got from the scripts. make(script_based_plan) #&gt; target data #&gt; target munged_data #&gt; target fit #&gt; target hist #&gt; target report 8.7 Keeping the results up to date Any time we change a script, we need to run code_to_function() again to keep our function up to date. drake notices when this function changes, and make() reruns the updated function and the all downstream functions that rely on the output. For example, let’s fine tune our histogram. We open 03_histogram.R, change the binwidth argument from 0.5 to 0.25, and call code_to_function(&quot;03_histogram.R&quot;) all over again. # We need to rerun code_to_function() to tell drake that the script changed. make_histogram &lt;- code_to_function(&quot;03_histogram.R&quot;) Targets hist and report depend on the code we modified, so drake marks those targets as outdated. outdated(script_based_plan) #&gt; [1] &quot;hist&quot; &quot;report&quot; vis_drake_graph(script_based_plan, targets_only = TRUE) When you call make(), drake runs make_histogram() because the underlying script changed, and it runs generate_report() because the report depends on hist. make(script_based_plan) #&gt; target hist #&gt; target report All the targets are now up to date! vis_drake_graph(script_based_plan, targets_only = TRUE) 8.8 Final thoughts Countless data science workflows consist of numbered imperative scripts, and code_to_function() lets drake accommodate script-based projects too big and cumbersome to refactor. However, for new projects, we strongly recommend that you write functions. Functions help organize your thoughts, and they improve portability, readability, and compatibility with drake. For a deeper discussion of functions and their role in drake, consider watching the webinar recording of the 2019-09-23 rOpenSci Community Call. Even old projects are sometimes pliable enough to refactor into functions, especially with the new Rclean package. "],
["churn.html", "Chapter 9 Customer churn and deep learning 9.1 Packages 9.2 Functions 9.3 Plan 9.4 Dependency graph 9.5 Run the models 9.6 Inspect the results 9.7 Add models 9.8 Inspect the results again 9.9 Update your code 9.10 History and provenance", " Chapter 9 Customer churn and deep learning drake is designed for workflows with long runtimes, and a major use case is deep learning. This chapter demonstrates how to leverage drake to manage a deep learning workflow. The original example comes from a blog post by Matt Dancho, and the chapter’s content itself comes directly from this R notebook, part of an RStudio Solutions Engineering example demonstrating TensorFlow in R. The notebook is modified and redistributed under the terms of the Apache 2.0 license, copyright RStudio (details here). 9.1 Packages First, we load our packages into a fresh R session. library(drake) library(keras) library(tidyverse) library(rsample) library(recipes) library(yardstick) 9.2 Functions drake is R-focused and function-oriented. We create functions to preprocess the data, prepare_recipe &lt;- function(data) { data %&gt;% training() %&gt;% recipe(Churn ~ .) %&gt;% step_rm(customerID) %&gt;% step_naomit(all_outcomes(), all_predictors()) %&gt;% step_discretize(tenure, options = list(cuts = 6)) %&gt;% step_log(TotalCharges) %&gt;% step_mutate(Churn = ifelse(Churn == &quot;Yes&quot;, 1, 0)) %&gt;% step_dummy(all_nominal(), -all_outcomes()) %&gt;% step_center(all_predictors(), -all_outcomes()) %&gt;% step_scale(all_predictors(), -all_outcomes()) %&gt;% prep() } define a keras model, exposing arguments to set the dimensionality and activation functions of the layers, define_model &lt;- function(rec, units1, units2, act1, act2, act3) { input_shape &lt;- ncol( juice(rec, all_predictors(), composition = &quot;matrix&quot;) ) keras_model_sequential() %&gt;% layer_dense( units = units1, kernel_initializer = &quot;uniform&quot;, activation = act1, input_shape = input_shape ) %&gt;% layer_dropout(rate = 0.1) %&gt;% layer_dense( units = units2, kernel_initializer = &quot;uniform&quot;, activation = act2 ) %&gt;% layer_dropout(rate = 0.1) %&gt;% layer_dense( units = 1, kernel_initializer = &quot;uniform&quot;, activation = act3 ) } train a model, train_model &lt;- function( rec, units1 = 16, units2 = 16, act1 = &quot;relu&quot;, act2 = &quot;relu&quot;, act3 = &quot;sigmoid&quot; ) { model &lt;- define_model( rec = rec, units1 = units1, units2 = units2, act1 = act1, act2 = act2, act3 = act3 ) compile( model, optimizer = &quot;adam&quot;, loss = &quot;binary_crossentropy&quot;, metrics = c(&quot;accuracy&quot;) ) x_train_tbl &lt;- juice( rec, all_predictors(), composition = &quot;matrix&quot; ) y_train_vec &lt;- juice(rec, all_outcomes()) %&gt;% pull() fit( object = model, x = x_train_tbl, y = y_train_vec, batch_size = 32, epochs = 32, validation_split = 0.3, verbose = 0 ) model } compare predictions against reality, confusion_matrix &lt;- function(data, rec, model) { testing_data &lt;- bake(rec, testing(data)) x_test_tbl &lt;- testing_data %&gt;% select(-Churn) %&gt;% as.matrix() y_test_vec &lt;- testing_data %&gt;% select(Churn) %&gt;% pull() yhat_keras_class_vec &lt;- model %&gt;% predict_classes(x_test_tbl) %&gt;% as.factor() %&gt;% fct_recode(yes = &quot;1&quot;, no = &quot;0&quot;) yhat_keras_prob_vec &lt;- model %&gt;% predict_proba(x_test_tbl) %&gt;% as.vector() test_truth &lt;- y_test_vec %&gt;% as.factor() %&gt;% fct_recode(yes = &quot;1&quot;, no = &quot;0&quot;) estimates_keras_tbl &lt;- tibble( truth = test_truth, estimate = yhat_keras_class_vec, class_prob = yhat_keras_prob_vec ) estimates_keras_tbl %&gt;% conf_mat(truth, estimate) } and compare the performance of multiple models. compare_models &lt;- function(...) { name &lt;- match.call()[-1] %&gt;% as.character() df &lt;- map_df(list(...), summary) %&gt;% filter(.metric %in% c(&quot;accuracy&quot;, &quot;sens&quot;, &quot;spec&quot;)) %&gt;% mutate(name = rep(name, each = n() / length(name))) %&gt;% rename(metric = .metric, estimate = .estimate) ggplot(df) + geom_line(aes(x = metric, y = estimate, color = name, group = name)) + theme_gray(24) } 9.3 Plan Next, we define our workflow in a drake plan. We will prepare the data, train different models with different activation functions, and compare the models in terms of performance. activations &lt;- c(&quot;relu&quot;, &quot;sigmoid&quot;) plan &lt;- drake_plan( data = read_csv(file_in(&quot;customer_churn.csv&quot;), col_types = cols()) %&gt;% initial_split(prop = 0.3), rec = prepare_recipe(data), model = target( train_model(rec, act1 = act), format = &quot;keras&quot;, # Supported in drake &gt; 7.5.2 to store models properly. transform = map(act = !!activations) ), conf = target( confusion_matrix(data, rec, model), transform = map(model, .id = act) ), metrics = target( compare_models(conf), transform = combine(conf) ) ) The plan is a data frame with the steps we are going to do. plan #&gt; # A tibble: 7 x 3 #&gt; target command format #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; #&gt; 1 data read_csv(file_in(&quot;customer_churn.csv&quot;), col_types = cols(… &lt;NA&gt; #&gt; 2 rec prepare_recipe(data) … &lt;NA&gt; #&gt; 3 model_relu train_model(rec, act1 = &quot;relu&quot;) … keras #&gt; 4 model_sigmo… train_model(rec, act1 = &quot;sigmoid&quot;) … keras #&gt; 5 conf_relu confusion_matrix(data, rec, model_relu) … &lt;NA&gt; #&gt; 6 conf_sigmoid confusion_matrix(data, rec, model_sigmoid) … &lt;NA&gt; #&gt; 7 metrics compare_models(conf_relu, conf_sigmoid) … &lt;NA&gt; 9.4 Dependency graph The graph visualizes the dependency relationships among the steps of the workflow. vis_drake_graph(plan) 9.5 Run the models Call make() to actually run the workflow. make(plan) #&gt; target data #&gt; target rec #&gt; target model_sigmoid #&gt; target model_relu #&gt; target conf_sigmoid #&gt; target conf_relu #&gt; target metrics 9.6 Inspect the results The two models performed about the same. readd(metrics) # see also loadd() 9.7 Add models Let’s try the softmax activation function. activations &lt;- c(&quot;relu&quot;, &quot;sigmoid&quot;, &quot;softmax&quot;) plan &lt;- drake_plan( data = read_csv(file_in(&quot;customer_churn.csv&quot;), col_types = cols()) %&gt;% initial_split(prop = 0.3), rec = prepare_recipe(data), model = target( train_model(rec, act1 = act), format = &quot;keras&quot;, # Supported in drake &gt; 7.5.2 to store models properly. transform = map(act = !!activations) ), conf = target( confusion_matrix(data, rec, model), transform = map(model, .id = act) ), metrics = target( compare_models(conf), transform = combine(conf) ) ) vis_drake_graph(plan) # see also outdated() and predict_runtime() make() skips the relu and sigmoid models because they are already up to date. (Their dependencies did not change.) Only the softmax model needs to run. make(plan) #&gt; target model_softmax #&gt; target conf_softmax #&gt; target metrics 9.8 Inspect the results again readd(metrics) # see also loadd() 9.9 Update your code If you change upstream functions, even nested ones, drake automatically refits the affected models. Let’s increase dropout in both layers. define_model &lt;- function(rec, units1, units2, act1, act2, act3) { input_shape &lt;- ncol( juice(rec, all_predictors(), composition = &quot;matrix&quot;) ) keras_model_sequential() %&gt;% layer_dense( units = units1, kernel_initializer = &quot;uniform&quot;, activation = act1, input_shape = input_shape ) %&gt;% layer_dropout(rate = 0.15) %&gt;% # Changed from 0.1 to 0.15. layer_dense( units = units2, kernel_initializer = &quot;uniform&quot;, activation = act2 ) %&gt;% layer_dropout(rate = 0.15) %&gt;% # Changed from 0.1 to 0.15. layer_dense( units = 1, kernel_initializer = &quot;uniform&quot;, activation = act3 ) } All the models and downstream results are affected. make(plan) #&gt; target model_relu #&gt; target model_softmax #&gt; target model_sigmoid #&gt; target conf_relu #&gt; target conf_softmax #&gt; target conf_sigmoid #&gt; target metrics 9.10 History and provenance drake version 7.5.0 and above tracks history and provenance. You can see which models you ran, when you ran them, how long they took, and which settings you tried (i.e. named arguments to function calls in your commands). history &lt;- drake_history() history #&gt; # A tibble: 17 x 10 #&gt; target current built exists hash command seed runtime prop act1 #&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 conf_r… FALSE 2020-02… TRUE 5041… &quot;confusion_… 4.05e8 0.674 NA &lt;NA&gt; #&gt; 2 conf_r… TRUE 2020-02… TRUE d108… &quot;confusion_… 4.05e8 0.396 NA &lt;NA&gt; #&gt; 3 conf_s… FALSE 2020-02… TRUE e6f7… &quot;confusion_… 1.93e9 0.421 NA &lt;NA&gt; #&gt; 4 conf_s… TRUE 2020-02… TRUE d0ce… &quot;confusion_… 1.93e9 0.412 NA &lt;NA&gt; #&gt; 5 conf_s… FALSE 2020-02… TRUE f77e… &quot;confusion_… 1.80e9 0.404 NA &lt;NA&gt; #&gt; 6 conf_s… TRUE 2020-02… TRUE 4e73… &quot;confusion_… 1.80e9 0.419 NA &lt;NA&gt; #&gt; 7 data TRUE 2020-02… TRUE ca84… &quot;read_csv(f… 1.29e9 0.0870 0.3 &lt;NA&gt; #&gt; 8 metrics FALSE 2020-02… TRUE ea0b… &quot;compare_mo… 1.21e9 0.0260 NA &lt;NA&gt; #&gt; 9 metrics FALSE 2020-02… TRUE 6158… &quot;compare_mo… 1.21e9 0.0340 NA &lt;NA&gt; #&gt; 10 metrics TRUE 2020-02… TRUE 71c2… &quot;compare_mo… 1.21e9 0.0260 NA &lt;NA&gt; #&gt; 11 model_… FALSE 2020-02… TRUE 579c… &quot;train_mode… 1.47e9 4.12 NA relu #&gt; 12 model_… TRUE 2020-02… TRUE 2512… &quot;train_mode… 1.47e9 4.16 NA relu #&gt; 13 model_… FALSE 2020-02… TRUE 27b6… &quot;train_mode… 1.26e9 5.86 NA sigm… #&gt; 14 model_… TRUE 2020-02… TRUE 7776… &quot;train_mode… 1.26e9 4.21 NA sigm… #&gt; 15 model_… FALSE 2020-02… TRUE dcfc… &quot;train_mode… 8.05e8 4.19 NA soft… #&gt; 16 model_… TRUE 2020-02… TRUE 7009… &quot;train_mode… 8.05e8 4.07 NA soft… #&gt; 17 rec TRUE 2020-02… TRUE 2cfc… &quot;prepare_re… 6.29e8 0.188 NA &lt;NA&gt; And as long as you did not run clean(garbage_collection = TRUE), you can get the old data back. Let’s find the oldest run of the relu model. hash &lt;- history %&gt;% filter(act1 == &quot;relu&quot;) %&gt;% pull(hash) %&gt;% head(n = 1) drake_cache()$get_value(hash) #&gt; Model #&gt; Model: &quot;sequential_1&quot; #&gt; ________________________________________________________________________________ #&gt; Layer (type) Output Shape Param # #&gt; ================================================================================ #&gt; dense_3 (Dense) (None, 16) 576 #&gt; ________________________________________________________________________________ #&gt; dropout_2 (Dropout) (None, 16) 0 #&gt; ________________________________________________________________________________ #&gt; dense_4 (Dense) (None, 16) 272 #&gt; ________________________________________________________________________________ #&gt; dropout_3 (Dropout) (None, 16) 0 #&gt; ________________________________________________________________________________ #&gt; dense_5 (Dense) (None, 1) 17 #&gt; ================================================================================ #&gt; Total params: 865 #&gt; Trainable params: 865 #&gt; Non-trainable params: 0 #&gt; ________________________________________________________________________________ "],
["packages.html", "Chapter 10 An analysis of R package download trends 10.1 Get the code. 10.2 Overview 10.3 Analysis 10.4 Other ways to trigger downloads", " Chapter 10 An analysis of R package download trends This chapter explores R package download trends using the cranlogs package, and it shows how drake’s custom triggers can help with workflows with remote data sources. 10.1 Get the code. Write the code files to your workspace. drake_example(&quot;packages&quot;) The new packages folder now includes a file structure of a serious drake project, plus an interactive-tutorial.R to narrate the example. The code is also online here. 10.2 Overview This small data analysis project explores some trends in R package downloads over time. The datasets are downloaded using the cranlogs package. library(cranlogs) cran_downloads(packages = &quot;dplyr&quot;, when = &quot;last-week&quot;) #&gt; date count package #&gt; 1 2020-01-29 45781 dplyr #&gt; 2 2020-01-30 44282 dplyr #&gt; 3 2020-01-31 44269 dplyr #&gt; 4 2020-02-01 33725 dplyr #&gt; 5 2020-02-02 31395 dplyr #&gt; 6 2020-02-03 57613 dplyr #&gt; 7 2020-02-04 57062 dplyr Above, each count is the number of times dplyr was downloaded from the RStudio CRAN mirror on the given day. To stay up to date with the latest download statistics, we need to refresh the data frequently. With drake, we can bring all our work up to date without restarting everything from scratch. 10.3 Analysis First, we load the required packages. drake detects the packages you install and load. library(cranlogs) library(drake) library(dplyr) library(ggplot2) library(knitr) library(rvest) We will want custom functions to summarize the CRAN logs we download. make_my_table &lt;- function(downloads){ group_by(downloads, package) %&gt;% summarize(mean_downloads = mean(count)) } make_my_plot &lt;- function(downloads){ ggplot(downloads) + geom_line(aes(x = date, y = count, group = package, color = package)) } Next, we generate the plan. We want to explore the daily downloads from the knitr, Rcpp, and ggplot2 packages. We will use the cranlogs package to get daily logs of package downloads from RStudio’s CRAN mirror. In our drake_plan(), we declare targets older and recent to kcontain snapshots of the logs. The following drake_plan() syntax is described here, which is supported in drake 7.0.0 and above. plan &lt;- drake_plan( older = cran_downloads( packages = c(&quot;knitr&quot;, &quot;Rcpp&quot;, &quot;ggplot2&quot;), from = &quot;2016-11-01&quot;, to = &quot;2016-12-01&quot; ), recent = target( command = cran_downloads( packages = c(&quot;knitr&quot;, &quot;Rcpp&quot;, &quot;ggplot2&quot;), when = &quot;last-month&quot; ), trigger = trigger(change = latest_log_date()) ), averages = target( make_my_table(data), transform = map(data = c(older, recent)) ), plot = target( make_my_plot(data), transform = map(data) ), report = knit( knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE ) ) Notice the custom trigger for the target recent. Here, we are telling drake to rebuild recent whenever a new day’s log is uploaded to http://cran-logs.rstudio.com. In other words, drake keeps track of the return value of latest_log_date() and recomputes recent (during make()) if that value changed since the last make(). Here, latest_log_date() is one of our custom imported functions. We use it to scrape http://cran-logs.rstudio.com using the rvest package. latest_log_date &lt;- function(){ read_html(&quot;http://cran-logs.rstudio.com/&quot;) %&gt;% html_nodes(&quot;li:last-of-type&quot;) %&gt;% html_nodes(&quot;a:last-of-type&quot;) %&gt;% html_text() %&gt;% max } Now, we run the project to download the data and analyze it. The results will be summarized in the knitted report, report.md, but you can also read the results directly from the cache. make(plan) #&gt; target older #&gt; target recent #&gt; target averages_older #&gt; target plot_older #&gt; target averages_recent #&gt; target plot_recent #&gt; target report readd(averages_recent) #&gt; # A tibble: 3 x 2 #&gt; package mean_downloads #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ggplot2 33808. #&gt; 2 knitr 25821. #&gt; 3 Rcpp 33260. readd(averages_older) #&gt; # A tibble: 3 x 2 #&gt; package mean_downloads #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ggplot2 14641. #&gt; 2 knitr 9069. #&gt; 3 Rcpp 14408. readd(plot_recent) readd(plot_older) If we run make() again right away, we see that everything is up to date. But if we wait until a new day’s log is uploaded, make() will update recent and everything that depends on it. make(plan) #&gt; All targets are already up to date. To visualize the build behavior, you can plot the dependency network. vis_drake_graph(plan) 10.4 Other ways to trigger downloads Sometimes, our remote data sources get revised, and web scraping may not be the best way to detect changes. We may want to look at our remote dataset’s modification time or HTTP ETag. To see how this works, consider the CRAN log file from February 9, 2018. url &lt;- &quot;http://cran-logs.rstudio.com/2018/2018-02-09-r.csv.gz&quot; We can track the modification date using the httr package. library(httr) # For querying websites. HEAD(url)$headers[[&quot;last-modified&quot;]] #&gt; [1] &quot;Mon, 12 Feb 2018 16:34:48 GMT&quot; In our drake plan, we can track this timestamp and trigger a download whenever it changes. plan &lt;- drake_plan( logs = target( get_logs(url), trigger = trigger(change = HEAD(url)$headers[[&quot;last-modified&quot;]]) ) ) plan #&gt; # A tibble: 1 x 3 #&gt; target command trigger #&gt; &lt;chr&gt; &lt;expr&gt; &lt;expr&gt; #&gt; 1 logs get_logs(url) trigger(change = HEAD(url)$headers[[&quot;last-modified&quot;]]) where library(R.utils) # For unzipping the files we download. library(curl) # For downloading data. get_logs &lt;- function(url){ curl_download(url, &quot;logs.csv.gz&quot;) # Get a big file. gunzip(&quot;logs.csv.gz&quot;, overwrite = TRUE) # Unzip it. out &lt;- read.csv(&quot;logs.csv&quot;, nrows = 4) # Extract the data you need. unlink(c(&quot;logs.csv.gz&quot;, &quot;logs.csv&quot;)) # Remove the big files out # Value of the target. } When we are ready, we run the workflow. make(plan) #&gt; target logs readd(logs) #&gt; date time size version os country ip_id #&gt; 1 2018-02-09 13:01:13 82375220 3.4.3 win RO 1 #&gt; 2 2018-02-09 13:02:06 74286541 3.3.3 win US 2 #&gt; 3 2018-02-09 13:02:10 82375216 3.4.3 win US 3 #&gt; 4 2018-02-09 13:03:30 82375220 3.4.3 win IS 4 If the log file at the url ever changes, the timestamp will update remotely, and make() will download the file again. "],
["gsp.html", "Chapter 11 Finding the best model of gross state product 11.1 Get the code. 11.2 Objective and methods 11.3 Data 11.4 Analysis 11.5 Results 11.6 Comparison with GNU Make 11.7 References", " Chapter 11 Finding the best model of gross state product The following data analysis workflow shows off drake’s ability to generate lots of reproducibly-tracked tasks with ease. The same technique would be cumbersome, even intractable, with GNU Make. 11.1 Get the code. Write the code files to your workspace. drake_example(&quot;gsp&quot;) The new gsp folder now includes a file structure of a serious drake project, plus an interactive-tutorial.R to narrate the example. The code is also online here. 11.2 Objective and methods The goal is to search for factors closely associated with the productivity of states in the USA around the 1970s and 1980s. For the sake of simplicity, we use gross state product as a metric of productivity, and we restrict ourselves to multiple linear regression models with three variables. For each of the 84 possible models, we fit the data and then evaluate the root mean squared prediction error (RMSPE). \\[ \\begin{aligned} \\text{RMSPE} = \\sqrt{(\\text{y} - \\widehat{y})^T(y - \\widehat{y})} \\end{aligned} \\] Here, \\(y\\) is the vector of observed gross state products in the data, and \\(\\widehat{y}\\) is the vector of predicted gross state products under one of the models. We take the best variables to be the triplet in the model with the lowest RMSPE. 11.3 Data The Produc dataset from the Ecdat package contains data on the Gross State Product from 1970 to 1986. Each row is a single observation on a single state for a single year. The dataset has the following variables as columns. See the references later in this report for more details. gsp: gross state product. state: the state. year: the year. pcap: private capital stock. hwy: highway and streets. water: water and sewer facilities. util: other public buildings and structures. pc: public capital. emp: labor input measured by the employment in non-agricultural payrolls. unemp: state unemployment rate. library(Ecdat) data(Produc) head(Produc) #&gt; state year pcap hwy water util pc gsp emp unemp #&gt; 1 ALABAMA 1970 15032.67 7325.80 1655.68 6051.20 35793.80 28418 1010.5 4.7 #&gt; 2 ALABAMA 1971 15501.94 7525.94 1721.02 6254.98 37299.91 29375 1021.9 5.2 #&gt; 3 ALABAMA 1972 15972.41 7765.42 1764.75 6442.23 38670.30 31303 1072.3 4.7 #&gt; 4 ALABAMA 1973 16406.26 7907.66 1742.41 6756.19 40084.01 33430 1135.5 3.9 #&gt; 5 ALABAMA 1974 16762.67 8025.52 1734.85 7002.29 42057.31 33749 1169.8 5.5 #&gt; 6 ALABAMA 1975 17316.26 8158.23 1752.27 7405.76 43971.71 33604 1155.4 7.7 11.4 Analysis First, we load the required packages. drake is aware of all the packages you load with library() or require(). library(biglm) # lightweight models, easier to store than with lm() library(drake) library(Ecdat) # econometrics datasets library(ggplot2) library(knitr) library(purrr) library(tidyverse) Next, we construct our plan. The following code uses drake’s special new language for generating plans (learn more here). predictors &lt;- setdiff(colnames(Produc), &quot;gsp&quot;) # We will try all combinations of three covariates. combos &lt;- combn(predictors, 3) %&gt;% t() %&gt;% as.data.frame(stringsAsFactors = FALSE) %&gt;% setNames(c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)) head(combos) #&gt; x1 x2 x3 #&gt; 1 state year pcap #&gt; 2 state year hwy #&gt; 3 state year water #&gt; 4 state year util #&gt; 5 state year pc #&gt; 6 state year emp # We need to list each covariate as a symbol. for (col in colnames(combos)) { combos[[col]] &lt;- rlang::syms(combos[[col]]) } # Requires drake &gt;= 7.0.0 or the development version # at github.com/ropensci/drake. # Install with remotes::install_github(&quot;ropensci/drake&quot;). plan &lt;- drake_plan( model = target( biglm(gsp ~ x1 + x2 + x3, data = Ecdat::Produc), transform = map(.data = !!combos) # Remember the bang-bang!! ), rmspe_i = target( get_rmspe(model, Ecdat::Produc), transform = map(model) ), rmspe = target( bind_rows(rmspe_i, .id = &quot;model&quot;), transform = combine(rmspe_i) ), plot = ggsave( filename = file_out(&quot;rmspe.pdf&quot;), plot = plot_rmspe(rmspe), width = 8, height = 8 ), report = knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) ) plan #&gt; # A tibble: 171 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 model_state_year_pcap biglm(gsp ~ state + year + pcap, data = Ecdat::Produc… #&gt; 2 model_state_year_hwy biglm(gsp ~ state + year + hwy, data = Ecdat::Produc)… #&gt; 3 model_state_year_water biglm(gsp ~ state + year + water, data = Ecdat::Produ… #&gt; 4 model_state_year_util biglm(gsp ~ state + year + util, data = Ecdat::Produc… #&gt; 5 model_state_year_pc biglm(gsp ~ state + year + pc, data = Ecdat::Produc) … #&gt; 6 model_state_year_emp biglm(gsp ~ state + year + emp, data = Ecdat::Produc)… #&gt; 7 model_state_year_unemp biglm(gsp ~ state + year + unemp, data = Ecdat::Produ… #&gt; 8 model_state_pcap_hwy biglm(gsp ~ state + pcap + hwy, data = Ecdat::Produc)… #&gt; 9 model_state_pcap_water biglm(gsp ~ state + pcap + water, data = Ecdat::Produ… #&gt; 10 model_state_pcap_util biglm(gsp ~ state + pcap + util, data = Ecdat::Produc… #&gt; # … with 161 more rows We also need to define functions for summaries and plots. get_rmspe &lt;- function(model_fit, data){ y &lt;- data$gsp yhat &lt;- as.numeric(predict(model_fit, newdata = data)) terms &lt;- attr(model_fit$terms, &quot;term.labels&quot;) tibble( rmspe = sqrt(mean((y - yhat)^2)), # nolint X1 = terms[1], X2 = terms[2], X3 = terms[3] ) } plot_rmspe &lt;- function(rmspe){ ggplot(rmspe) + geom_histogram(aes(x = rmspe), bins = 15) } We have a report.Rmd file to summarize our results at the end. drake_example(&quot;gsp&quot;) file.copy(from = &quot;gsp/report.Rmd&quot;, to = &quot;.&quot;, overwrite = TRUE) #&gt; [1] TRUE We can inspect the project before we run it. vis_drake_graph(plan) Now, we can run the project. make(plan, verbose = 0L) 11.5 Results Here are the root mean squared prediction errors of all the models. results &lt;- readd(rmspe) library(ggplot2) plot_rmspe(rmspe = results) And here are the best models. The best variables are in the top row under X1, X2, and X3. head(results[order(results$rmspe, decreasing = FALSE), ]) #&gt; # A tibble: 6 x 5 #&gt; model rmspe X1 X2 X3 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 17 2614. state hwy emp #&gt; 2 21 2665. state water emp #&gt; 3 24 2666. state util emp #&gt; 4 26 2666. state pc emp #&gt; 5 12 2675. state pcap emp #&gt; 6 28 2693. state emp unemp 11.6 Comparison with GNU Make If we were using Make instead of drake with the same set of targets, the analogous Makefile would look something like this pseudo-code sketch. models = model_state_year_pcap.rds model_state_year_hwy.rds ... # 84 of these model_% Rscript -e 'saveRDS(lm(...), ...)' rmspe_%: model_% Rscript -e 'saveRDS(get_rmspe(...), ...)' rmspe.rds: rmspe_% Rscript -e 'saveRDS(dplyr::bind_rows(...), ...)' rmspe.pdf: rmspe.rds Rscript -e 'ggplot2::ggsave(plot_rmspe(readRDS(\"rmspe.rds\")), \"rmspe.pdf\")' report.md: report.Rmd Rscript -e 'knitr::knit(\"report.Rmd\")' There are three main disadvantages to this approach. Every target requires a new call to Rscript, which means that more time is spent initializing R sessions than doing the actual work. The user must micromanage nearly one hundred output files (in this case, *.rds files), which is cumbersome, messy, and inconvenient. drake, on the other hand, automatically manages storage using a storr cache. The user needs to write the names of the 84 models near the top of the Makefile, which is less convenient than maintaining a data frame in R. 11.7 References Baltagi, Badi H (2003). Econometric analysis of panel data, John Wiley and sons, http://www.wiley.com/legacy/wileychi/baltagi/. Baltagi, B. H. and N. Pinnoi (1995). “Public capital stock and state productivity growth: further evidence”, Empirical Economics, 20, 351-359. Munnell, A. (1990). “Why has productivity growth declined? Productivity and public investment”&quot;, New England Economic Review, 3-22. Yves Croissant (2016). Ecdat: Data Sets for Econometrics. R package version 0.3-1. https://CRAN.R-project.org/package=Ecdat. "],
["hpc.html", "Chapter 12 High-performance computing 12.1 Start small 12.2 Let make() schedule your targets. 12.3 The master process 12.4 Parallel backends 12.5 The clustermq backend 12.6 The future backend 12.7 Advanced options", " Chapter 12 High-performance computing This chapter provides guidance on time-consuming drake workflows and high-level parallel computation. 12.1 Start small Before you jump into high-performance computing with a large workflow, consider running a downsized version to debug and test things first. That way, you can avoid consuming lots of computing resources until you are reasonably sure everything works. Create a test plan with drake_plan(max_expand = SMALL_NUMBER) before scaling up to the full set of targets, and take temporary shortcuts in your commands so your targets build more quickly for test mode. See this section on plans for details. 12.2 Let make() schedule your targets. When it comes time to activate high-performance computing, drake launches its own parallel workers and sends targets to those workers. The workers can be local processes or jobs on a cluster. drake uses your project’s implicit dependency graph to figure out which targets can run in parallel and which ones need to wait for dependencies. load_mtcars_example() # from https://github.com/wlandau/drake-examples/tree/master/mtcars vis_drake_graph(my_plan) You do not need to not micromanage how targets are scheduled, and you do not need to run simultaneous instances of make(). 12.3 The master process make() takes care of the jobs it launches, but make() itself is a job too, and it is your responsibility to manage it. 12.3.1 Master on a cluster Most clusters will let you submit make() as a job on a compute node. Let’s consider the Sun Grid Engine (SGE) as an example. First, we create a script that calls make() (or r_make()). # make.R source(&quot;R/packages.R&quot;) source(&quot;R/packages.R&quot;) source(&quot;R/packages.R&quot;) options( clustermq.scheduler = &quot;sge&quot;, # Created by drake_hpc_template_file(&quot;sge_clustermq.tmpl&quot;): clustermq.template = &quot;sge_clustermq.tmpl&quot; ) make( plan, parallelism = &quot;clustermq&quot;, jobs = 8, console_log_file = &quot;drake.log&quot; ) Then, we create a shell script (say, run.sh) to call make.R. This script may look different if you use a different scheduler such as SLURM. # run.sh #!/bin/bash #$ -j y # combine stdout/error in one file #$ -o log.out # output file #$ -cwd # use pwd as work dir #$ -V # use environment variable module load R # Uncomment if R is an environment module. R --no-save CMD BATCH make.R Finally, to run the whole workflow, we call qsub. qsub run.sh And here is what happens: A new job starts on the cluster with the configuration flags next to #$ in run.sh. run.sh opens R and runs make.R. make.R invokes drake using the make() function. make() launches 8 new jobs on the cluster So 9 simultaneous jobs run on the cluster and we avoid bothering the headnode / login node. 12.3.2 Local master Alternatively, you can run make() in a persistent background process. The following should work in the Mac/Linux terminal/shell. nohup nice -19 R --no-save CMD BATCH make.R & where: nohup: Keep the job running even if you log out of the machine. nice -19: This is a low-priority job that should not consume many resources. Other processes should take priority. R CMD BATCH: Run the R script in a fresh new R session. --no-save: do not save the workspace in a .RData file. &amp;: Run this job in the background so you can do other stuff in the terminal window. Alternatives to nohup include screen and Byobu. 12.4 Parallel backends Choose the parallel backend with the parallelism argument and set the jobs argument to scale the work appropriately. make(my_plan, parallelism = &quot;future&quot;, jobs = 2) The two primary backends with long term support are clustermq and future. If you can install ZeroMQ, the best choice is usually clustermq. (It is faster than future.) However, future is more accessible: it does not require ZeroMQ, it supports parallel computing on Windows, it can work with more restrictive wall time limits on clusters, and it can deploy targets to Docker images (drake_example(&quot;Docker-psock&quot;)). 12.5 The clustermq backend 12.5.1 Persistent workers The make(parallelism = &quot;clustermq&quot;, jobs = 2) launches 2 parallel persistent workers. The master process assigns targets to workers, and the workers simultaneously traverse the dependency graph. 12.5.2 Installation Persistent workers require the clustermq R package, which in turn requires ZeroMQ. Please refer to the clustermq installation guide for specific instructions. 12.5.3 On your local machine To run your targets in parallel over the cores of your local machine, set the global option below and run make(). options(clustermq.scheduler = &quot;multicore&quot;) make(plan, parallelism = &quot;clustermq&quot;, jobs = 2) 12.5.4 On a cluster Set the clustermq global options to register your computing resources. For SLURM: options(clustermq.scheduler = &quot;slurm&quot;, clustermq.template = &quot;slurm_clustermq.tmpl&quot;) Here, slurm_clustermq.tmpl is a template file with configuration details. Use drake_hpc_template_file() to write one of the available examples. drake_hpc_template_file(&quot;slurm_clustermq.tmpl&quot;) # Write the file slurm_clustermq.tmpl. After modifying slurm_clustermq.tmpl by hand to meet your needs, call make() as usual. make(plan, parallelism = &quot;clustermq&quot;, jobs = 4) 12.6 The future backend 12.6.1 Transient workers make(parallelism = &quot;future&quot;, jobs = 2) launches transient workers to build your targets. When a target is ready to build, the master process creates a fresh worker to build it, and the worker terminates when the target is done. jobs = 2 means that at most 2 transient workers are allowed to run at a given time. 12.6.2 Installation Install the future package. install.packages(&quot;future&quot;) # CRAN release # Alternatively, install the GitHub development version. devtools::install_github(&quot;HenrikBengtsson/future&quot;, ref = &quot;develop&quot;) If you intend to use a cluster, be sure to install the future.batchtools package too. The future ecosystem contains even more packages that extend future’s parallel computing functionality, such as future.callr. 12.6.3 On your local machine First, select a future plan to tell future how to create the workers. See this table for descriptions of the core options. future::plan(future::multiprocess) Next, run make(). make(plan, parallelism = &quot;future&quot;, jobs = 2) 12.6.4 On a cluster Install the future.batchtools package and use this list to select a future plan that matches your resources. You will also need a compatible template file with configuration details. As with clustermq, drake can generate some examples: drake_hpc_template_file(&quot;slurm_batchtools.tmpl&quot;) # Edit by hand. Next, register the template file with a plan. library(future.batchtools) future::plan(batchtools_slurm, template = &quot;slurm_batchtools.tmpl&quot;) Finally, run make(). make(plan, parallelism = &quot;future&quot;, jobs = 2) 12.7 Advanced options 12.7.1 Selectivity Some targets build so quickly that it is not worth sending them to parallel workers. To run these targets locally in the master process, define a special hpc column of your drake plan. Below, NA and TRUE are treated the same, and make(plan, parallelism = &quot;clustermq&quot;) only sends model_1 and model_2 to parallel workers. drake_plan( model = target( crazy_long_computation(index), transform = map(index = c(1, 2)) ), accuracy = target( summarize_accuracy(model), transform = combine(model), hpc = FALSE ), specificity = target( summarize_specificity(model), transform = combine(model), hpc = FALSE ), report = target( render(knitr_in(&quot;results.Rmd&quot;), output_file = file_out(&quot;results.html&quot;)), hpc = FALSE ) ) #&gt; # A tibble: 5 x 3 #&gt; target command hpc #&gt; &lt;chr&gt; &lt;expr&gt; &lt;lgl&gt; #&gt; 1 model_1 crazy_long_computation(1) … NA #&gt; 2 model_2 crazy_long_computation(2) … NA #&gt; 3 accuracy summarize_accuracy(model_1, model_2) … FALSE #&gt; 4 specificity summarize_specificity(model_1, model_2) … FALSE #&gt; 5 report render(knitr_in(&quot;results.Rmd&quot;), output_file = file_out(&quot;res… FALSE 12.7.2 Memory options By default, make() keeps targets in memory during runtime. Some targets are dependencies of other targets downstream, while others may be no longer actually need to be in memory. The memory_strategy argument to make() allows you to choose the tradeoff that best suits your project. Options: &quot;speed&quot;: Once a target is loaded in memory, just keep it there. This choice maximizes speed and hogs memory. &quot;memory&quot;: Just before building each new target, unload everything from memory except the target’s direct dependencies. This option conserves memory, but it sacrifices speed because each new target needs to reload any previously unloaded targets from storage. &quot;lookahead&quot;: Just before building each new target, search the dependency graph to find targets that will not be needed for the rest of the current make() session. In this mode, targets are only in memory if they need to be loaded, and we avoid superfluous reads from the cache. However, searching the graph takes time, and it could even double the computational overhead for large projects. 12.7.3 Storage options In make(caching = &quot;master&quot;), the workers send the targets to the master process, and the master process stores them one by one in the cache. caching = &quot;master&quot; is compatible with all storr cache formats, including the more esoteric ones like storr_dbi() and storr_environment(). In make(caching = &quot;worker&quot;), the parallel workers are responsible for writing the targets to the cache. Some output-heavy projects can benefit from this form of parallelism. However, it can sometimes add slowness on clusters due to lag from network file systems. And there are additional restrictions: All the workers must have the same file system and the same working directory as the master process. Only the default storr_rds() cache may be used. Other formats like storr_dbi() and storr_environment() cannot accommodate parallel cache operations. See the storage chapter for details. 12.7.4 The template argument for persistent workers For more control and flexibility in the clustermq backend, you can parameterize your template file and use the template argument of make(). For example, suppose you want to programatically set the number of “slots” (basically cores) per job on an SGE system (clustermq guide to SGE setup here). Begin with a parameterized template file sge_clustermq.tmpl with a custom n_slots placeholder. # File: sge_clustermq.tmpl # Modified from https://github.com/mschubert/clustermq/wiki/SGE #$ -N {{ job_name }} # job name #$ -t 1-{{ n_jobs }} # submit jobs as array #$ -j y # combine stdout/error in one file #$ -o {{ log_file | /dev/null }} # output file #$ -cwd # use pwd as work dir #$ -V # use environment variable #$ -pe smp {{ n_slots | 1 }} # request n_slots cores per job module load R ulimit -v $(( 1024 * {{ memory | 4096 }} )) CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(&quot;{{ master }}&quot;)&#39; Then when you run make(), use the template argument to set n_slots. options(clustermq.scheduler = &quot;sge&quot;, clustermq.template = &quot;sge_clustermq.tmpl&quot;) library(drake) load_mtcars_example() make( my_plan, parallelism = &quot;clustermq&quot;, jobs = 16, template = list(n_slots = 4) # Request 4 cores per persistent worker. ) Custom placeholders like n_slots are processed with the infuser package. 12.7.5 The resources column for transient workers Different targets may need different resources. For example, plan &lt;- drake_plan( data = download_data(), model = big_machine_learning_model(data) ) The model needs a GPU and multiple CPU cores, and the data only needs the bare minimum resources. Declare these requirements with target(), as below. This is equivalent to adding a new list column to the plan, where each element is a named list for the resources argument of future::future(). plan &lt;- drake_plan( data = target( download_data(), resources = list(cores = 1, gpus = 0) ), model = target( big_machine_learning_model(data), resources = list(cores = 4, gpus = 1) ) ) plan #&gt; # A tibble: 2 x 3 #&gt; target command resources #&gt; &lt;chr&gt; &lt;expr&gt; &lt;list&gt; #&gt; 1 data download_data() &lt;named list [2]&gt; #&gt; 2 model big_machine_learning_model(data) &lt;named list [2]&gt; str(plan$resources) #&gt; List of 2 #&gt; $ :List of 2 #&gt; ..$ cores: num 1 #&gt; ..$ gpus : num 0 #&gt; $ :List of 2 #&gt; ..$ cores: num 4 #&gt; ..$ gpus : num 1 Next, plug the names of your resources into the brew patterns of your batchtools template file. The following sge_batchtools.tmpl file shows how to do it, but the file itself probably requires modification before it will work with your own machine. #!/bin/bash #$ -cwd #$ -j y #$ -o &lt;%= log.file %&gt; #$ -V #$ -N &lt;%= job.name %&gt; #$ -pe smp &lt;%= resources[[&quot;cores&quot;]] %&gt; # CPU cores #$ -l gpu=&lt;%= resources[[&quot;gpus&quot;]] %&gt; # GPUs. Rscript -e &#39;batchtools::doJobCollection(&quot;&lt;%= uri %&gt;&quot;)&#39; exit 0 Finally, register the template file and run your project. library(drake) library(future.batchtools) future::plan(batchtools_sge, template = &quot;sge_batchtools.tmpl&quot;) make(plan, parallelism = &quot;future&quot;, jobs = 2) 12.7.6 Parallel computing within targets To recruit parallel processes within individual targets, we recommend the future.callr and furrr packages. Usage details depend on the parallel backend you choose for make(). If you must write custom code with mclapply(), please read the subsection below on locked bindings/environments. 12.7.6.1 Locally Use future.callr and furrr normally. library(drake) # The targets just collect the process IDs of the callr processes. plan &lt;- drake_plan( x = furrr::future_map_int(1:2, function(x) Sys.getpid()), y = furrr::future_map_int(1:2, function(x) Sys.getpid()) ) # Tell the drake targets to fork up to 4 callr processes. future::plan(future.callr::callr) # Build the targets. make(plan) # Process IDs of the local workers of x: readd(x) 12.7.6.2 Persistent workers Each persistent worker needs its own future::plan(), which we set with the prework argument of make(). The following example uses SGE. To learn about templates for other clusters, please consult the clustermq documentation. library(drake) # The targets just collect the process IDs of the callr processes. plan &lt;- drake_plan( x = furrr::future_map_int(1:2, function(x) Sys.getpid()), y = furrr::future_map_int(1:2, function(x) Sys.getpid()) ) # Write a template file for clustermq. writeLines( c( &quot;#!/bin/bash&quot;, &quot;#$ -N {{ job_name }} # job name&quot;, &quot;#$ -t 1-{{ n_jobs }} # submit jobs as array&quot;, &quot;#$ -j y # combine stdout/error in one file&quot;, &quot;#$ -o {{ log_file | /dev/null }} # output file&quot;, &quot;#$ -cwd # use pwd as work dir&quot;, &quot;#$ -V # use environment variables&quot;, &quot;#$ -pe smp 4 # request 4 cores per job&quot;, &quot;module load R-qualified/3.5.2 # if loading R from an environment module&quot;, &quot;ulimit -v $(( 1024 * {{ memory | 4096 }} ))&quot;, &quot;CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(\\&quot;{{ master }}\\&quot;)&#39;&quot; ), &quot;sge_clustermq.tmpl&quot; ) # Register the scheduler and template file with clustermq. options( clustermq.scheduler = &quot;sge&quot;, clustermq.template = &quot;sge_clustermq.tmpl&quot; ) # Build the targets. make( plan, parallelism = &quot;clustermq&quot;, jobs = 2, # Each of the two workers can spawn up to 4 local processes. prework = quote(future::plan(future.callr::callr)) ) # Process IDs of the local workers of x: readd(x) 12.7.6.3 Transient workers As explained in the future vignette, we can nest our future::plans(). Each target gets its own remote job, and each job can spawn up to 4 local callr processes. The following example uses SGE. To learn about templates for other clusters, please consult the future.batchtools documentation. library(drake) # The targets just collect the process IDs of the callr processes. plan &lt;- drake_plan( x = furrr::future_map_int(1:2, function(x) Sys.getpid()), y = furrr::future_map_int(1:2, function(x) Sys.getpid()) ) # Write a template file for future.batchtools. writeLines( c( &quot;#!/bin/bash&quot;, &quot;#$ -cwd # use pwd as work dir&quot;, &quot;#$ -j y # combine stdout/error in one file&quot;, &quot;#$ -o &lt;%= log.file %&gt; # output file&quot;, &quot;#$ -V # use environment variables&quot;, &quot;#$ -N &lt;%= job.name %&gt; # job name&quot;, &quot;#$ -pe smp 4 # 4 cores per job&quot;, &quot;module load R # if loading R from an environment module&quot;, &quot;Rscript -e &#39;batchtools::doJobCollection(\\&quot;&lt;%= uri %&gt;\\&quot;)&#39;&quot;, &quot;exit 0&quot; ), &quot;sge_batchtools.tmpl&quot; ) # In our nested plans, each target gets its own remote SGE job, # and each worker can spawn up to 4 `callr` processes. future::plan( list( future::tweak( future.batchtools::batchtools_sge, template = &quot;sge_batchtools.tmpl&quot; ), future.callr::callr ) ) # Build the targets. make(plan, parallelism = &quot;future&quot;, jobs = 2) # Process IDs of the local workers of x: readd(x) 12.7.6.4 Number of local workers per target By default, future::availableCores() determines the number of local callr workers. To better manage resources, you may wish to further restrict the number of callr workers for all targets in the plan, e.g. future::plan(future::callr, workers = 4L) or: future::plan( list( future::tweak( future.batchtools::batchtools_sge, template = &quot;sge_batchtools.tmpl&quot; ), future::tweak(future.callr::callr, workers = 4L) ) ) Alternatively, you can use chunking to prevent individual targets from using too many workers, e.g. furrr::future_map(.options = furrr::future_options(scheduling = 4)). Here, the scheduling argument sets the average number of futures per worker. 12.7.6.5 Locked binding/environment errors Some workflows unavoidably use mclapply(), which is known to modify the global environment against drake’s will. If you are stuck, there are two workarounds. Use make(lock_envir = FALSE). Use the envir argument of make(). That way, drake locks your special custom environment instead of the global environment. # Load the main example: https://github.com/wlandau/drake-examples library(drake) drake_example(&quot;main&quot;) setwd(&quot;main&quot;) # Define and populate a special custom environment. envir &lt;- new.env(parent = globalenv()) source(&quot;R/packages.R&quot;, local = envir) source(&quot;R/functions.R&quot;, local = envir) source(&quot;R/plan.R&quot;, local = envir) # Check the contents of your environments. ls(envir) # Should have your functions and plan ls() # The global environment should only have what you started with. # Build the targets using your custom environment make(envir$plan, envir = envir) 12.7.7 Custom job schedulers It is possible to supply a custom job scheduler function to the parallelism argument of make(). The backend_future_lapply_staged() function from the drake.future.lapply.staged package is an example. You might consider writing your own such function if you wish to Experiment with a more efficient job scheduler before proposing a patch to core drake, or Aggressively optimize drake for your specialized computing resources. This feature is very advanced, and you should only attempt it in production if you really know what you are doing. Use at your own risk. 12.7.8 Hasty mode The drake.hasty package is a bare-bones experimental spin-off of drake. It sacrifices reproducibility to aggressively boost speed when scheduling and executing your targets. It is not recommended for most serious production use cases, but it can useful for experimentation. "],
["time.html", "Chapter 13 Time: speed, time logging, prediction, and strategy 13.1 Why is drake so slow? 13.2 Time logging 13.3 Predict total runtime 13.4 Strategize your high-performance computing", " Chapter 13 Time: speed, time logging, prediction, and strategy 13.1 Why is drake so slow? 13.1.1 Help us find out! If you encounter slowness, please report it to https://github.com/ropensci/drake/issues and we will do our best to speed up drake for your use case. Please include a reproducible example and tell us about your operating system and version of R. In addition, flame graphs from the proffer package really help us identify bottlenecks. 13.1.2 Too many targets? make() and friends tend to slow down if you have a huge number of targets. There are unavoidable overhead costs from storing each single target and checking if it is up to date, so please read this advice on choosing good targets and consider dividing your work into a manageably small number of meaningful targets. Dynamic branching can also help in many cases. 13.1.3 Big data? drake saves the return value of each target to on disk storage. So in addition to dividing your work into a smaller number of targets, specialized storage formats can help speed things up. It may also be worth reflecting on how much data you really need to store. And if the cache is too big, the storage chapter has advice for downsizing it. 13.1.4 Aggressive shortcuts If your plan still needs tens of thousands of targets, you can take aggressive shortcuts to make things run faster. make( plan, verbose = 0L, # Console messages can pile up runtime. log_progress = FALSE, # progress() will be useless. log_build_times = FALSE, # build_times() will be useless. recoverable = FALSE, # make(recover = TRUE) cannot be used later. history = FALSE, # drake_history() cannot be used later. session_info = FALSE, # drake_get_session_info() cannot be used later. lock_envir = FALSE, # See https://docs.ropensci.org/drake/reference/make.html#self-invalidation. ) 13.2 Time logging Thanks to Jasper Clarkberg, drake records how long it takes to build each target. For large projects that take hours or days to run, this feature becomes important for planning and execution. library(drake) load_mtcars_example() # from https://github.com/wlandau/drake-examples/tree/master/mtcars make(my_plan) #&gt; target large #&gt; target small #&gt; target regression1_large #&gt; target regression2_large #&gt; target regression2_small #&gt; target regression1_small #&gt; target coef_regression1_large #&gt; target summ_regression1_large #&gt; target coef_regression2_large #&gt; target summ_regression2_large #&gt; target coef_regression2_small #&gt; target summ_regression2_small #&gt; target summ_regression1_small #&gt; target coef_regression1_small #&gt; target report build_times(digits = 8) # From the cache. #&gt; # A tibble: 15 x 4 #&gt; target elapsed user system #&gt; &lt;chr&gt; &lt;Duration&gt; &lt;Duration&gt; &lt;Duration&gt; #&gt; 1 coef_regression1_large 0.011s 0.012s 0s #&gt; 2 coef_regression1_small 0.004s 0.003s 0s #&gt; 3 coef_regression2_large 0.004s 0.004s 0s #&gt; 4 coef_regression2_small 0.003s 0s 0.003s #&gt; 5 large 0.005s 0.004s 0s #&gt; 6 regression1_large 0.005s 0.004s 0s #&gt; 7 regression1_small 0.006s 0.006s 0s #&gt; 8 regression2_large 0.004s 0.001s 0.004s #&gt; 9 regression2_small 0.008s 0.007s 0s #&gt; 10 report 0.042s 0.042s 0s #&gt; 11 small 0.009s 0.009s 0s #&gt; 12 summ_regression1_large 0.004s 0.004s 0s #&gt; 13 summ_regression1_small 0.003s 0.004s 0s #&gt; 14 summ_regression2_large 0.004s 0.004s 0s #&gt; 15 summ_regression2_small 0.004s 0.003s 0s ## `dplyr`-style `tidyselect` commands build_times(starts_with(&quot;coef&quot;), digits = 8) #&gt; # A tibble: 4 x 4 #&gt; target elapsed user system #&gt; &lt;chr&gt; &lt;Duration&gt; &lt;Duration&gt; &lt;Duration&gt; #&gt; 1 coef_regression1_large 0.011s 0.012s 0s #&gt; 2 coef_regression1_small 0.004s 0.003s 0s #&gt; 3 coef_regression2_large 0.004s 0.004s 0s #&gt; 4 coef_regression2_small 0.003s 0s 0.003s 13.3 Predict total runtime drake uses these times to predict the runtime of the next make(). At this moment, everything is up to date in the current example, so the next make() should ideally take no time at all (except for preprocessing overhead). predict_runtime(my_plan) #&gt; [1] &quot;0s&quot; Suppose we change a dependency to make some targets out of date. Now, the next make() should take longer since some targets are out of date. reg2 &lt;- function(d){ d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) } predict_runtime(my_plan) #&gt; [1] &quot;0.069s&quot; And what if you plan to delete the cache and build all the targets from scratch? predict_runtime(my_plan, from_scratch = TRUE) #&gt; [1] &quot;0.116s&quot; 13.4 Strategize your high-performance computing Let’s say you are scaling up your workflow. You just put bigger data and heavier computation in your custom code, and the next time you run make(), your targets will take much longer to build. In fact, you estimate that every target except for your R Markdown report will take two hours to complete. Let’s write down these known times in seconds. known_times &lt;- rep(7200, nrow(my_plan)) names(known_times) &lt;- my_plan$target known_times[&quot;report&quot;] &lt;- 5 known_times #&gt; report small large #&gt; 5 7200 7200 #&gt; regression1_small regression1_large regression2_small #&gt; 7200 7200 7200 #&gt; regression2_large summ_regression1_small summ_regression1_large #&gt; 7200 7200 7200 #&gt; summ_regression2_small summ_regression2_large coef_regression1_small #&gt; 7200 7200 7200 #&gt; coef_regression1_large coef_regression2_small coef_regression2_large #&gt; 7200 7200 7200 How many parallel jobs should you use in the next make()? The predict_runtime() function can help you decide. predict_runtime(jobs = n) simulates persistent parallel workers and reports the estimated total runtime of make(jobs = n). (See also predict_workers().) time &lt;- c() for (jobs in 1:12){ time[jobs] &lt;- predict_runtime( my_plan, jobs = jobs, from_scratch = TRUE, known_times = known_times ) } #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 library(ggplot2) ggplot(data.frame(time = time / 3600, jobs = ordered(1:12), group = 1)) + geom_line(aes(x = jobs, y = time, group = group)) + scale_y_continuous(breaks = 0:10 * 4, limits = c(0, 29)) + theme_gray(16) + xlab(&quot;jobs argument of make()&quot;) + ylab(&quot;Predicted runtime of make() (hours)&quot;) We see serious potential speed gains up to 4 jobs, but beyond that point, we have to double the jobs to shave off another 2 hours. Your choice of jobs for make() ultimately depends on the runtime you can tolerate and the computing resources at your disposal. A final note on predicting runtime: the output of predict_runtime() and predict_workers() also depends the optional workers column of your drake_plan(). If you micromanage which workers are allowed to build which targets, you may minimize reads from disk, but you could also slow down your workflow if you are not careful. See the high-performance computing guide for more. "],
["memory.html", "Chapter 14 Memory management 14.1 Garbage collection and custom files 14.2 Memory strategies 14.3 Data splitting", " Chapter 14 Memory management The default settings of drake prioritize speed over memory efficiency. For projects with large data, this default behavior can cause problems. Consider the following hypothetical workflow, where we simulate several large datasets and summarize them. reps &lt;- 10 # Serious workflows may have several times more. # Reduce `n` to lighten the load if you want to try this workflow yourself. # It is super high in this chapter to motivate the memory issues. generate_large_data &lt;- function(rep, n = 1e8) { tibble(x = rnorm(n), y = rnorm(n), rep = rep) } get_means &lt;- function(...) { out &lt;- NULL for (dataset in list(...)) { out &lt;- bind_rows(out, colMeans(dataset)) } out } plan &lt;- drake_plan( large_data = target( generate_large_data(rep), transform = map(rep = !!seq_len(reps), .id = FALSE) ), means = target( get_means(large_data), transform = combine(large_data) ), summ = summary(means) ) print(plan) #&gt; # A tibble: 12 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 large_data generate_large_data(1L) … #&gt; 2 large_data_2 generate_large_data(2L) … #&gt; 3 large_data_3 generate_large_data(3L) … #&gt; 4 large_data_4 generate_large_data(4L) … #&gt; 5 large_data_5 generate_large_data(5L) … #&gt; 6 large_data_6 generate_large_data(6L) … #&gt; 7 large_data_7 generate_large_data(7L) … #&gt; 8 large_data_8 generate_large_data(8L) … #&gt; 9 large_data_9 generate_large_data(9L) … #&gt; 10 large_data_… generate_large_data(10L) … #&gt; 11 means get_means(large_data, large_data_2, large_data_3, large_data_4,… #&gt; 12 summ summary(means) … vis_drake_graph(plan) If you call make(plan) with no additional arguments, drake will try to load all the datasets into the same R session. Each dataset from generate_large_data(n = 1e8) occupies about 2.4 GB of memory, and most machines cannot handle all the data at once. We should use memory more wisely. 14.1 Garbage collection and custom files make() has a garbage_collection argument, which tells drake to periodically unload data objects that no longer belong to variables. You can also run garbage collection manually with the gc() function. For more on garbage collection, please refer to the memory usage chapter of Advanced R. Let’s reduce the memory consumption of our example workflow: Call gc() after every loop iteration of get_means(). Avoid drake’s caching system with custom file_out() files in the plan. Call make(plan, garbage_collection = TRUE). reps &lt;- 10 # Serious workflows may have several times more. files &lt;- paste0(seq_len(reps), &quot;.rds&quot;) generate_large_data &lt;- function(file, n = 1e8) { out &lt;- tibble(x = rnorm(n), y = rnorm(n)) # a billion rows saveRDS(out, file) } get_means &lt;- function(files) { out &lt;- NULL for (file in files) { x &lt;- colMeans(readRDS(file)) out &lt;- bind_rows(out, x) gc() # Use the gc() function here to make sure each x gets unloaded. } out } plan &lt;- drake_plan( large_data = target( generate_large_data(file = file_out(file)), transform = map(file = !!files, .id = FALSE) ), means = get_means(file_in(!!files)), summ = summary(means) ) print(plan) #&gt; # A tibble: 12 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 large_data generate_large_data(file = file_out(&quot;1.rds&quot;)) … #&gt; 2 large_data_2 generate_large_data(file = file_out(&quot;2.rds&quot;)) … #&gt; 3 large_data_3 generate_large_data(file = file_out(&quot;3.rds&quot;)) … #&gt; 4 large_data_4 generate_large_data(file = file_out(&quot;4.rds&quot;)) … #&gt; 5 large_data_5 generate_large_data(file = file_out(&quot;5.rds&quot;)) … #&gt; 6 large_data_6 generate_large_data(file = file_out(&quot;6.rds&quot;)) … #&gt; 7 large_data_7 generate_large_data(file = file_out(&quot;7.rds&quot;)) … #&gt; 8 large_data_8 generate_large_data(file = file_out(&quot;8.rds&quot;)) … #&gt; 9 large_data_9 generate_large_data(file = file_out(&quot;9.rds&quot;)) … #&gt; 10 large_data_… generate_large_data(file = file_out(&quot;10.rds&quot;)) … #&gt; 11 means get_means(file_in(c(&quot;1.rds&quot;, &quot;2.rds&quot;, &quot;3.rds&quot;, &quot;4.rds&quot;, &quot;5.rds&quot;… #&gt; 12 summ summary(means) … vis_drake_graph(plan) make(plan, garbage_collection = TRUE) 14.2 Memory strategies make() has a memory_strategy argument to customize how drake loads and unloads targets. With the right memory strategy, you can rely on drake’s built-in caching system without having to bother with messy file_out() files. Each memory strategy follows three stages for each target: Initial discard: before building the target, optionally discard some other targets from the R session. The choice of discards depends on the memory strategy. (Note: we do not actually get the memory back until we call gc().) Initial load: before building the target, optionally load any dependencies that are not already in memory. Final discard: optionally discard or keep the return value after the target finishes building. Either way, the return value is still stored in the cache, so you can load it with loadd() and readd(). The implementation of these steps varies from strategy to strategy. Memory strategy Initial discard Initial load Final discard “speed” Discard nothing Load any missing dependencies. Keep the return value loaded. “autoclean”1 Discard all targets which are not dependencies of the current target. Load any missing dependencies. Discard the return value. “preclean” Discard all targets which are not dependencies of the current target. Load any missing dependencies. Keep the return value loaded. “lookahead” Discard all targets which are not dependencies of either (1) the current target or (2) other targets waiting to be checked or built. Load any missing dependencies. Keep the return value loaded. “unload”2 Unload all targets. Load nothing. Discard the return value. “none”3 Unload nothing. Load nothing. Discard the return value. With the &quot;speed&quot;, &quot;autoclean&quot;, &quot;preclean&quot;, and &quot;lookahead&quot; strategies, you can simply call make(plan, memory_strategy = YOUR_CHOICE, garbage_collection = TRUE) and trust that your targets will build normally. For the &quot;unload&quot; and &quot;none&quot; strategies, there is extra work to do: you will need to manually load each target’s dependencies with loadd() or readd(). This manual bookkeeping lets you aggressively optimize your workflow, and it is less cumbersome than swarms of file_out() files. It is particularly useful when you have a large combine() step. Let’s redesign the workflow to reap the benefits of make(plan, memory_strategy = &quot;none&quot;, garbage_collection = TRUE). The trick is to use match.call() inside get_means() so we can load and unload dependencies one at a time instead of all at once. reps &lt;- 10 # Serious workflows may have several times more. generate_large_data &lt;- function(rep, n = 1e8) { tibble(x = rnorm(n), y = rnorm(n), rep = rep) } # Load targets one at a time get_means &lt;- function(...) { arg_symbols &lt;- match.call(expand.dots = FALSE)$... arg_names &lt;- as.character(arg_symbols) out &lt;- NULL for (arg_name in arg_names) { dataset &lt;- readd(arg_name, character_only = TRUE) out &lt;- bind_rows(out, colMeans(dataset)) gc() # Run garbage collection. } out } plan &lt;- drake_plan( large_data = target( generate_large_data(rep), transform = map(rep = !!seq_len(reps), .id = FALSE) ), means = target( get_means(large_data), transform = combine(large_data) ), summ = { loadd(means) # Annoying, but necessary with the &quot;none&quot; strategy. summary(means) } ) Now, we can build our targets. make(plan, memory_strategy = &quot;none&quot;, garbage_collection = TRUE) But there is a snag: we needed to manually load means in the command for summ (notice the call to loadd()). This is annoying, especially because means is quite small. Fortunately, drake lets you define different memory strategies for different targets in the plan. The target-specific memory strategies override the global one (i.e. the memory_strategy argument of make()). plan &lt;- drake_plan( large_data = target( generate_large_data(rep), transform = map(rep = !!seq_len(reps), .id = FALSE), memory_strategy = &quot;none&quot; ), means = target( get_means(large_data), transform = combine(large_data), memory_strategy = &quot;unload&quot; # Be careful with this one. ), summ = summary(means) ) print(plan) #&gt; # A tibble: 12 x 3 #&gt; target command memory_strategy #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; #&gt; 1 large_data generate_large_data(1L) … none #&gt; 2 large_data… generate_large_data(2L) … none #&gt; 3 large_data… generate_large_data(3L) … none #&gt; 4 large_data… generate_large_data(4L) … none #&gt; 5 large_data… generate_large_data(5L) … none #&gt; 6 large_data… generate_large_data(6L) … none #&gt; 7 large_data… generate_large_data(7L) … none #&gt; 8 large_data… generate_large_data(8L) … none #&gt; 9 large_data… generate_large_data(9L) … none #&gt; 10 large_data… generate_large_data(10L) … none #&gt; 11 means get_means(large_data, large_data_2, large_data_3… unload #&gt; 12 summ summary(means) … &lt;NA&gt; In fact, now you can run make() without setting a global memory strategy at all. make(plan, garbage_collection = TRUE) 14.3 Data splitting The split() transformation breaks up a dataset into smaller targets. The ordinary use of split() is to partition an in-memory dataset into slices. drake_plan( data = get_large_data(), x = target( data %&gt;% analyze_data(), transform = split(data, slices = 4) ) ) #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data get_large_data() #&gt; 2 x_1 drake_slice(data = data, slices = 4, index = 1) %&gt;% analyze_data() #&gt; 3 x_2 drake_slice(data = data, slices = 4, index = 2) %&gt;% analyze_data() #&gt; 4 x_3 drake_slice(data = data, slices = 4, index = 3) %&gt;% analyze_data() #&gt; 5 x_4 drake_slice(data = data, slices = 4, index = 4) %&gt;% analyze_data() However, you can also use it to load individual pieces of a large file, thus conserving memory. The trick is to break up an index set instead of the data itself. In the following sketch, get_number_of_rows() and read_selected_rows() are user-defined functions, and %&gt;% is the magrittr pipe. get_number_of_rows &lt;- function(file) { # ... } read_selected_rows &lt;- function(which_rows, file) { # ... } plan &lt;- drake_plan( row_indices = file_in(&quot;large_file.csv&quot;) %&gt;% get_number_of_rows() %&gt;% seq_len(), subset = target( row_indices %&gt;% read_selected_rows(file = file_in(&quot;large_file.csv&quot;)), transform = split(row_indices, slices = 4) ) ) plan #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 row_indices file_in(&quot;large_file.csv&quot;) %&gt;% get_number_of_rows() %&gt;% seq_len() … #&gt; 2 subset_1 drake_slice(data = row_indices, slices = 4, index = 1) %&gt;% read_s… #&gt; 3 subset_2 drake_slice(data = row_indices, slices = 4, index = 2) %&gt;% read_s… #&gt; 4 subset_3 drake_slice(data = row_indices, slices = 4, index = 3) %&gt;% read_s… #&gt; 5 subset_4 drake_slice(data = row_indices, slices = 4, index = 4) %&gt;% read_s… drake_plan_source(plan) #&gt; drake_plan( #&gt; row_indices = file_in(&quot;large_file.csv&quot;) %&gt;% #&gt; get_number_of_rows() %&gt;% #&gt; seq_len(), #&gt; subset_1 = drake_slice(data = row_indices, slices = 4, index = 1) %&gt;% read_selected_rows(file = file_in(&quot;large_file.csv&quot;)), #&gt; subset_2 = drake_slice(data = row_indices, slices = 4, index = 2) %&gt;% read_selected_rows(file = file_in(&quot;large_file.csv&quot;)), #&gt; subset_3 = drake_slice(data = row_indices, slices = 4, index = 3) %&gt;% read_selected_rows(file = file_in(&quot;large_file.csv&quot;)), #&gt; subset_4 = drake_slice(data = row_indices, slices = 4, index = 4) %&gt;% read_selected_rows(file = file_in(&quot;large_file.csv&quot;)) #&gt; ) Only supported in drake version 7.5.0 and above.↩ Only supported in drake version 7.4.0 and above.↩ Only supported in drake version 7.4.0 and above.↩ "],
["storage.html", "Chapter 15 Storage 15.1 drake’s cache 15.2 Efficient target storage 15.3 Why is my cache so big? 15.4 Interfaces to the cache", " Chapter 15 Storage 15.1 drake’s cache When you run make(), drake stores your targets in a hidden storage cache. library(drake) load_mtcars_example() # from https://github.com/wlandau/drake-examples/tree/master/mtcars make(my_plan, verbose = 0L) The default cache is a hidden .drake folder. find_cache() ### [1] &quot;/home/you/project/.drake&quot; drake’s loadd() and readd() functions load targets into memory. loadd(large) head(large) #&gt; x y #&gt; 1 2.140 26.0 #&gt; 2 3.730 17.3 #&gt; 3 1.935 27.3 #&gt; 4 2.780 21.4 #&gt; 5 2.320 22.8 #&gt; 6 2.780 21.4 head(readd(small)) #&gt; x y #&gt; 1 3.780 15.2 #&gt; 2 3.440 17.8 #&gt; 3 3.215 21.4 #&gt; 4 3.440 19.2 #&gt; 5 2.770 19.7 #&gt; 6 2.465 21.5 15.2 Efficient target storage drake supports custom formats for large and specialized targets. For example, the &quot;fst&quot; format uses the fst package to save data frames faster. Simply enclose the command and the format together with the target() function. library(drake) n &lt;- 1e8 # Each target is 1.6 GB in memory. plan &lt;- drake_plan( data_fst = target( data.frame(x = runif(n), y = runif(n)), format = &quot;fst&quot; ), data_old = data.frame(x = runif(n), y = runif(n)) ) make(plan) #&gt; target data_fst #&gt; target data_old build_times(type = &quot;build&quot;) #&gt; # A tibble: 2 x 4 #&gt; target elapsed user system #&gt; &lt;chr&gt; &lt;Duration&gt; &lt;Duration&gt; &lt;Duration&gt; #&gt; 1 data_fst 13.93s 37.562s 7.954s #&gt; 2 data_old 184s (~3.07 minutes) 177s (~2.95 minutes) 4.157s For more details and a complete list of formats, see https://books.ropensci.org/drake/plans.html#special-data-formats-for-targets. 15.3 Why is my cache so big? 15.3.1 Old targets By default, drake holds on to all your targets from all your runs of make(). Even if you run clean(), the data stays in the cache in case you need to recover it. clean() make(my_plan, recover = TRUE) #&gt; unload targets from environment: #&gt; large #&gt; recover large #&gt; recover small #&gt; recover regression1_large #&gt; recover regression2_large #&gt; recover regression1_small #&gt; recover regression2_small #&gt; recover coef_regression1_large #&gt; recover summ_regression1_large #&gt; recover summ_regression2_large #&gt; recover coef_regression2_large #&gt; recover summ_regression1_small #&gt; recover coef_regression1_small #&gt; recover coef_regression2_small #&gt; recover summ_regression2_small #&gt; recover report If you really want to remove old historical values of targets, run drake_gc() or drake_cache()$gc(). drake_gc() clean() also has a garbage_collection argument for this purpose. Here is a slick way to remove historical targets and targets no longer in your plan. clean(list = cached_unplanned(my_plan), garbage_collection = TRUE) 15.3.2 Garbage from interrupted builds If make() crashes or gets interrupted, old files can accumulate in .drake/scratch/ and .drake/drake/tmp/. As long as make() is no longer running, can safely remove the files in those folders (but keep the folders themselves). 15.4 Interfaces to the cache drake uses the storr package to create and modify caches. library(storr) cache &lt;- storr_rds(&quot;.drake&quot;) head(cache$list()) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; &quot;coef_regression2_large&quot; #&gt; [4] &quot;coef_regression2_small&quot; &quot;large&quot; &quot;regression1_large&quot; head(cache$get(&quot;small&quot;)) #&gt; x y #&gt; 1 3.780 15.2 #&gt; 2 3.440 17.8 #&gt; 3 3.215 21.4 #&gt; 4 3.440 19.2 #&gt; 5 2.770 19.7 #&gt; 6 2.465 21.5 drake has its own interface on top of storr to make it easier to work with the default .drake/ cache. The loadd(), readd(), and cached() functions explore saved targets. head(cached()) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; &quot;coef_regression2_large&quot; #&gt; [4] &quot;coef_regression2_small&quot; &quot;large&quot; &quot;regression1_large&quot; head(readd(small)) #&gt; x y #&gt; 1 3.780 15.2 #&gt; 2 3.440 17.8 #&gt; 3 3.215 21.4 #&gt; 4 3.440 19.2 #&gt; 5 2.770 19.7 #&gt; 6 2.465 21.5 loadd(large) head(large) #&gt; x y #&gt; 1 2.140 26.0 #&gt; 2 3.730 17.3 #&gt; 3 1.935 27.3 #&gt; 4 2.780 21.4 #&gt; 5 2.320 22.8 #&gt; 6 2.780 21.4 rm(large) # Does not remove `large` from the cache. new_cache() create caches and drake_cache() recovers existing ones. (drake_cache() is only supported in drake version 7.4.0 and above.) cache &lt;- drake_cache() cache$driver$path #&gt; [1] &quot;/tmp/Rtmpt21Xyj/file3885227a9609/.drake&quot; cache &lt;- drake_cache(path = &quot;.drake&quot;) # File path to drake&#39;s cache. cache$driver$path #&gt; [1] &quot;/tmp/Rtmpt21Xyj/file3885227a9609/.drake&quot; You can supply your own cache to make() and friends (including specialized storr caches like storr_dbi()). plan &lt;- drake_plan(x = 1, y = sqrt(x)) make(plan, cache = cache) #&gt; target x #&gt; target y vis_drake_graph(plan, cache = cache) Destroy caches to remove them from your file system. cache$destroy() file.exists(&quot;.drake&quot;) #&gt; [1] FALSE "],
["visuals.html", "Chapter 16 Visualization with drake 16.1 Plotting plans 16.2 Underlying graph data: node and edge data frames 16.3 Visualizing target status 16.4 Subgraphs 16.5 Control the vis_drake_graph() legend. 16.6 Clusters 16.7 Output files 16.8 Node Selection", " Chapter 16 Visualization with drake Data analysis projects have complicated networks of dependencies, and drake can help you visualize them with vis_drake_graph(), sankey_drake_graph(), and drake_ggraph() (note the two g’s). 16.1 Plotting plans Except for drake 7.7.0 and below, you can simply plot() the plan to show the targets and their dependency relationships. library(drake) # from https://github.com/wlandau/drake-examples/tree/master/mtcars load_mtcars_example() my_plan #&gt; # A tibble: 15 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 report knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), qui… #&gt; 2 small simulate(48) … #&gt; 3 large simulate(64) … #&gt; 4 regression1_small reg1(small) … #&gt; 5 regression1_large reg1(large) … #&gt; 6 regression2_small reg2(small) … #&gt; 7 regression2_large reg2(large) … #&gt; 8 summ_regression1_sm… suppressWarnings(summary(regression1_small$residuals)) … #&gt; 9 summ_regression1_la… suppressWarnings(summary(regression1_large$residuals)) … #&gt; 10 summ_regression2_sm… suppressWarnings(summary(regression2_small$residuals)) … #&gt; 11 summ_regression2_la… suppressWarnings(summary(regression2_large$residuals)) … #&gt; 12 coef_regression1_sm… suppressWarnings(summary(regression1_small))$coefficien… #&gt; 13 coef_regression1_la… suppressWarnings(summary(regression1_large))$coefficien… #&gt; 14 coef_regression2_sm… suppressWarnings(summary(regression2_small))$coefficien… #&gt; 15 coef_regression2_la… suppressWarnings(summary(regression2_large))$coefficien… plot(my_plan) 16.1.1 vis_drake_graph() Powered by visNetwork. Colors represent target status, and shapes represent data type. These graphs are interactive, so you can click, drag, zoom, and and pan to adjust the size and position. Double-click on nodes to contract neighborhoods into clusters or expand them back out again. If you hover over a node, you will see text in a tooltip showing the first few lines of The command of a target, or The body of an imported function, or The content of an imported text file. vis_drake_graph(my_plan) To save this interactive widget for later, just supply the name of an HTML file. vis_drake_graph(my_plan, file = &quot;graph.html&quot;) To save a static image file, supply a file name that ends in &quot;.png&quot;, &quot;.pdf&quot;, &quot;.jpeg&quot;, or &quot;.jpg&quot;. vis_drake_graph(my_plan, file = &quot;graph.png&quot;) 16.1.2 sankey_drake_graph() These interactive networkD3 Sankey diagrams have more nuance: the height of each node is proportional to its number of connections. Nodes with many incoming connnections tend to fall out of date more often, and nodes with many outgoing connections can invalidate bigger chunks of the downstream pipeline. sankey_drake_graph(my_plan) Saving the graphs is the same as before. sankey_drake_graph(my_plan, file = &quot;graph.html&quot;) # Interactive HTML widget sankey_drake_graph(my_plan, file = &quot;graph.png&quot;) # Static image file Unfortunately, a legend is not yet available for Sankey diagrams, but drake exposes a separate legend for the colors and shapes. library(visNetwork) legend_nodes() #&gt; # A tibble: 12 x 6 #&gt; label color shape font.color font.size id #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 Up to date #228B22 dot black 20 1 #&gt; 2 Outdated #000000 dot black 20 2 #&gt; 3 Running #FF7221 dot black 20 3 #&gt; 4 Cancelled #ECB753 dot black 20 4 #&gt; 5 Failed #AA0000 dot black 20 5 #&gt; 6 Imported #1874CD dot black 20 6 #&gt; 7 Missing #9A32CD dot black 20 7 #&gt; 8 Object #888888 dot black 20 8 #&gt; 9 Dynamic #888888 star black 20 9 #&gt; 10 Function #888888 triangle black 20 10 #&gt; 11 File #888888 square black 20 11 #&gt; 12 Cluster #888888 diamond black 20 12 visNetwork(nodes = legend_nodes()) 16.1.3 drake_ggraph() drake_ggraph() can handle larger workflows than the other graphing functions. If your project has thousands of targets and vis_drake_graph()/sankey_drake_graph() does not render properly, consider drake_ggraph(). Powered by ggraph, drake_ggraph()s are static ggplot2 objects, and you can save them with ggsave(). drake_ggraph(my_plan) 16.1.4 text_drake_graph() If you are running R in a terminal without X Window support, the usual visualizations will show up interactively in your session. Here, you can use text_drake_graph() to see a text display in your terminal window. Terminal colors are deactivated in this manual, but you will see color in your console. # Use nchar = 0 or nchar = 1 for better results. # The color display is better in your own terminal. text_drake_graph(my_plan, nchar = 3) #&gt; sum #&gt; reg reg coe #&gt; #&gt; dat sma coe #&gt; reg #&gt; sum #&gt; reg sim rep fil #&gt; coe #&gt; reg #&gt; ran lar sum #&gt; #&gt; fil reg sum #&gt; coe 16.2 Underlying graph data: node and edge data frames drake_graph_info() is used behind the scenes in vis_drake_graph(), sankey_drake_graph(), and drake_ggraph() to get the graph information ready for rendering. To save time, you can call drake_graph_info() to get these internals and then call render_drake_graph(), render_sankey_drake_graph(), or render_drake_ggraph(). str(drake_graph_info(my_plan)) #&gt; List of 4 #&gt; $ nodes :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 22 obs. of 12 variables: #&gt; ..$ id : chr [1:22] &quot;p-OJSXA33SOQXFE3LE&quot; &quot;random_rows&quot; &quot;reg1&quot; &quot;n-MRQXIYLTMV2HGOR2NV2GGYLSOM&quot; ... #&gt; ..$ imported : logi [1:22] TRUE TRUE TRUE TRUE TRUE TRUE ... #&gt; ..$ label : chr [1:22] &quot;file report.Rmd&quot; &quot;random_rows&quot; &quot;reg1&quot; &quot;datasets::mtcars&quot; ... #&gt; ..$ status : chr [1:22] &quot;imported&quot; &quot;imported&quot; &quot;imported&quot; &quot;imported&quot; ... #&gt; ..$ type : chr [1:22] &quot;file&quot; &quot;function&quot; &quot;function&quot; &quot;object&quot; ... #&gt; ..$ font.size: num [1:22] 20 20 20 20 20 20 20 20 20 20 ... #&gt; ..$ color : chr [1:22] &quot;#1874CD&quot; &quot;#1874CD&quot; &quot;#1874CD&quot; &quot;#1874CD&quot; ... #&gt; ..$ shape : chr [1:22] &quot;square&quot; &quot;triangle&quot; &quot;triangle&quot; &quot;dot&quot; ... #&gt; ..$ level : num [1:22] 1 1 1 1 1 2 3 3 4 4 ... #&gt; ..$ title : chr [1:22] &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; ... #&gt; ..$ x : num [1:22] -1 -1 -1 -1 -1 ... #&gt; ..$ y : num [1:22] -8.57e-01 -4.29e-01 2.22e-16 4.29e-01 8.57e-01 ... #&gt; $ edges :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 25 obs. of 3 variables: #&gt; ..$ from : chr [1:25] &quot;simulate&quot; &quot;simulate&quot; &quot;large&quot; &quot;large&quot; ... #&gt; ..$ to : chr [1:25] &quot;large&quot; &quot;small&quot; &quot;report&quot; &quot;regression1_large&quot; ... #&gt; ..$ arrows: chr [1:25] &quot;to&quot; &quot;to&quot; &quot;to&quot; &quot;to&quot; ... #&gt; $ legend_nodes :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 5 obs. of 6 variables: #&gt; ..$ label : chr [1:5] &quot;Outdated&quot; &quot;Imported&quot; &quot;Object&quot; &quot;Function&quot; ... #&gt; ..$ color : chr [1:5] &quot;#000000&quot; &quot;#1874CD&quot; &quot;#888888&quot; &quot;#888888&quot; ... #&gt; ..$ shape : chr [1:5] &quot;dot&quot; &quot;dot&quot; &quot;dot&quot; &quot;triangle&quot; ... #&gt; ..$ font.color: chr [1:5] &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; ... #&gt; ..$ font.size : num [1:5] 20 20 20 20 20 #&gt; ..$ id : int [1:5] 2 6 8 10 11 #&gt; $ default_title: chr &quot;Dependency graph&quot; #&gt; - attr(*, &quot;class&quot;)= chr &quot;drake_graph_info&quot; 16.3 Visualizing target status drake’s visuals tell you which targets are up to date and which are outdated. make(my_plan, verbose = 0L) outdated(my_plan) #&gt; character(0) sankey_drake_graph(my_plan) When you change a dependency, some targets fall out of date (black nodes). reg2 &lt;- function(d){ d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) } sankey_drake_graph(my_plan) 16.4 Subgraphs Graphs can grow enormous for serious projects, so there are multiple ways to focus on a manageable subgraph. The most brute-force way is to just pick a manual subset of nodes. However, with the subset argument, the graphing functions can drop intermediate nodes and edges. vis_drake_graph( my_plan, subset = c(&quot;regression2_small&quot;, &quot;large&quot;) ) The rest of the subgraph functionality preserves connectedness. Use targets_only to ignore the imports. vis_drake_graph(my_plan, targets_only = TRUE) Similarly, you can just show downstream nodes. vis_drake_graph(my_plan, from = c(&quot;regression2_small&quot;, &quot;regression2_large&quot;)) Or upstream ones. vis_drake_graph(my_plan, from = &quot;small&quot;, mode = &quot;in&quot;) In fact, let us just take a small neighborhood around a target in both directions. For the graph below, given order is 1, but all the custom file_out() output files of the neighborhood’s targets appear as well. This ensures consistent behavior between show_output_files = TRUE and show_output_files = FALSE (more on that later). vis_drake_graph(my_plan, from = &quot;small&quot;, mode = &quot;all&quot;, order = 1) 16.5 Control the vis_drake_graph() legend. Some arguments to vis_drake_graph() control the legend. vis_drake_graph(my_plan, full_legend = TRUE, ncol_legend = 2) To remove the legend altogether, set the ncol_legend argument to 0. vis_drake_graph(my_plan, ncol_legend = 0) 16.6 Clusters With the group and clusters arguments to the graphing functions, you can condense nodes into clusters. This is handy for workflows with lots of targets. Take the schools scenario from the drake plan guide. Our plan was generated with drake_plan(trace = TRUE), so it has wildcard columns that group nodes into natural clusters already. You can manually add such columns if you wish. # Visit https://books.ropensci.org/drake/static.html # to learn about the syntax with target(transform = ...). plan &lt;- drake_plan( school = target( get_school_data(id), transform = map(id = c(1, 2, 3)) ), credits = target( fun(school), transform = cross( school, fun = c(check_credit_hours, check_students, check_graduations) ) ), public_funds_school = target( command = check_public_funding(school), transform = map(school = c(school_1, school_2)) ), trace = TRUE ) plan #&gt; # A tibble: 14 x 7 #&gt; target command id school public_funds_scho… fun credits #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 school_1 get_school_… 1 schoo… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 school_2 get_school_… 2 schoo… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 school_3 get_school_… 3 schoo… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 4 credits_che… check_credi… &lt;NA&gt; schoo… &lt;NA&gt; check_… credits_ch… #&gt; 5 credits_che… check_stude… &lt;NA&gt; schoo… &lt;NA&gt; check_… credits_ch… #&gt; 6 credits_che… check_gradu… &lt;NA&gt; schoo… &lt;NA&gt; check_… credits_ch… #&gt; 7 credits_che… check_credi… &lt;NA&gt; schoo… &lt;NA&gt; check_… credits_ch… #&gt; 8 credits_che… check_stude… &lt;NA&gt; schoo… &lt;NA&gt; check_… credits_ch… #&gt; 9 credits_che… check_gradu… &lt;NA&gt; schoo… &lt;NA&gt; check_… credits_ch… #&gt; 10 credits_che… check_credi… &lt;NA&gt; schoo… &lt;NA&gt; check_… credits_ch… #&gt; 11 credits_che… check_stude… &lt;NA&gt; schoo… &lt;NA&gt; check_… credits_ch… #&gt; 12 credits_che… check_gradu… &lt;NA&gt; schoo… &lt;NA&gt; check_… credits_ch… #&gt; 13 public_fund… check_publi… &lt;NA&gt; schoo… public_funds_scho… &lt;NA&gt; &lt;NA&gt; #&gt; 14 public_fund… check_publi… &lt;NA&gt; schoo… public_funds_scho… &lt;NA&gt; &lt;NA&gt; Ordinarily, the workflow graph gives a separate node to each individual import object or target. vis_drake_graph(plan) For large projects with hundreds of nodes, this can get quite cumbersome. But here, we can choose a wildcard column (or any other column in the plan, even custom columns) to condense nodes into natural clusters. For the group argument to the graphing functions, choose the name of a column in plan or a column you know will be in drake_graph_info(my_plan)$nodes. Then for clusters, choose the values in your group column that correspond to nodes you want to bunch together. The new graph is not as cumbersome. vis_drake_graph(plan, group = &quot;school&quot;, clusters = c(&quot;school_1&quot;, &quot;school_2&quot;, &quot;school_3&quot;) ) As previously mentioned, you can group on any column in drake_graph_info(my_plan)$nodes. Let’s return to the mtcars project for demonstration. vis_drake_graph(my_plan) Let’s condense all the imports into one node and all the up-to-date targets into another. That way, the outdated targets stand out. vis_drake_graph( my_plan, group = &quot;status&quot;, clusters = c(&quot;imported&quot;, &quot;up to date&quot;) ) 16.7 Output files drake can reproducibly track multiple output files per target and show them in the graph. plan &lt;- drake_plan( target1 = { file.copy(file_in(&quot;in1.txt&quot;), file_out(&quot;out1.txt&quot;)) file.copy(file_in(&quot;in2.txt&quot;), file_out(&quot;out2.txt&quot;)) }, target2 = { file.copy(file_in(&quot;out1.txt&quot;), file_out(&quot;out3.txt&quot;)) file.copy(file_in(&quot;out2.txt&quot;), file_out(&quot;out4.txt&quot;)) } ) writeLines(&quot;in1&quot;, &quot;in1.txt&quot;) writeLines(&quot;in2&quot;, &quot;in2.txt&quot;) make(plan) #&gt; target target1 #&gt; target target2 writeLines(&quot;abcdefg&quot;, &quot;out3.txt&quot;) vis_drake_graph(plan, targets_only = TRUE) If your graph is too busy, you can hide the output files with show_output_files = FALSE. vis_drake_graph(plan, show_output_files = FALSE, targets_only = TRUE) 16.8 Node Selection (Supported in drake &gt; 7.7.0 only) First, we define our plan, adding a custom column named “link”. mtcars_link &lt;- &quot;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html&quot; iris_link &lt;- &quot;https://archive.ics.uci.edu/ml/datasets/iris&quot; iris3_link &lt;- &quot;https://www.rdocumentation.org/packages/datasets/versions/3.6.1/topics/iris&quot; plan &lt;- drake_plan( mtc = target( mtcars, link = !!mtcars_link ), ir = target( iris, link = !!iris_link ), ir3 = target( modify_iris3(iris3, number), transform = map(number = !!c(1:3), .tag_in = cluster_id ), link = !!iris3_link ), trace = TRUE ) unique_stems &lt;- unique(plan$cluster_id) 16.8.1 Perform the default action on select By supplying vis_drake_graph(on_select = TRUE, on_select_col = &quot;my_column&quot;), treats the values in the column named &quot;my_column&quot; as hyperlinks. vis_drake_graph( my_plan, clusters = unique_stems, group = &quot;cluster_id&quot;, on_select_col = &quot;link&quot;, on_select = TRUE ) 16.8.2 Perform no action on select No action will be taken if any of the following are given to vis_drake_graph(): on_select = NULL, on_select = FALSE, on_select_col = NULL This is the default behaviour. vis_drake_graph( my_plan, clusters = unique_stems, group = &quot;cluster_id&quot;, on_select_col = &quot;link&quot;, on_select = NULL ) 16.8.3 Customize the onSelect event behaviour What if we instead wanted the browser to display an alert when a node is clicked? alert_behaviour &lt;- function(){ js &lt;- &quot; function(props) { alert(&#39;selected node with on_select_col: \\\\r\\\\n&#39; + this.body.data.nodes.get(props.nodes[0]).on_select_col); }&quot; } vis_drake_graph( my_plan, on_select_col = &quot;link&quot;, on_select = alert_behaviour() ) "],
["debugging.html", "Chapter 17 Debugging and testing drake projects 17.1 Debugging failed targets 17.2 Why do my targets keep rerunning? 17.3 More help", " Chapter 17 Debugging and testing drake projects This chapter aims to help users detect and diagnose problems with large complex workflows. 17.1 Debugging failed targets 17.1.1 Diagnosing errors When a target fails, drake tries to tell you. large_dataset &lt;- function() { data.frame(x = rnorm(1e6), y = rnorm(1e6)) } expensive_analysis &lt;- function(data) { # More operations go here. tricky_operation(data) } tricky_operation &lt;- function(data) { # Expensive code here. stop(&quot;there is a bug somewhere.&quot;) } plan &lt;- drake_plan( data = large_dataset(), analysis = expensive_analysis(data) ) make(plan) #&gt; target data #&gt; target analysis #&gt; fail analysis #&gt; Error: target `analysis` failed. Call `drake::diagnose(analysis)` for details. Error message: #&gt; there is a bug somewhere. diagnose() recovers the metadata on targets. For failed targets, this includes an error object. error &lt;- diagnose(analysis)$error error #&gt; &lt;simpleError in tricky_operation(data): there is a bug somewhere.&gt; names(error) #&gt; [1] &quot;message&quot; &quot;call&quot; &quot;calls&quot; Using the call stack, you can trace back the location of the error. Once you know roughly where to find the bug, you can troubleshoot interactively. invisible(lapply(tail(error$calls, 3), print)) #&gt; expensive_analysis(data) #&gt; tricky_operation(data) #&gt; stop(&quot;there is a bug somewhere.&quot;) 17.1.2 Interactive debugging The clues from diagnose() help us go back and inspect the failing code. debug() is an interactive debugging tool which helps you verify exactly what is going wrong. Below, make(plan) pauses execution and turn interactive control over to you inside tricky_operation(). debug(tricky_operation) make(plan) # Pauses at tricky_operation(data). undebug(tricky_operation) # Undoes debug(). drake’s own drake_debug() function is nearly equivalent. drake_debug(analysis, plan) # Pauses at the command expensive_analysis(data). browser() is similar, but it affords you finer control over to pause execution tricky_operation &lt;- function(data) { # Expensive code here. browser() # Pauses right here to give you control. stop(&quot;there is a bug somewhere.&quot;) } make(plan) 17.1.3 Efficient trial and error If you are using drake, then chances are your targets are computationally expensive and the long runtimes make debugging difficult. To speed up trial and error, run the plan on a small dataset when you debug and repair things. plan &lt;- drake_plan( data = head(large_dataset()), # Just work with the first few rows. analysis = expensive_analysis(data) # Runs faster now. ) tricky_operation &lt;- ... # Try to fix the function. debug(tricky_operation) # Set up to debug interactively. make(plan) # Try to run the workflow. After a lot of quick trial and error, we finally fix the function and run it on the small data. tricky_operation &lt;- function(data) { # Good code goes here. } make(plan) #&gt; target analysis Now, that the code works, it is time to scale back up to the large data. Use make(plan, recover = TRUE) to salvage old targets from before the debugging process. plan &lt;- drake_plan( data = large_dataset(), # Use the large data again. analysis = expensive_analysis(data) # Should be repaired now. ) make(plan, recover = TRUE) #&gt; All targets are already up to date. 17.2 Why do my targets keep rerunning? Consider the following completed workflow. load_mtcars_example() make(my_plan) #&gt; target small #&gt; target large #&gt; target regression1_small #&gt; target regression2_small #&gt; target regression1_large #&gt; target regression2_large #&gt; target summ_regression1_small #&gt; target coef_regression1_small #&gt; target coef_regression2_small #&gt; target summ_regression2_small #&gt; target summ_regression1_large #&gt; target coef_regression1_large #&gt; target coef_regression2_large #&gt; target summ_regression2_large #&gt; target report At this point, if you change the reg1() function, then make() will automatically detect and rerun downstream targets such as regression1_large. reg1 &lt;- function (d) { lm(y ~ 1 + x, data = d) } make(my_plan) #&gt; target regression1_small #&gt; target regression1_large #&gt; target summ_regression1_small #&gt; target coef_regression1_small #&gt; target summ_regression1_large #&gt; target coef_regression1_large In general, targets are “outdated” or “invalidated” they are out of sync with their dependencies. If a target is outdated, the next make() automatically detects discrepancies and rebuild the affected targets. Usually, this automation adds convenience, saves time, and ensures reproducibility in the face of long runtimes. However, it can be frustrating when drake detects outdated targets when you think everything is up to date. If this happens, it is important to understand How your workflow fits together. Which targets are outdated. Why your targets are outdated. Strategies to prevent unexpected changes in the future. drake’s utility functions offer clues to guide you. 17.2.1 How your workflow fits together drake automatically analyzes your plan and functions to understand how your targets depend on each other. It assembles this information in a directed acyclic graph (DAG) which you can visualize and explore. vis_drake_graph(my_plan) To get a more localized version of the graph, use deps_target(). Unlike vis_drake_graph(), deps_target() gives you a more granular view of the dependencies of an individual target. deps_target(regression1_large, my_plan) #&gt; # A tibble: 2 x 3 #&gt; name type hash #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 large globals d87293dd4d012762 #&gt; 2 reg1 globals 7d6ad46722ffc91b deps_target(report, my_plan) #&gt; # A tibble: 5 x 3 #&gt; name type hash #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 large loadd d87293dd4d012762 #&gt; 2 small readd b588209b7f4f5b0a #&gt; 3 coef_regression2_small readd b5502b9c62933484 #&gt; 4 report.md file_out &lt;NA&gt; #&gt; 5 report.Rmd knitr_in &lt;NA&gt; To understand how drake detects dependencies in the first place, use deps_code(). This is what drake first sees when it reads your plan and functions to understand the dependencies. deps_code(quote( suppressWarnings(summary(regression1_large$residuals)) )) #&gt; # A tibble: 3 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 regression1_large globals #&gt; 2 summary globals #&gt; 3 suppressWarnings globals deps_code(quote( knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) )) #&gt; # A tibble: 6 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 knit globals #&gt; 2 large loadd #&gt; 3 small readd #&gt; 4 coef_regression2_small readd #&gt; 5 report.md file_out #&gt; 6 report.Rmd knitr_in If drake detects new dependencies you were unaware of, that could be a reason why your targets are out of date. 17.2.2 Which targets are outdated Graphing utilities like vis_drake_graph() label the outdated targets, but sometimes it is helpful to get a more programmatic view. outdated(my_plan) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; &quot;regression1_large&quot; #&gt; [4] &quot;regression1_small&quot; &quot;summ_regression1_large&quot; &quot;summ_regression1_small&quot; 17.2.3 Why your targets are outdated The deps_profile() function offers clues. deps_profile(regression1_small, my_plan) #&gt; # A tibble: 5 x 4 #&gt; name changed old new #&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 command FALSE &quot;c4d2a1810ff71195&quot; &quot;c4d2a1810ff71195&quot; #&gt; 2 depend TRUE &quot;80aea64bc1193d2c&quot; &quot;6588e8c7a6761937&quot; #&gt; 3 file_in FALSE &quot;&quot; &quot;&quot; #&gt; 4 file_out FALSE &quot;&quot; &quot;&quot; #&gt; 5 seed FALSE &quot;1186782247&quot; &quot;1186782247&quot; From the data frame above, regression1_small is outdated because an R object dependency changed since the last make(). drake does not hold on to enough information to tell you precisely which object is the culprit, but functions like vis_drake_graph(), deps_target(), and deps_code() can help narrow down the possibilities. 17.2.4 Strategies to prevent unexpected changes in the future drake is sensitive to changing functions in your global environment, and this sensitivity can invalidate targets unexpectedly. Whenever you plan to run make(), it is always best to restart your R session and load your packages and functions into a fresh clean workspace. r_make() does all this cleaning and prep work for you automatically, and it is more robust and dependable (and childproofed) than ordinary r_make(). To read more, visit https://books.ropensci.org/drake/projects#safer-interactivity. 17.3 More help The GitHub issue tracker is the best place to request help with your specific use case. "],
["triggers.html", "Chapter 18 Triggers: decision rules for building targets 18.1 What are triggers? 18.2 Customization 18.3 Alternative trigger modes 18.4 A more practical example", " Chapter 18 Triggers: decision rules for building targets When you call make(), drake tries to skip as many targets as possible. If it thinks a command will return the same value as last time, it does not bother running it. In other words, drake is lazy, and laziness saves you time. 18.1 What are triggers? To figure out whether it can skip a target, drake goes through an intricate checklist of triggers: The missing trigger: Do we lack a return value from a previous make()? Maybe you are building the target for the first time or you removed it from the cache with clean(). The command trigger: did the command in the drake plan change nontrivially since the last make()? Changes to spacing, formatting, and comments are ignored. The depend trigger: did any non-file dependencies change since the last make()? These could be: Other targets. Imported objects. Imported functions (ignoring changes to spacing, formatting, and comments). Any dependencies of imported functions. Any dependencies of dependencies of imported functions, and so on. The file trigger: did any file inputs or file outputs change since the last make()? These files are the ones explicitly declared in the command with file_in(), knitr_in(), and file_out(). The seed trigger: did the target’s pseudo-random number generator seed change? Only applies if you supplied a seed column to the plan at some point. The format trigger: did you add or change the target’s storage format since last build? Details: https://books.ropensci.org/drake/plans.html#special-data-formats-for-targets. The condition trigger: an optional user-defined piece of code that evaluates to a TRUE/FALSE value. The target builds if the value is TRUE. The change trigger: an optional user-defined piece of code that evaluates to any value (preferably small and quick to compute). The target builds if the value changed since the last make(). If any trigger detects something wrong or different with the target or its dependencies, the next make() will run the command and (re)build the target. 18.2 Customization With the trigger() function, you can create your own customized checklist of triggers. Let’s run a simple workflow with just the missing trigger. We deactivate the command, depend, and file triggers by setting the respective command, depend, and file arguments to FALSE. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2, psi_2 = (sqrt(5) - 1) / 2 ) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; target psi_1 #&gt; target psi_2 Now, even if you wreck all the commands, nothing rebuilds. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2 + 9999999999999, psi_2 = (sqrt(5) - 1) / 2 - 9999999999999 ) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; All targets are already up to date. You can also give different targets to different triggers. Triggers in the drake plan override the trigger argument to make(). Below, psi_2 always builds, but psi_1 only builds if it has never been built before. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2 + 9999999999999, psi_2 = target( command = (sqrt(5) - 1) / 2 - 9999999999999, trigger = trigger(condition = psi_1 &gt; 0) ) ) plan #&gt; # A tibble: 2 x 3 #&gt; target command trigger #&gt; &lt;chr&gt; &lt;expr&gt; &lt;expr&gt; #&gt; 1 psi_1 (sqrt(5) + 1)/2 + 9999999999999 NA #&gt; 2 psi_2 (sqrt(5) - 1)/2 - 9999999999999 trigger(condition = psi_1 &gt; 0) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; target psi_2 make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; target psi_2 Interestingly, psi_2 now depends on psi_1. Since psi_1 is part of the because of the condition trigger, it needs to be up to date before we attempt psi_2. However, since psi_1 is not part of the command, changing it will not trip the other triggers such as depend. vis_drake_graph(plan) In the next toy example below, drake reads from a file to decide whether to build x. Try it out. plan &lt;- drake_plan( x = target( 1 + 1, trigger = trigger(condition = file_in(readRDS(&quot;file.rds&quot;))) ) ) saveRDS(TRUE, &quot;file.rds&quot;) make(plan) #&gt; target x make(plan) #&gt; target x make(plan) #&gt; target x saveRDS(FALSE, &quot;file.rds&quot;) make(plan) #&gt; All targets are already up to date. make(plan) #&gt; All targets are already up to date. make(plan) #&gt; All targets are already up to date. In a real project with remote data sources, you may want to use the condition trigger to limit your builds to times when enough bandwidth is available for a large download. For example, drake_plan( x = target( command = download_large_dataset(), trigger = trigger(condition = is_enough_bandwidth()) ) ) Since the change trigger can return any value, it is often easier to use than the condition trigger. clean(destroy = TRUE) plan &lt;- drake_plan( x = target( command = 1 + 1, trigger = trigger(change = sqrt(y)) ) ) y &lt;- 1 make(plan) #&gt; target x make(plan) #&gt; All targets are already up to date. y &lt;- 2 make(plan) #&gt; target x In practice, you may want to use the change trigger to check a large remote before downloading it. drake_plan( x = target( command = download_large_dataset(), trigger = trigger( condition = is_enough_bandwidth(), change = date_last_modified() ) ) ) A word of caution: every non-NULL change trigger is always evaluated, and its value is carried around in memory throughout make(). So if you are not careful, heavy use of the change trigger could slow down your workflow and consume extra resources. The change trigger should return small values (and should ideally be quick to evaluate). To reduce memory consumption, you may want to return a fingerprint of your trigger value rather than the value itself. See the digest package for more information on computing hashes/fingerprints. library(digest) drake_plan( x = target( command = download_large_dataset(), trigger = trigger( change = digest(download_medium_dataset()) ) ) ) 18.3 Alternative trigger modes Sometimes, you may want to suppress a target without having to worry about turning off every single trigger. That is why the trigger() function has a mode argument, which controls the role of the condition trigger in the decision to build or skip a target. The available trigger modes are &quot;whitelist&quot; (default), &quot;blacklist&quot;, and &quot;condition&quot;. trigger(mode = &quot;whitelist&quot;): we rebuild the target whenever condition evaluates to TRUE. Otherwise, we defer to the other triggers. This is the default behaviro described above in this chapter. trigger(mode = &quot;blacklist&quot;): we skip the target whenever condition evaluates to FALSE. Otherwise, we defer to the other triggers. trigger(mode = &quot;condition&quot;): here, the condition trigger is the only decider, and we ignore all the other triggers. We rebuild target whenever condition evaluates to TRUE and skip it whenever condition evaluates to FALSE. 18.4 A more practical example See the “packages” example for a more practical demonstration of triggers and their usefulness. "],
["faq.html", "A Frequently-asked questions", " A Frequently-asked questions This FAQ is a compendium of pedagogically useful issues tagged on GitHub. To contribute, please submit a new issue and ask that it be labeled a frequently asked question. Import drake caches into other drake caches Feature request: Better progress log output Making drake play nice w/brms for modeling w/Stan Serialize caches instead of storing them in memory? Efficient track of parameters/ artifacts for different models Why does wrapping make() in a function invalidate some targets? new transform function split to chunk a data.frame Plan should be out-of-date but isn’t map using initial parameter only follows through for one stage Dynamically scale clustermq workers knitr file paths List columns don’t work in map(.data) map() back to original variables after after combine() FAQ: Functions as data Avoid re-running targets if supplied args are the same as default args How to create a jagged cross() transform How to combine() while keeping track of the sources of targets target invalidated when referenced from another plan Within-target parallelism fails cannot remove bindings from a locked environment Functions that depend on targets Erroneous circular workflow error when using NSE in function function dependencies are missing: drake_config() in a magrittr pipe Best practices for including a drake workflow in a package Can you have multiple drake plans? evaluate file.path and variables in file_out and friends Working with HPC time limits Reproducibility with random numbers How should I mix non-R code (e.g. Python and shell scripts) in a large drake workflow? Reproducible remote data sources Trouble with caches sent through Dropbox How to add .R files to drake_plan() "]
]
