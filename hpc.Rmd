# High-performance computing {#hpc}

```{r suppression08, echo = FALSE}
suppressMessages(suppressWarnings(library(future)))
suppressMessages(suppressWarnings(library(drake)))
suppressMessages(suppressWarnings(library(dplyr)))
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

This chapter provides guidance on time-consuming `drake` workflows and high-level parallel computation.

```{r a, eval = FALSE}
library(drake)
load_mtcars_example()
make(my_plan, jobs = 2)
```

## Batch mode for long workflows

If you expect `make()` to take a long time, create a master script for your project (say, `drake-work.R`) and run it in a persistent background process. The following should work in the Mac/Linux terminal/shell.

<pre><code>nohup nice -19 R CMD BATCH drake_work.R &
</code></pre>

where:

- `nohup`: Keep the job running even if you log out of the machine.
- `nice -19`: This is a low-priority job that should not consume many resources. Other processes should take priority.
- `R CMD BATCH drake_work.R`: Run the `drake_work.R` script in a new R session.
- `&`: Run this job in the background so you can do other stuff in the terminal window.

## Let `make()` schedule your targets.

`drake` uses your project's implicit dependency graph to figure out which targets can run in parallel and which ones need to wait for dependencies.

```{r hpcgraph}
load_mtcars_example()
config <- drake_config(my_plan)
vis_drake_graph(config)
```

You do not need to not micromanage the timing among targets, and you do not need to run parallel instances of `make()`. As the next sections describe, `drake` has built-in parallel and distributed computing support.

## Parallel backends

Choose the parallel backend with the `parallelism` argument and set the `jobs` argument to scale the work appropriately.

```{r b, eval = FALSE}
make(my_plan, parallelism = "future", jobs = 2)
```
The two primary backends with long term support are [`clustermq`](https://github.com/mschubert/clustermq) and [`future`](https://github.com/HenrikBengtsson/future). If you can install [ZeroMQ](http://zeromq.org), the best choice is usually [`clustermq`](https://github.com/mschubert/clustermq). (It is faster than [`future`](https://github.com/HenrikBengtsson/future).) However, [`future`](https://github.com/HenrikBengtsson/future) is more accessible: it does not require [ZeroMQ](http://zeromq.org), it supports parallel computing on Windows, it can work with more restrictive wall time limits on clusters, and it [can deploy targets to Docker images](https://github.com/wlandau/drake-examples/tree/master/Docker-psock) (`drake_example("Docker-psock")`).

## The `clustermq` backend

### Persistent workers

The `make(parallelism = "clustermq", jobs = 2)` launches 2 parallel *persistent workers*. The master process assigns targets to workers, and the workers simultaneously traverse the dependency graph.

<script src="https://fast.wistia.com/embed/medias/ycczhxwkjw.jsonp" async></script><script src="https://fast.wistia.com/assets/external/E-v1.js" async></script><div class="wistia_responsive_padding" style="padding:56.21% 0 0 0;position:relative;"><div class="wistia_responsive_wrapper" style="height:100%;left:0;position:absolute;top:0;width:100%;"><div class="wistia_embed wistia_async_ycczhxwkjw videoFoam=true" style="height:100%;position:relative;width:100%"><div class="wistia_swatch" style="height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;"><img src="https://fast.wistia.com/embed/medias/ycczhxwkjw/swatch" style="filter:blur(5px);height:100%;object-fit:contain;width:100%;" alt="" onload="this.parentNode.style.opacity=1;" /></div></div></div></div>

### Installation

You must first install [ZeroMQ](http://zeromq.org/) ([instructions here](http://zeromq.org/intro:get-the-software)) and then install the [`clustermq`](https://github.com/mschubert/clustermq) package.

```{r installclustermq, eval = FALSE}
install.packages("clustermq") # CRAN release
# Alternatively, install the GitHub development version.
devtools::install_github("mschubert/clustermq", ref = "develop")
```

### On your local machine

To run your targets in parallel over the cores of your local machine, set the global option below and run `make()`.

```{r clustermqmulticore, eval = FALSE}
options(clustermq.scheduler = "multicore")
make(plan, parallelism = "clustermq", jobs = 2)
```

### On a cluster

Set the [`clustermq`](https://github.com/mschubert/clustermq) global options to register your computing resources. For [SLURM](https://slurm.schedmd.com/slurmd.html):

```{r clustermqopts, eval = FALSE}
options(clustermq.scheduler = "slurm", clustermq.template = "slurm_clustermq.tmpl")
```

Here, `slurm_clustermq.tmpl` is a [template file](https://github.com/ropensci/drake/tree/master/inst/hpc_template_files) with configuration details. Use `drake_hpc_template_file()` to write one of the available examples.

```{r clustermqtemplatefile, eval = FALSE}
drake_hpc_template_file("slurm_clustermq.tmpl") # Write the file slurm_clustermq.tmpl.
```

After modifying `slurm_clustermq.tmpl` by hand to meet your needs, call `make()` as usual.

```{r clustermqrun, eval = FALSE}
make(plan, parallelism = "clustermq", jobs = 4)
```

## The `future` backend {#future-backend}

### Transient workers

`make(parallelism = "future", jobs = 2)` launches *transient workers* to build your targets. When a target is ready to build, the master process creates a fresh worker to build it, and the worker terminates when the target is done. `jobs = 2` means that at most 2 transient workers are allowed to run at a given time.

<script src="https://fast.wistia.com/embed/medias/340yvlp515.jsonp" async></script><script src="https://fast.wistia.com/assets/external/E-v1.js" async></script><div class="wistia_responsive_padding" style="padding:56.21% 0 0 0;position:relative;"><div class="wistia_responsive_wrapper" style="height:100%;left:0;position:absolute;top:0;width:100%;"><div class="wistia_embed wistia_async_340yvlp515 videoFoam=true" style="height:100%;position:relative;width:100%"><div class="wistia_swatch" style="height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;"><img src="https://fast.wistia.com/embed/medias/340yvlp515/swatch" style="filter:blur(5px);height:100%;object-fit:contain;width:100%;" alt="" onload="this.parentNode.style.opacity=1;" /></div></div></div></div><br>

### Installation

Install the [`future`](https://github.com/HenrikBengtsson/future) package.

```{r installfuture, eval = FALSE}
install.packages("future") # CRAN release
# Alternatively, install the GitHub development version.
devtools::install_github("HenrikBengtsson/future", ref = "develop")
```

If you intend to use a cluster, be sure to install the [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools) package too. The [`future`](https://github.com/HenrikBengtsson/future) ecosystem contains even more packages that extend [`future`](https://github.com/HenrikBengtsson/future)'s parallel computing functionality, such as [`future.callr`](https://github.com/HenrikBengtsson/future.callr).

### On your local machine

First, select a [`future`](https://github.com/HenrikBengtsson/future) plan to tell [`future`](https://github.com/HenrikBengtsson/future) how to create the workers. See [this table](https://github.com/HenrikBengtsson/future#controlling-how-futures-are-resolved) for descriptions of the core options.

```{r futureworkers, eval = FALSE}
future::plan(future::multiprocess) 
```

Next, run `make()`.

```{r futureworkersmake, eval = FALSE}
make(plan, parallelism = "future", jobs = 2)
```

### On a cluster

Install the [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools) package and use [this list](https://github.com/HenrikBengtsson/future.batchtools#choosing-batchtools-backend) to select a [`future`](https://github.com/HenrikBengtsson/future) plan that matches your resources. You will also need a compatible [template file](https://github.com/mllg/batchtools/tree/master/inst/templates) with configuration details. As with [`clustermq`](https://github.com/mschubert/clustermq), `drake` can generate some examples:

```{r exlksjdf, eval = FALSE}
drake_hpc_template_file("slurm_batchtools.tmpl") # Edit by hand.
```

Next, register the template file with a plan.

```{r futureslurmplan, eval = FALSE}
library(future.batchtools)
future::plan(batchtools_slurm, template = "slurm_batchtools.tmpl")
```

Finally, run `make()`.

```{r futureslurmmake, eval = FALSE}
make(plan, parallelism = "future", jobs = 2)
```

## Within-target parallelization

### General

You may wish to invoke parallel computing within individual targets, e.g.

```{r fromplanworkershpc}
plan <- drake_plan(
  a = parallel::mclapply(1:8, sqrt, mc.cores = 4),
  b = parallel::mclapply(1:4, sqrt, mc.cores = 2)
)
```

or even

```{r fromplanworkershpc2}
plan <- drake_plan(
  a = target(
    parallel::mclapply(
      1:8,
      sqrt,
      # You can change the # of cores without changing the command.
      mc.cores = from_plan("cores")
    ),
    cores = 4 # Changes to this number do not invalidate `a`.
  ),
  b = target(
    parallel::mclapply(1:4, sqrt, mc.cores = from_plan("cores")),
    cores = 2
  )
)
```

Unfortunately, for reasons described [here](https://github.com/ropensci/drake/issues/675) and [here](https://stackoverflow.com/questions/54229295/parallelmclapply-adds-or-removes-bindings-to-the-global-environment-which-o), `make(plan)` will fail in each case. Workarounds:

- Avoid `mclapply()`.  [`furrr::map()`](https://davisvaughan.github.io/furrr/) and `parallel::parLapply()` are more dependable alternatives anyway. In the case of [`furrr`](https://davisvaughan.github.io/furrr/), invoke [`future::plan(future.callr::callr)`](https://github.com/HenrikBengtsson/future.callr) or `future::plan(future::multisession)` first.
- In `make()`, set the `lock_envir` argument to `FALSE`. This approach deactivates important reproducibility guardrails, so use with caution.
- In `mclapply()`, set the `mc.set.seed` argument to `FALSE`. If your computations require pseudo-random numbers (`rnorm()`, `runif()`, etc.) you will need to manually set a different seed for each parallel process, e.g.

```{r setseedself, eval = FALSE}
parallel::mclapply(X = 1:4, mc.cores = 4, FUN = function(i) {
  set.seed(sum(.Random.seed) + i)
  # Do some work...
})
```

### `future`-based parallelism 

This section differs to [the section about the future backend](#future-backend) in that it focuses *within-target* parallelism with `future` while the linked section talks about parallelizing targets (= outer level).

Please note the following:

- `future::multisession` is kind of slow in startup (we advise to use `future.callr::callr`)
- the `future::plan` call needs to be placed in the `make.R` file
- there can only be one call to a `future::plan()` as otherwise targets will continously become outdated
- `multicore` only runs on Unix and is known to throw errors when being used with `drake`
- You need to have functions that can make use of the specified `future` backend, such as `furrr::future_map()` or `future::future_lapply()`.

#### on your local machine

There are differences when using `future`-based within-target parallelism on a single machine and when deploying the plan to a HPC.
If the plan is executed on a single machine, the call to `future::plan()` needs to be placed in the `make.R` or `_drake.R` file.

```{r}
library(drake)
future::plan(future::multisession, workers = 4)
plan <- drake_plan(x = furrr::future_map(list(1, 1, 1, 1), Sys.sleep))
make(plan)
```

The specified plan applies to all functions using the `future` backend in your plan.

#### on a cluster

When executing your plan on a HPC, things are slightly different.
Here the `future::plan()` setting needs to be passed to each worker that is spawned on the cluster.
To do so, the `future::plan()` setting needs to be specified in the `prework` argument of `make()` (or `drake_config()` in case you are using the `r_make()` approach).

This way, the settings gets populated to each worker spawned by your respective scheduler.
Note that if you are using the `clustermq` backend, setting individual resources for a target is [not yet supported](https://github.com/mschubert/clustermq/issues/123).

You might want to try the `future` backend for the target-parallelization level which uses transient workers and [supports the specification](#resources-column) of a `resources` column for each worker.

## Advanced options

### Memory

By default, `make()` keeps targets in memory during runtime. Some targets are dependencies of other targets downstream, while others may be no longer actually need to be in memory. The `memory_strategy` argument to `make()` allows you to choose the tradeoff that best suits your project. Options:

- `"speed"`: Once a target is loaded in memory, just keep it there. This choice maximizes speed and hogs memory.
- `"memory"`: Just before building each new target, unload everything from memory except the target's direct dependencies. This option conserves memory, but it sacrifices speed because each new target needs to reload any previously unloaded targets from storage.
- `"lookahead"`: Just before building each new target, search the dependency graph to find targets that will not be needed for the rest of the current `make()` session. In this mode, targets are only in memory if they need to be loaded, and we avoid superfluous reads from the cache. However, searching the graph takes time, and it could even double the computational overhead for large projects.

### Storage

In `make(caching = "master")`, the workers send the targets to the master process, and the master process stores them one by one in the cache. `caching = "master"` is compatible with all [`storr`](https://github.com/richfitz/storr) cache formats, including the more esoteric ones like `storr_dbi()` and `storr_environment()`. 

In `make(caching = "worker")`, the parallel workers are responsible for writing the targets to the cache. Some output-heavy projects can benefit from this form of parallelism. However, it can sometimes add slowness on clusters due to lag from network file systems. And there are additional restrictions:

- All the workers must have the same file system and the same working directory as the master process.
- Only the default `storr_rds()` cache may be used. Other formats like `storr_dbi()` and `storr_environment()` cannot accommodate parallel cache operations.

See the [storage chapter](#store) for details.

### The `template` argument for persistent workers

For more control and flexibility in the [`clustermq`](https://github.com/mschubert/clustermq) backend, you can parameterize your template file and use the `template` argument of `make()`. For example, suppose you want to programatically set the number of "slots" (basically cores) per job on an [SGE system](http://gridscheduler.sourceforge.net/htmlman/manuals.html) (`clustermq` guide to SGE setup [here](https://github.com/mschubert/clustermq/wiki/SGE)). Begin with a parameterized template file `sge_clustermq.tmpl` with a custom `n_slots` placeholder.

```
# File: sge_clustermq.tmpl
# Modified from https://github.com/mschubert/clustermq/wiki/SGE
#$ -N {{ job_name }}               # job name
#$ -t 1-{{ n_jobs }}               # submit jobs as array
#$ -j y                            # combine stdout/error in one file
#$ -o {{ log_file | /dev/null }}   # output file
#$ -cwd                            # use pwd as work dir
#$ -V                              # use environment variable
#$ -pe smp {{ n_slots | 1 }}       # request n_slots cores per job
module load R
ulimit -v $(( 1024 * {{ memory | 4096 }} ))
CMQ_AUTH={{ auth }} R --no-save --no-restore -e 'clustermq:::worker("{{ master }}")'
```

Then when you run `make()`, use the `template` argument to set `n_slots`.

```{r templateslots, eval = FALSE}
options(clustermq.scheduler = "sge", clustermq.template = "sge_clustermq.tmpl")
library(drake)
load_mtcars_example()
make(
  my_plan,
  parallelism = "clustermq",
  jobs = 16,
  template = list(n_slots = 4) # Request 4 cores per persistent worker.
)
```

Custom placeholders like `n_slots` are processed with the [`infuser`](https://github.com/Bart6114/infuser) package.

### The `resources` column for transient workers {#resources-column}

Different targets may need different resources. For example,

```{r diffresources1}
plan <- drake_plan(
  data = download_data(),
  model = big_machine_learning_model(data)
)
```

The `model` needs a GPU and multiple CPU cores, and the `data` only needs the bare minimum resources. Declare these requirements in a new list column of the `plan`. Here, each element is a named list for the `resources` argument of `future::future()`.

```{r planresources}
plan$resources <- list(
  list(cores = 1, gpus = 0),
  list(cores = 4, gpus = 1)
)
```

Next, plug your resources into the [`brew`](https://CRAN.R-project.org/package=brew) patterns of your [`batchtools`](https://github.com/mllg/batchtools) template file. The following `sge_batchtools.tmpl` file shows how to do it, but the file itself probably requires modification before it will work with your own machine.

```
#!/bin/bash
#$ -cwd
#$ -j y
#$ -o <%= log.file %>
#$ -V
#$ -N <%= job.name %>
#$ -pe smp <%= resources[["cores"]] %> # CPU cores
#$ -l gpu=<%= resources[["gpus"]] %>   # GPUs.
Rscript -e 'batchtools::doJobCollection("<%= uri %>")'
exit 0
```

Finally, register the template file and run your project.

```{r futuresgeplanresources, eval = FALSE}
library(drake)
library(future.batchtools)
future::plan(batchtools_sge, template = "sge_batchtools.tmpl")
make(plan, parallelism = "future", jobs = 2)
```

### Custom job schedulers

It is possible to supply a custom job scheduler function to the `parallelism` argument of `make()`. The  `backend_future_lapply_staged()` function from the [`drake.future.lapply.staged`](https://github.com/wlandau/drake.future.lapply.staged) package is an example. You might consider writing your own such function if you wish to

1. Experiment with a more efficient job scheduler before proposing a patch to core `drake`, or
2. Aggressively optimize `drake` for your specialized computing resources.

This feature is very advanced, and you should only attempt it in production if you really know what you are doing. Use at your own risk.

### Hasty mode

The [`drake.hasty`](https://github.com/wlandau/drake.hasty) package is a bare-bones spin-off of `drake`. It sacrifices reproducibility to aggressively boost speed when scheduling and executing your targets. It is not recommended for most serious production use cases, but it can useful for experimentation.

```{r endoflinehpc, echo = FALSE}
clean(destroy = TRUE, verbose = FALSE)
unlink(
  c(
    "Makefile", "report.Rmd", "shell.sh",
    "STDIN.o*", "Thumbs.db", "raw_data.xlsx"
  )
)
```
